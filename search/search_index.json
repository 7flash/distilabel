{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"distilabel","text":"<p>AI Feedback (AIF) framework to build datasets with and for LLMs:</p> <ul> <li>Integrations with the most popular libraries and APIs for LLMs: HF Transformers, OpenAI, vLLM, etc.</li> <li>Multiple tasks for Self-Instruct, Preference datasets and more.</li> <li>Dataset export to Argilla for easy data exploration and further annotation.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p><pre><code>pip install distilabel\n</code></pre> Requires Python 3.8+</p> <p>In addition, the following extras are available:</p> <ul> <li><code>hf-transformers</code>: for using models available in transformers package via the <code>TransformersLLM</code> integration.</li> <li><code>hf-inference-endpoints</code>: for using the Hugging Face Inference Endpoints via the <code>InferenceEndpointsLLM</code> integration.</li> <li><code>openai</code>: for using OpenAI API models via the <code>OpenAILLM</code> integration.</li> <li><code>vllm</code>: for using vllm serving engine via the <code>vLLM</code> integration.</li> <li><code>argilla</code>: for exporting the generated datasets to Argilla.</li> </ul>"},{"location":"#quick-example","title":"Quick example","text":"<pre><code>from datasets import load_dataset\nfrom distilabel.llm import OpenAILLM\nfrom distilabel.pipeline import pipeline\nfrom distilabel.tasks import TextGenerationTask\n\ndataset = (\n    load_dataset(\"HuggingFaceH4/instruction-dataset\", split=\"test[:10]\")\n    .remove_columns([\"completion\", \"meta\"])\n    .rename_column(\"prompt\", \"input\")\n)\n\ntask = TextGenerationTask()  # (1)\n\ngenerator = OpenAILLM(task=task, max_new_tokens=512)  # (2)\n\npipeline = pipeline(\"preference\", \"instruction-following\", generator=generator)  # (3)\n\ndataset = pipeline.generate(dataset)\n</code></pre> <ol> <li>Create a <code>Task</code> for generating text given an instruction.</li> <li>Create a <code>LLM</code> for generating text using the <code>Task</code> created in the first step. As the <code>LLM</code> will generate text, it will be a <code>generator</code>.</li> <li>Create a pre-defined <code>Pipeline</code> using the <code>pipeline</code> function and the <code>generator</code> created in step 2. The <code>pipeline</code> function will create a <code>labeller</code> LLM using <code>OpenAILLM</code> with the <code>UltraFeedback</code> task for instruction following assessment.</li> </ol> <p>Note</p> <p>To run the script successfully, ensure you have assigned your OpenAI API key to the <code>OPENAI_API_KEY</code> environment variable.</p>"},{"location":"concepts/","title":"Concepts","text":"<p>This page aims to get you familiarized with the basic concepts of the framework, describing the most important components or classes and how they work together. The following sections will guide you through the primary components of the framework: <code>Pipeline</code>, <code>LLM</code> (both generator and labeller), and the <code>Task</code>.</p> <p> </p> distilabel flow diagram"},{"location":"concepts/#components","title":"Components","text":""},{"location":"concepts/#task","title":"Task","text":"<p>The <code>Task</code> class in the one in charge of defining the behaviour of the <code>LLM</code>, and therefore it can define if an LLM is a <code>generator</code> or a <code>labeller</code>. To do so, the <code>Task</code> class generates the prompt that will be sent to the <code>LLM</code> from a template. It also defines, which input arguments are required to generate the prompt, and which output arguments will be extracted from the <code>LLM</code> response. It's worth mentioning that the <code>Task</code> class doesn't return a <code>str</code>, but a <code>Prompt</code> class which will generate the <code>str</code> format depending on the <code>LLM</code> that is going to be used (Zephyr, Llama, OpenAI, etc).</p> <pre><code>from distilabel.tasks import UltraJudgeTask\n\ntask = UltraJudgeTask()\n\ninput = (\n    \"Can you provide a corrected version of the following sentence using proper \"\n    'English grammar? \"We going to the beach\" Additionally, could you please '\n    \"provide your correction in an Excel table format with the following columns: \"\n    \"| Incorrect Sentence | Corrected Sentence | |-------------------|--------------------|\"\n)\n\ngenerations = [\n    (\n        \"| Incorrect Sentence | Corrected Sentence |\\n|-------------------|-------------------\"\n        '-----|\\n| \"We going to the beach\" | \"We are going to the beach\" |\\n\\nCorrectio'\n        'n: The verb in the second sentence (\"are\") changes to reflect the subject\\'s (\"w'\n        'e\") agreement with the verb \"be.\" This is called subject-verb agreement. In the '\n        'first sentence, the verb \"going\" infers that the action is ongoing or in a contin'\n        \"uous state, which is not the case. Therefore, the second sentence is grammatically \"\n        \"correct.\"\n    ),\n    (\n        \"| Incorrect Sentence | Corrected Sentence |\\n|-------------------|-------------------\"\n        \"-----|\\n| We going to the beach | We are going to the beach | \\n\\nHere's a breakdo\"\n        'wn of the correction:\\n\\n- \"We going to the beach\" is an example of a subject-ve'\n        'rb agreement error. The verb changing from the third person singular (\"is\") to t'\n        'he third person plural (\"are\") in this instance, as there are multiple people go'\n        'ing to the beach.\\n- The \"g\" in \"going\" changes to an \"e\" due to a hard \"g\"'\n        ' sound being followed by an \"e,\" which is a common spelling rule in English.'\n    ),\n]\n\n\nprompt = task.generate_prompt(input, generations)\nprint(prompt.format_as(\"default\"))  # format as \"openai\", \"zephyr\", \"llama\", ...\n</code></pre>"},{"location":"concepts/#llm","title":"LLM","text":"<p>The <code>LLM</code> class represents a language model and implements the way to interact with it. It also defines the generation parameters that can be passed to the model to tweak the generations. As mentioned above, the <code>LLM</code> will have a <code>Task</code> associated that will use to generate the prompt and extract the output from the generation.</p> <pre><code>from distilabel.llm import OpenAILLM\nfrom distilabel.tasks import UltraJudgeTask\n\nlabeller = OpenAILLM(\n    model=\"gpt-3.5-turbo\",\n    task=UltraJudgeTask(),\n    prompt_format=\"openai\",\n    max_new_tokens=2048,\n    temperature=0.0,\n)\n\noutputs = labeller.generate(\n    inputs=[\n        {\n            \"input\": \"Here's a math problem that you need to resolve: 2 + 2 * 3. What's the result of this problem? Explain it\",\n            \"generations\": [\n                (\n                    \"The output of the math problem 2 + 2 * 3 is calculated by following \"\n                    \"the order of operations (PEMDAS). First, perform the multiplication: \"\n                    \"2 * 3 = 6. Then, perform the addition: 2 + 6 = 8. Therefore, the \"\n                    \"output of the problem is 8.\"\n                ),\n                (\n                    \"The correct solution to the math problem is 8. To get the correct \"\n                    \"answer, we follow the order of operations (PEMDAS) and perform \"\n                    \"multiplication before addition. So, first, we solve 2 * 3 = 6, \"\n                    \"then we add 2 to 6 to get 8.\"\n                ),\n            ],\n        }\n    ]\n)\n\nprint(outputs[0][0][\"parsed_output\"])\n</code></pre> <p>Note</p> <p>To run the script successfully, ensure you have assigned your OpenAI API key to the <code>OPENAI_API_KEY</code> environment variable.</p>"},{"location":"concepts/#pipeline","title":"Pipeline","text":"<p>The <code>Pipeline</code> class orchestrates the whole generation and labelling process, and it's in charge of the batching of the input dataset, as well as reporting the generation progress. It's worth mentioning that is not mandatory to pass both a generator <code>LLM</code> and a labeller <code>LLM</code> to the <code>Pipeline</code> class, as it can also be used only for generation or labelling.</p> <p>Pipelines</p> Generator and labellerOnly generatorOnly labeller <pre><code>from datasets import load_dataset\nfrom distilabel.llm import LlamaCppLLM, OpenAILLM\nfrom distilabel.pipeline import Pipeline\nfrom distilabel.tasks import TextGenerationTask, UltraJudgeTask\nfrom llama_cpp import Llama\n\ndataset = load_dataset(\"argilla/distilabel-docs\", split=\"train\")\ndataset = dataset.remove_columns(\n    [\n        column\n        for column in dataset.column_names\n        if column not in [\"input\", \"generations\"]\n    ]\n)\n\npipeline = Pipeline(\n    generator=LlamaCppLLM(\n        model=Llama(\n            model_path=\"./llama-2-7b-chat.Q4_0.gguf\",\n            verbose=False,\n            n_ctx=1024,\n        ),\n        task=TextGenerationTask(),\n        max_new_tokens=512,\n        prompt_format=\"llama2\",\n    ),\n    labeller=OpenAILLM(\n        model=\"gpt-3.5-turbo\",\n        task=UltraJudgeTask(),\n        prompt_format=\"openai\",\n        max_new_tokens=1024,\n        num_threads=1,\n        temperature=0.0,\n    ),\n)\n\n\ndataset = pipeline.generate(dataset, num_generations=2, batch_size=5)\n</code></pre> <p>Note</p> <p>To run the script successfully, ensure you have assigned your OpenAI API key to the <code>OPENAI_API_KEY</code> environment variable and that you have download the file llama-2-7b-chat.Q4_O.gguf in the same folder as the script.</p> <pre><code>from datasets import load_dataset\nfrom distilabel.llm import LlamaCppLLM\nfrom distilabel.pipeline import Pipeline\nfrom distilabel.tasks import TextGenerationTask\nfrom llama_cpp import Llama\n\ndataset = load_dataset(\"argilla/distilabel-docs\", split=\"train\")\ndataset = dataset.remove_columns(\n    [column for column in dataset.column_names if column not in [\"input\"]]\n)\n\npipeline = Pipeline(\n    generator=LlamaCppLLM(\n        model=Llama(\n            model_path=\"./llama-2-7b-chat.Q4_0.gguf\",\n            verbose=False,\n            n_ctx=1024,\n        ),\n        task=TextGenerationTask(),\n        max_new_tokens=512,\n        prompt_format=\"llama2\",\n    ),\n)\n\n\ndataset = pipeline.generate(dataset, num_generations=2, batch_size=5)\n</code></pre> <pre><code>from datasets import load_dataset\nfrom distilabel.llm import OpenAILLM\nfrom distilabel.pipeline import Pipeline\nfrom distilabel.tasks import UltraJudgeTask\n\ndataset = load_dataset(\"argilla/distilabel-docs\", split=\"train\")\ndataset = dataset.remove_columns(\n    [\n        column\n        for column in dataset.column_names\n        if column not in [\"input\", \"generations\"]\n    ]\n)\n\npipeline = Pipeline(\n    labeller=OpenAILLM(\n        model=\"gpt-3.5-turbo\",\n        task=UltraJudgeTask(),\n        prompt_format=\"openai\",\n        max_new_tokens=1024,\n        num_threads=1,\n        temperature=0.0,\n    ),\n)\n\n\ndataset = pipeline.generate(dataset, num_generations=2, batch_size=5)\n</code></pre>"},{"location":"guides/","title":"Guides","text":"<p>This page is still WIP, stay tuned!</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>distilabel<ul> <li>dataset</li> <li>llm<ul> <li>base</li> <li>huggingface<ul> <li>inference_endpoints</li> <li>transformers</li> </ul> </li> <li>llama_cpp</li> <li>openai</li> <li>utils</li> <li>vllm</li> </ul> </li> <li>logger</li> <li>pipeline</li> <li>progress_bar</li> <li>tasks<ul> <li>base</li> <li>preference<ul> <li>base</li> <li>judgelm</li> <li>ultrafeedback</li> <li>ultrajudge</li> </ul> </li> <li>prompt</li> <li>text_generation<ul> <li>base</li> <li>llama</li> <li>openai</li> <li>principles</li> <li>self_instruct</li> </ul> </li> </ul> </li> <li>utils<ul> <li>dicts</li> <li>imports</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/distilabel/","title":"distilabel","text":""},{"location":"reference/distilabel/dataset/","title":"dataset","text":""},{"location":"reference/distilabel/dataset/#distilabel.dataset.CustomDataset","title":"<code>CustomDataset</code>","text":"<p>             Bases: <code>Dataset</code></p> <p>A custom dataset class that extends from <code>datasets.Dataset</code> and is used to generate an Argilla <code>FeedbackDataset</code> instance from the pre-defined configuration within the task provided to <code>Pipeline.generate</code>.</p> Source code in <code>src/distilabel/dataset.py</code> <pre><code>class CustomDataset(Dataset):\n    \"\"\"A custom dataset class that extends from `datasets.Dataset` and is used to generate\n    an Argilla `FeedbackDataset` instance from the pre-defined configuration within the task\n    provided to `Pipeline.generate`.\n    \"\"\"\n\n    task: Union[\"Task\", None] = None\n\n    def to_argilla(self) -&gt; \"FeedbackDataset\":\n        \"\"\"Converts the dataset to an Argilla `FeedbackDataset` instance, based on the\n        task defined in the dataset as part of `Pipeline.generate`.\n\n        Raises:\n            ImportError: if the argilla library is not installed.\n            ValueError: if the task is not set.\n\n        Returns:\n            FeedbackDataset: the Argilla `FeedbackDataset` instance.\n        \"\"\"\n        if not _ARGILLA_AVAILABLE:\n            raise ImportError(\n                \"To use `to_argilla` method is required to have `argilla` installed. \"\n                \"Please install it with `pip install argilla`.\"\n            )\n\n        if self.task is None:\n            raise ValueError(\n                \"The task is not set. Please set it with `dataset.task = &lt;task&gt;`.\"\n            )\n\n        rg_dataset = rg.FeedbackDataset(\n            fields=self.task.to_argilla_fields(dataset_row=self[0]),\n            questions=self.task.to_argilla_questions(dataset_row=self[0]),\n            metadata_properties=self.task.to_argilla_metadata_properties(\n                dataset_row=self[0]\n            ),\n        )\n        for dataset_row in self:\n            if any(\n                dataset_row[input_arg_name] is None  # type: ignore\n                for input_arg_name in self.task.input_args_names\n            ):\n                continue\n            rg_dataset.add_records(\n                self.task.to_argilla_record(dataset_row=dataset_row)  # type: ignore\n            )\n        return rg_dataset\n</code></pre>"},{"location":"reference/distilabel/dataset/#distilabel.dataset.CustomDataset.to_argilla","title":"<code>to_argilla()</code>","text":"<p>Converts the dataset to an Argilla <code>FeedbackDataset</code> instance, based on the task defined in the dataset as part of <code>Pipeline.generate</code>.</p> <p>Raises:</p> Type Description <code>ImportError</code> <p>if the argilla library is not installed.</p> <code>ValueError</code> <p>if the task is not set.</p> <p>Returns:</p> Name Type Description <code>FeedbackDataset</code> <code>FeedbackDataset</code> <p>the Argilla <code>FeedbackDataset</code> instance.</p> Source code in <code>src/distilabel/dataset.py</code> <pre><code>def to_argilla(self) -&gt; \"FeedbackDataset\":\n    \"\"\"Converts the dataset to an Argilla `FeedbackDataset` instance, based on the\n    task defined in the dataset as part of `Pipeline.generate`.\n\n    Raises:\n        ImportError: if the argilla library is not installed.\n        ValueError: if the task is not set.\n\n    Returns:\n        FeedbackDataset: the Argilla `FeedbackDataset` instance.\n    \"\"\"\n    if not _ARGILLA_AVAILABLE:\n        raise ImportError(\n            \"To use `to_argilla` method is required to have `argilla` installed. \"\n            \"Please install it with `pip install argilla`.\"\n        )\n\n    if self.task is None:\n        raise ValueError(\n            \"The task is not set. Please set it with `dataset.task = &lt;task&gt;`.\"\n        )\n\n    rg_dataset = rg.FeedbackDataset(\n        fields=self.task.to_argilla_fields(dataset_row=self[0]),\n        questions=self.task.to_argilla_questions(dataset_row=self[0]),\n        metadata_properties=self.task.to_argilla_metadata_properties(\n            dataset_row=self[0]\n        ),\n    )\n    for dataset_row in self:\n        if any(\n            dataset_row[input_arg_name] is None  # type: ignore\n            for input_arg_name in self.task.input_args_names\n        ):\n            continue\n        rg_dataset.add_records(\n            self.task.to_argilla_record(dataset_row=dataset_row)  # type: ignore\n        )\n    return rg_dataset\n</code></pre>"},{"location":"reference/distilabel/logger/","title":"logger","text":""},{"location":"reference/distilabel/pipeline/","title":"pipeline","text":""},{"location":"reference/distilabel/pipeline/#distilabel.pipeline.Pipeline","title":"<code>Pipeline</code>","text":"Source code in <code>src/distilabel/pipeline.py</code> <pre><code>class Pipeline:\n    def __init__(\n        self,\n        generator: Union[\"LLM\", None] = None,\n        labeller: Union[\"LLM\", None] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the Pipeline class.\n\n        Args:\n            generator (Union[\"LLM\", None], optional): the LLM to be used for generation.\n                Defaults to None.\n            labeller (Union[\"LLM\", None], optional): the LLM to be used for labelling.\n                Defaults to None.\n\n        Raises:\n            ValueError: if no LLM is provided.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.llm.huggingface import TransformersLLM\n            &gt;&gt;&gt; from distilabel.llm.openai_ import OpenAILLM\n            &gt;&gt;&gt; from distilabel.tasks.preference.ultrafeedback import UltraFeedbackTask\n            &gt;&gt;&gt; from distilabel.tasks.text_generation.llama import Llama2TextGenerationTask\n            &gt;&gt;&gt; from distilabel.pipeline import Pipeline\n\n            &gt;&gt;&gt; generator = TransformersLLM(\n            ...     model=\"meta-llama/Llama-2-7b-chat-hf\",\n            ...     tokenizer=\"meta-llama/Llama-2-7b-chat-hf\",\n            ...     task=Llama2TextGenerationTask(),\n            ... )\n            &gt;&gt;&gt; labeller = OpenAILLM(\n            ...     model=\"gpt-3.5-turbo\",\n            ...     task=UltraFeedbackTask.for_text_quality(),\n            ... )\n            &gt;&gt;&gt; pipeline = Pipeline(generator=generator, labeller=labeller)\n            &gt;&gt;&gt; dataset = pipeline.generate(dataset=..., num_generations=1, batch_size=1)\n        \"\"\"\n        self.generator = generator\n        self.labeller = labeller\n\n        if self.generator is None and self.labeller is None:\n            raise ValueError(\"At least one LLM has to be provided to the pipeline\")\n\n    def __repr__(self) -&gt; str:\n        return (\n            f\"Pipeline(\\n\\tgenerator={self.generator},\\n\\tlabeller={self.labeller}\\n)\"\n        )\n\n    def __rich_repr__(self) -&gt; Generator[Any, None, None]:\n        yield \"generator\", self.generator\n        yield \"labeller\", self.labeller\n\n    def _validate_dataset(self, dataset: Dataset) -&gt; None:\n        \"\"\"Validates that the provided dataset contains the columns needed by the LLMs, and\n        warns the user if the columns to be generated already exist.\n\n        Args:\n            dataset (Dataset): the dataset to be validated.\n\n        Raises:\n            KeyError: if the dataset does not contain the columns needed by the LLMs.\n        \"\"\"\n        # Generation LLM has not been provided, so the columns needed by the Labelling\n        # LLM must be in the provided dataset\n        if self.labeller is not None:\n            if self.generator is None:\n                try:\n                    self.labeller.task.validate_dataset(dataset.column_names)\n                except KeyError as err:\n                    raise KeyError(\n                        \"Labelling LLM expects a dataset with at least the following\"\n                        f\" columns: {self.labeller.task.input_args_names}, but the provided\"\n                        f\" dataset just contains: {dataset.column_names}\"\n                    ) from err\n            else:\n                expected_columns = (\n                    dataset.column_names + self.generator.task.output_args_names\n                )\n                try:\n                    self.labeller.task.validate_dataset(expected_columns)\n                except KeyError as err:\n                    raise KeyError(\n                        \"Labelling LLM expects to receive the following columns after the\"\n                        f\" generation process: {self.labeller.task.input_args_names}, but the\"\n                        f\" provided dataset including the columns to generate just contains: {expected_columns}\"\n                    ) from err\n\n        if self.generator is not None:\n            try:\n                self.generator.task.validate_dataset(dataset.column_names)\n            except KeyError as err:\n                raise KeyError(\n                    \"Generation LLM expects a dataset with the following columns:\"\n                    f\" {self.generator.task.input_args_names}, but the provided dataset\"\n                    f\" just contains: {dataset.column_names}\"\n                ) from err\n\n        # Additionally, we need to check that if the columns to be generated already exist,\n        # then we should look for `None`/`null` values and just fulfill those, while skipping\n        # the rest. This is useful to be able to continue a generation that broke or a process\n        # that was interrupted\n        generated_columns = []\n        if self.generator is not None:\n            generated_columns += self.generator.task.output_args_names\n        if self.labeller is not None:\n            generated_columns += self.labeller.task.output_args_names\n\n        if set(generated_columns) == set(dataset.column_names).intersection(\n            set(generated_columns)\n        ):\n            warnings.warn(\n                \"The provided dataset already contains the columns to be generated:\"\n                f\" {generated_columns}; which means that the generation process will\"\n                \" be skipped for the rows with values for those columns. If you want\"\n                \" to re-generate those columns, please remove them from the dataset.\",\n                UserWarning,\n                stacklevel=2,\n            )\n\n    def _get_batch_generations(\n        self,\n        inputs: List[Dict[str, Any]],\n        num_generations: int,\n        progress_callback_func: Union[Callable, None] = None,\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Gets the batch generations for the given inputs, capturing the futures if the\n        LLM returns them, and then processes the batch generations.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n            num_generations (int): the number of generations to be performed for each\n                input.\n            progress_callback_func (Union[Callable, None], optional): the callback function\n                to be called when the progress of the generation process changes. Defaults\n                to None.\n\n        Returns:\n            List[Dict[str, Any]]: the processed batch generations.\n        \"\"\"\n        batch_generations = self.generator.generate(\n            inputs=inputs,\n            num_generations=num_generations,\n            progress_callback_func=progress_callback_func,\n        )\n\n        processed_generations = []\n        if self.generator.return_futures:  # type: ignore\n            for future in batch_generations:\n                result = future.result()\n                processed_generations.extend(result)\n        else:\n            processed_generations = batch_generations\n\n        return self._process_batch_generations(batch_generations=processed_generations)\n\n    def _process_batch_generations(\n        self,\n        batch_generations: List[List[\"LLMOutput\"]],\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Processes the batch generations, combining the outputs of the LLMs into a single\n        dictionary.\n\n        Args:\n            batch_generations (List[List[\"LLMOutput\"]]): the batch generations to be processed.\n\n        Returns:\n            List[Dict[str, Any]]: the processed batch generations.\n        \"\"\"\n        processed_generations = []\n        for generations in batch_generations:\n            processed_generation = {\n                # Since all the generations for the same `model_name` also share the same\n                # `prompt_used`, then we just keep the first element in `generations`\n                \"generation_model\": generations[0][\"model_name\"],\n                \"generation_prompt\": generations[0][\"prompt_used\"],\n                \"raw_generation_responses\": [\n                    generation[\"raw_output\"] for generation in generations\n                ],\n            }\n            try:\n                processed_generation.update(\n                    **combine_dicts(\n                        *[generation[\"parsed_output\"] for generation in generations]\n                    )\n                )\n            except Exception as e:\n                warnings.warn(\n                    f\"Generation processing step failed when combining dicts: {e}\",\n                    UserWarning,\n                    stacklevel=2,\n                )\n            processed_generations.append(processed_generation)\n        return processed_generations\n\n    def _include_generator_outputs_as_inputs(\n        self, inputs: List[Dict[str, Any]], outputs: List[Dict[str, Any]]\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Includes the outputs of the generator as inputs for the labeller.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for labelling.\n            outputs (List[Dict[str, Any]]): the outputs of the generator.\n\n        Returns:\n            List[Dict[str, Any]]: the inputs to be used for labelling.\n        \"\"\"\n        for input_, output in zip(inputs, outputs):\n            # Skip the keys not required by the labelling LLM\n            input_.update(\n                {\n                    k: v\n                    for k, v in output.items()\n                    if self.labeller is not None\n                    and k in self.labeller.task.input_args_names\n                }\n            )\n        return inputs\n\n    def _process_batch_labels(\n        self, batch_labels: List[List[\"LLMOutput\"]]\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Processes the batch labels, combining the outputs of the LLMs into a single\n        dictionary.\n\n        Args:\n            batch_labels (List[List[\"LLMOutput\"]]): the batch labels to be processed.\n\n        Returns:\n            List[Dict[str, Any]]: the processed batch labels.\n        \"\"\"\n        processed_labels = []\n        for labels in batch_labels:\n            for label in labels:\n                if label[\"parsed_output\"] is not None and not isinstance(\n                    label[\"parsed_output\"], (list, dict)\n                ):\n                    raise ValueError(\n                        f\"Unsupported type: {type(label['parsed_output'])}\"\n                    )\n\n                processed_label = {\n                    # Since all the generations for the same `model_name` also share the same\n                    # `prompt_used`, then we just keep the first element in `generations`\n                    \"labelling_model\": label[\"model_name\"],\n                    \"labelling_prompt\": label[\"prompt_used\"],\n                    \"raw_labelling_response\": label[\"raw_output\"],\n                }\n                try:\n                    if isinstance(label[\"parsed_output\"], list):\n                        processed_label.update(**combine_dicts(*label[\"parsed_output\"]))\n                    elif isinstance(label[\"parsed_output\"], dict):\n                        processed_label.update(**label[\"parsed_output\"])\n                except Exception as e:\n                    warnings.warn(\n                        f\"Label processing step failed when combining dicts: {e}\",\n                        UserWarning,\n                        stacklevel=2,\n                    )\n                processed_labels.append(processed_label)\n        return processed_labels\n\n    def _transform_dataset_to_expected_format(\n        self, rows: Dict[str, List[Any]]\n    ) -&gt; List[Dict[str, Any]]:\n        \"\"\"Transforms the `datasets.Dataset` to the expected format required by the LLMs\n        during the `generate` process.\n\n        Args:\n            rows (Dict[str, List[Any]]): the rows to be transformed.\n\n        Returns:\n            List[Dict[str, Any]]: the transformed rows.\n        \"\"\"\n        length = len(next(iter(rows.values())))\n\n        generator_column_names = []\n        if self.generator is not None:\n            generator_column_names = self.generator.task.input_args_names\n        labeller_column_names = []\n        if self.labeller is not None:\n            labeller_column_names = self.labeller.task.input_args_names\n        column_names = generator_column_names + labeller_column_names\n\n        inputs = []\n        for i in range(length):\n            input = {\n                col: values[i] for col, values in rows.items() if col in column_names\n            }\n            inputs.append(input)\n\n        return inputs\n\n    def _build_dataset(  # noqa: C901\n        self,\n        dataset: Dataset,\n        generations: List[Dict[str, Any]],\n        batch_labels: Union[List[Future[\"LLMOutput\"]], List[\"LLMOutput\"]],\n    ) -&gt; CustomDataset:\n        \"\"\"Builds the final dataset with either the generations, the labels, or both, depending\n        on the LLMs provided to the `Pipeline`.\n\n        Args:\n            dataset (Dataset): the original dataset.\n            generations (List[Dict[str, Any]]): the processed generations.\n            batch_labels (Union[List[Future[\"LLMOutput\"]], List[\"LLMOutput\"]]): the processed\n                batch labels.\n\n        Returns:\n            CustomDataset: the final dataset.\n\n        Raises:\n            RuntimeError: if the `Pipeline` fails during the generation or labelling steps.\n        \"\"\"\n        if self.generator is None:\n            generations = [{} for _ in range(len(dataset))]\n        else:\n            generator_column_names = [\n                \"generation_model\",\n                \"generation_prompt\",\n                \"raw_generation_responses\",\n            ] + self.generator.task.output_args_names\n\n            if len(generations) &lt; len(dataset):\n                generations.extend(\n                    [\n                        {key: None for key in generator_column_names}\n                        for _ in range(len(dataset) - len(generations))\n                    ]\n                )\n\n            for generation in generations:\n                for key in generator_column_names:\n                    if key not in generation:\n                        generation.update({key: None})\n\n        if self.labeller is None:\n            labels = [{} for _ in range(len(dataset))]\n        else:\n            # If the LLM returns futures, we need to wait for them to finish\n            processed_labels = []\n            if self.labeller.return_futures:\n                for future in batch_labels:\n                    try:\n                        processed_labels.extend(future.result())\n                    except Exception as e:\n                        logger.error(\n                            f\"An error ocurred when getting the result from the labeller: {e}\"\n                        )\n                        processed_labels.append(\n                            [\n                                LLMOutput(\n                                    model_name=self.labeller.model_name,\n                                    prompt_used=None,\n                                    raw_output=None,\n                                    parsed_output=None,\n                                )\n                            ]\n                        )\n            else:\n                processed_labels = batch_labels\n            labels = self._process_batch_labels(batch_labels=processed_labels)  # type: ignore\n\n            labeller_column_names = [\n                \"labelling_model\",\n                \"labelling_prompt\",\n                \"raw_labelling_response\",\n            ] + self.labeller.task.output_args_names\n\n            # Ensure the lengths of the labels and the dataset match (when pipeline\n            # fails in an intermediate step, the labels may be shorter than the dataset)\n            if len(labels) &lt; len(dataset):\n                labels.extend(\n                    [\n                        {key: None for key in labeller_column_names}\n                        for _ in range(len(dataset) - len(labels))\n                    ]\n                )\n\n            # Add missing keys/columns with a `None` value\n            for label in labels:\n                for key in labeller_column_names:\n                    if key not in label:\n                        label.update({key: None})\n\n        _dataset = Dataset(\n            arrow_table=dataset.flatten_indices().data, split=Split.TRAIN\n        )\n        _dataset = _dataset.map(lambda _: {**generations.pop(0), **labels.pop(0)})  # type: ignore\n        # Dynamically remaps the `datasets.Dataset` to be a `CustomDataset` instance\n        _dataset.__class__ = CustomDataset\n        _dataset.task = self.labeller.task if self.labeller is not None else None  # type: ignore\n        return _dataset  # type: ignore\n\n    @use_progress_bar\n    def generate(  # noqa: C901\n        self,\n        dataset: Dataset,\n        num_generations: int = 1,\n        batch_size: int = 1,\n        enable_checkpoints: bool = True,\n        display_progress_bar: bool = False,\n        verbose: bool = True,\n    ) -&gt; CustomDataset:\n        \"\"\"Generates the outputs for the given dataset using the LLMs provided to the `Pipeline`.\n\n        Args:\n            dataset (Dataset): the dataset to be used for generation.\n            num_generations (int, optional): the number of generations to be performed for each\n                input. Defaults to `1`.\n            batch_size (int, optional): the batch size to be used for generation. Defaults to `1`.\n            enable_checkpoints (bool, optional): whether to enable checkpoints or not. Defaults to `True`.\n            display_progress_bar (bool, optional): whether to display the progress bar or not. Defaults to `False`.\n            verbose (bool, optional): whether to display the logs or not. Defaults to `True`.\n\n        Returns:\n            CustomDataset: the final dataset.\n\n        Raises:\n            RuntimeError: if the `Pipeline` fails during the generation or labelling steps.\n            UserWarning: if the `Pipeline` fails during the generation or labelling steps and\n                `enable_checkpoints` is set to `False`.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.llm.huggingface import TransformersLLM\n            &gt;&gt;&gt; from distilabel.llm.openai_ import OpenAILLM\n            &gt;&gt;&gt; from distilabel.tasks.preference.ultrafeedback import UltraFeedbackTask\n            &gt;&gt;&gt; from distilabel.tasks.text_generation.llama import Llama2TextGenerationTask\n            &gt;&gt;&gt; from distilabel.pipeline import Pipeline\n\n            &gt;&gt;&gt; generator = TransformersLLM(\n            ...     model=\"meta-llama/Llama-2-7b-chat-hf\",\n            ...     tokenizer=\"meta-llama/Llama-2-7b-chat-hf\",\n            ...     task=Llama2TextGenerationTask(),\n            ... )\n            &gt;&gt;&gt; labeller = OpenAILLM(\n            ...     model=\"gpt-3.5-turbo\",\n            ...     task=UltraFeedbackTask.for_text_quality(),\n            ... )\n            &gt;&gt;&gt; pipeline = Pipeline(generator=generator, labeller=labeller)\n            &gt;&gt;&gt; dataset = pipeline.generate(dataset=..., num_generations=1, batch_size=1)\n        \"\"\"\n        if not verbose:\n            logger.setLevel(\"ERROR\")\n\n        if (\n            self.labeller is not None\n            and self.generator is not None\n            and num_generations &lt; 2\n        ):\n            warnings.warn(\n                f\"Provided `num_generations={num_generations}` which implies that the \"\n                \"`generator` LLM will just run once, while the `labelling` LLM expects \"\n                \"to recieve a list of N inputs to label, where N is &gt; 1. If this is not \"\n                \"intended, make sure to set `num_generations` to a value higher or \"\n                \"equal to 2.\",\n                UserWarning,\n                stacklevel=2,\n            )\n\n        self._validate_dataset(dataset)\n\n        generations: List[Dict[str, Any]] = []\n        batch_labels: Union[Future[List[\"LLMOutput\"]], List[\"LLMOutput\"]] = []\n\n        (\n            generation_progress_func,\n            labelling_progress_func,\n        ) = get_progress_bars_for_pipeline(\n            num_rows=len(dataset),\n            num_generations=num_generations,\n            display_progress_bar=display_progress_bar,\n        )\n\n        num_batches = math.ceil(len(dataset) / batch_size)\n\n        for batch_i, rows in enumerate(dataset.iter(batch_size=batch_size), start=1):\n            logger.info(f\"Processing batch {batch_i} of {num_batches}...\")\n            inputs = self._transform_dataset_to_expected_format(rows)  # type: ignore\n\n            if self.generator is not None:\n                logger.info(f\"Calling generator for batch {batch_i}...\")\n                try:\n                    batch_generations = self._get_batch_generations(\n                        inputs, num_generations, generation_progress_func\n                    )\n                    generations.extend(batch_generations)\n                except Exception as e:\n                    if not enable_checkpoints:\n                        raise RuntimeError(\n                            \"`Pipeline.generate` failed during generation step. Setting `enable_checkpoints=True` is recommended!\"\n                        ) from e\n                    logger.error(\n                        f\"`Pipeline.generate` failed during generation step with exception: {e}\"\n                    )\n                    return self._build_dataset(\n                        dataset, generations=generations, batch_labels=batch_labels\n                    )\n\n                inputs = self._include_generator_outputs_as_inputs(\n                    inputs, batch_generations\n                )\n\n            if self.labeller is not None:\n                logger.info(f\"Calling labeller for batch {batch_i}...\")\n                try:\n                    # TODO: move to `self._get_batch_labels` (without awaiting futures)\n                    batch_labels.extend(\n                        self.labeller.generate(  # type: ignore\n                            inputs=inputs,\n                            # `num_generations` is always 1 because labelling the same input multiple times\n                            # using the same LLM may not make sense\n                            num_generations=1,\n                            progress_callback_func=labelling_progress_func,\n                        )\n                    )\n                except Exception as e:\n                    if not enable_checkpoints:\n                        raise RuntimeError(\n                            \"`Pipeline.generate` failed during labelling step. Setting `enable_checkpoints=True` is recommended!\"\n                        ) from e\n                    logger.error(\n                        f\"`Pipeline.generate` failed during labelling step with exception: {e}\"\n                    )\n                    return self._build_dataset(\n                        dataset, generations=generations, batch_labels=batch_labels\n                    )\n\n        _pipeline_progress.stop()\n\n        return self._build_dataset(\n            dataset, generations=generations, batch_labels=batch_labels\n        )\n</code></pre>"},{"location":"reference/distilabel/pipeline/#distilabel.pipeline.Pipeline.__init__","title":"<code>__init__(generator=None, labeller=None)</code>","text":"<p>Initializes the Pipeline class.</p> <p>Parameters:</p> Name Type Description Default <code>generator</code> <code>Union[LLM, None]</code> <p>the LLM to be used for generation. Defaults to None.</p> <code>None</code> <code>labeller</code> <code>Union[LLM, None]</code> <p>the LLM to be used for labelling. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if no LLM is provided.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.llm.huggingface import TransformersLLM\n&gt;&gt;&gt; from distilabel.llm.openai_ import OpenAILLM\n&gt;&gt;&gt; from distilabel.tasks.preference.ultrafeedback import UltraFeedbackTask\n&gt;&gt;&gt; from distilabel.tasks.text_generation.llama import Llama2TextGenerationTask\n&gt;&gt;&gt; from distilabel.pipeline import Pipeline\n</code></pre> <pre><code>&gt;&gt;&gt; generator = TransformersLLM(\n...     model=\"meta-llama/Llama-2-7b-chat-hf\",\n...     tokenizer=\"meta-llama/Llama-2-7b-chat-hf\",\n...     task=Llama2TextGenerationTask(),\n... )\n&gt;&gt;&gt; labeller = OpenAILLM(\n...     model=\"gpt-3.5-turbo\",\n...     task=UltraFeedbackTask.for_text_quality(),\n... )\n&gt;&gt;&gt; pipeline = Pipeline(generator=generator, labeller=labeller)\n&gt;&gt;&gt; dataset = pipeline.generate(dataset=..., num_generations=1, batch_size=1)\n</code></pre> Source code in <code>src/distilabel/pipeline.py</code> <pre><code>def __init__(\n    self,\n    generator: Union[\"LLM\", None] = None,\n    labeller: Union[\"LLM\", None] = None,\n) -&gt; None:\n    \"\"\"Initializes the Pipeline class.\n\n    Args:\n        generator (Union[\"LLM\", None], optional): the LLM to be used for generation.\n            Defaults to None.\n        labeller (Union[\"LLM\", None], optional): the LLM to be used for labelling.\n            Defaults to None.\n\n    Raises:\n        ValueError: if no LLM is provided.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.llm.huggingface import TransformersLLM\n        &gt;&gt;&gt; from distilabel.llm.openai_ import OpenAILLM\n        &gt;&gt;&gt; from distilabel.tasks.preference.ultrafeedback import UltraFeedbackTask\n        &gt;&gt;&gt; from distilabel.tasks.text_generation.llama import Llama2TextGenerationTask\n        &gt;&gt;&gt; from distilabel.pipeline import Pipeline\n\n        &gt;&gt;&gt; generator = TransformersLLM(\n        ...     model=\"meta-llama/Llama-2-7b-chat-hf\",\n        ...     tokenizer=\"meta-llama/Llama-2-7b-chat-hf\",\n        ...     task=Llama2TextGenerationTask(),\n        ... )\n        &gt;&gt;&gt; labeller = OpenAILLM(\n        ...     model=\"gpt-3.5-turbo\",\n        ...     task=UltraFeedbackTask.for_text_quality(),\n        ... )\n        &gt;&gt;&gt; pipeline = Pipeline(generator=generator, labeller=labeller)\n        &gt;&gt;&gt; dataset = pipeline.generate(dataset=..., num_generations=1, batch_size=1)\n    \"\"\"\n    self.generator = generator\n    self.labeller = labeller\n\n    if self.generator is None and self.labeller is None:\n        raise ValueError(\"At least one LLM has to be provided to the pipeline\")\n</code></pre>"},{"location":"reference/distilabel/pipeline/#distilabel.pipeline.Pipeline.generate","title":"<code>generate(dataset, num_generations=1, batch_size=1, enable_checkpoints=True, display_progress_bar=False, verbose=True)</code>","text":"<p>Generates the outputs for the given dataset using the LLMs provided to the <code>Pipeline</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>the dataset to be used for generation.</p> required <code>num_generations</code> <code>int</code> <p>the number of generations to be performed for each input. Defaults to <code>1</code>.</p> <code>1</code> <code>batch_size</code> <code>int</code> <p>the batch size to be used for generation. Defaults to <code>1</code>.</p> <code>1</code> <code>enable_checkpoints</code> <code>bool</code> <p>whether to enable checkpoints or not. Defaults to <code>True</code>.</p> <code>True</code> <code>display_progress_bar</code> <code>bool</code> <p>whether to display the progress bar or not. Defaults to <code>False</code>.</p> <code>False</code> <code>verbose</code> <code>bool</code> <p>whether to display the logs or not. Defaults to <code>True</code>.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>CustomDataset</code> <code>CustomDataset</code> <p>the final dataset.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>if the <code>Pipeline</code> fails during the generation or labelling steps.</p> <code>UserWarning</code> <p>if the <code>Pipeline</code> fails during the generation or labelling steps and <code>enable_checkpoints</code> is set to <code>False</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.llm.huggingface import TransformersLLM\n&gt;&gt;&gt; from distilabel.llm.openai_ import OpenAILLM\n&gt;&gt;&gt; from distilabel.tasks.preference.ultrafeedback import UltraFeedbackTask\n&gt;&gt;&gt; from distilabel.tasks.text_generation.llama import Llama2TextGenerationTask\n&gt;&gt;&gt; from distilabel.pipeline import Pipeline\n</code></pre> <pre><code>&gt;&gt;&gt; generator = TransformersLLM(\n...     model=\"meta-llama/Llama-2-7b-chat-hf\",\n...     tokenizer=\"meta-llama/Llama-2-7b-chat-hf\",\n...     task=Llama2TextGenerationTask(),\n... )\n&gt;&gt;&gt; labeller = OpenAILLM(\n...     model=\"gpt-3.5-turbo\",\n...     task=UltraFeedbackTask.for_text_quality(),\n... )\n&gt;&gt;&gt; pipeline = Pipeline(generator=generator, labeller=labeller)\n&gt;&gt;&gt; dataset = pipeline.generate(dataset=..., num_generations=1, batch_size=1)\n</code></pre> Source code in <code>src/distilabel/pipeline.py</code> <pre><code>@use_progress_bar\ndef generate(  # noqa: C901\n    self,\n    dataset: Dataset,\n    num_generations: int = 1,\n    batch_size: int = 1,\n    enable_checkpoints: bool = True,\n    display_progress_bar: bool = False,\n    verbose: bool = True,\n) -&gt; CustomDataset:\n    \"\"\"Generates the outputs for the given dataset using the LLMs provided to the `Pipeline`.\n\n    Args:\n        dataset (Dataset): the dataset to be used for generation.\n        num_generations (int, optional): the number of generations to be performed for each\n            input. Defaults to `1`.\n        batch_size (int, optional): the batch size to be used for generation. Defaults to `1`.\n        enable_checkpoints (bool, optional): whether to enable checkpoints or not. Defaults to `True`.\n        display_progress_bar (bool, optional): whether to display the progress bar or not. Defaults to `False`.\n        verbose (bool, optional): whether to display the logs or not. Defaults to `True`.\n\n    Returns:\n        CustomDataset: the final dataset.\n\n    Raises:\n        RuntimeError: if the `Pipeline` fails during the generation or labelling steps.\n        UserWarning: if the `Pipeline` fails during the generation or labelling steps and\n            `enable_checkpoints` is set to `False`.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.llm.huggingface import TransformersLLM\n        &gt;&gt;&gt; from distilabel.llm.openai_ import OpenAILLM\n        &gt;&gt;&gt; from distilabel.tasks.preference.ultrafeedback import UltraFeedbackTask\n        &gt;&gt;&gt; from distilabel.tasks.text_generation.llama import Llama2TextGenerationTask\n        &gt;&gt;&gt; from distilabel.pipeline import Pipeline\n\n        &gt;&gt;&gt; generator = TransformersLLM(\n        ...     model=\"meta-llama/Llama-2-7b-chat-hf\",\n        ...     tokenizer=\"meta-llama/Llama-2-7b-chat-hf\",\n        ...     task=Llama2TextGenerationTask(),\n        ... )\n        &gt;&gt;&gt; labeller = OpenAILLM(\n        ...     model=\"gpt-3.5-turbo\",\n        ...     task=UltraFeedbackTask.for_text_quality(),\n        ... )\n        &gt;&gt;&gt; pipeline = Pipeline(generator=generator, labeller=labeller)\n        &gt;&gt;&gt; dataset = pipeline.generate(dataset=..., num_generations=1, batch_size=1)\n    \"\"\"\n    if not verbose:\n        logger.setLevel(\"ERROR\")\n\n    if (\n        self.labeller is not None\n        and self.generator is not None\n        and num_generations &lt; 2\n    ):\n        warnings.warn(\n            f\"Provided `num_generations={num_generations}` which implies that the \"\n            \"`generator` LLM will just run once, while the `labelling` LLM expects \"\n            \"to recieve a list of N inputs to label, where N is &gt; 1. If this is not \"\n            \"intended, make sure to set `num_generations` to a value higher or \"\n            \"equal to 2.\",\n            UserWarning,\n            stacklevel=2,\n        )\n\n    self._validate_dataset(dataset)\n\n    generations: List[Dict[str, Any]] = []\n    batch_labels: Union[Future[List[\"LLMOutput\"]], List[\"LLMOutput\"]] = []\n\n    (\n        generation_progress_func,\n        labelling_progress_func,\n    ) = get_progress_bars_for_pipeline(\n        num_rows=len(dataset),\n        num_generations=num_generations,\n        display_progress_bar=display_progress_bar,\n    )\n\n    num_batches = math.ceil(len(dataset) / batch_size)\n\n    for batch_i, rows in enumerate(dataset.iter(batch_size=batch_size), start=1):\n        logger.info(f\"Processing batch {batch_i} of {num_batches}...\")\n        inputs = self._transform_dataset_to_expected_format(rows)  # type: ignore\n\n        if self.generator is not None:\n            logger.info(f\"Calling generator for batch {batch_i}...\")\n            try:\n                batch_generations = self._get_batch_generations(\n                    inputs, num_generations, generation_progress_func\n                )\n                generations.extend(batch_generations)\n            except Exception as e:\n                if not enable_checkpoints:\n                    raise RuntimeError(\n                        \"`Pipeline.generate` failed during generation step. Setting `enable_checkpoints=True` is recommended!\"\n                    ) from e\n                logger.error(\n                    f\"`Pipeline.generate` failed during generation step with exception: {e}\"\n                )\n                return self._build_dataset(\n                    dataset, generations=generations, batch_labels=batch_labels\n                )\n\n            inputs = self._include_generator_outputs_as_inputs(\n                inputs, batch_generations\n            )\n\n        if self.labeller is not None:\n            logger.info(f\"Calling labeller for batch {batch_i}...\")\n            try:\n                # TODO: move to `self._get_batch_labels` (without awaiting futures)\n                batch_labels.extend(\n                    self.labeller.generate(  # type: ignore\n                        inputs=inputs,\n                        # `num_generations` is always 1 because labelling the same input multiple times\n                        # using the same LLM may not make sense\n                        num_generations=1,\n                        progress_callback_func=labelling_progress_func,\n                    )\n                )\n            except Exception as e:\n                if not enable_checkpoints:\n                    raise RuntimeError(\n                        \"`Pipeline.generate` failed during labelling step. Setting `enable_checkpoints=True` is recommended!\"\n                    ) from e\n                logger.error(\n                    f\"`Pipeline.generate` failed during labelling step with exception: {e}\"\n                )\n                return self._build_dataset(\n                    dataset, generations=generations, batch_labels=batch_labels\n                )\n\n    _pipeline_progress.stop()\n\n    return self._build_dataset(\n        dataset, generations=generations, batch_labels=batch_labels\n    )\n</code></pre>"},{"location":"reference/distilabel/pipeline/#distilabel.pipeline.pipeline","title":"<code>pipeline(task, subtask=None, *, generator=None, labeller=None, **kwargs)</code>","text":"<p>Creates a <code>Pipeline</code> instance with the provided LLMs for a given task, which is useful whenever you want to use a pre-defined <code>Pipeline</code> for a given task, or if you want to create a custom <code>Pipeline</code> for a given task. Ideally one using this function over the <code>Pipeline</code> class, don't want to worry about the details of the <code>labeller</code>, since it will come with a default configuration based on the <code>task</code>, by default the LLM used for <code>labelling</code> will always be <code>gpt-3.5-turbo</code> from OpenAI, as it's the one that provides the most consistent and fast results.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Literal['preference', 'critique']</code> <p>the task to be performed by the <code>Pipeline</code>.</p> required <code>subtask</code> <code>Optional[str]</code> <p>the subtask to be performed by the <code>Pipeline</code>. Defaults to None.</p> <code>None</code> <code>generator</code> <code>Optional[LLM]</code> <p>the LLM to be used for generation. Defaults to None.</p> <code>None</code> <code>labeller</code> <code>Optional[LLM]</code> <p>the LLM to be used for labelling. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <p>the keyword arguments to be passed to the <code>task</code> and <code>subtask</code> classes.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if an invalid task is provided.</p> <p>Returns:</p> Name Type Description <code>Pipeline</code> <code>Pipeline</code> <p>the <code>Pipeline</code> instance.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.llm.huggingface import TransformersLLM\n&gt;&gt;&gt; from distilabel.tasks.text_generation.llama import Llama2TextGenerationTask\n&gt;&gt;&gt; from distilabel.pipeline import pipeline\n</code></pre> <pre><code>&gt;&gt;&gt; generator = TransformersLLM(\n...     model=\"meta-llama/Llama-2-7b-chat-hf\",\n...     tokenizer=\"meta-llama/Llama-2-7b-chat-hf\",\n...     task=Llama2TextGenerationTask(),\n... )\n&gt;&gt;&gt; pipeline = pipeline(\n...     task=\"preference\",\n...     subtask=\"text-quality\",\n...     generator=generator,\n... )\n</code></pre> Source code in <code>src/distilabel/pipeline.py</code> <pre><code>def pipeline(\n    task: Literal[\"preference\"],\n    subtask: Optional[str] = None,\n    *,\n    generator: Optional[\"LLM\"] = None,\n    labeller: Optional[\"LLM\"] = None,\n    **kwargs,\n) -&gt; Pipeline:\n    \"\"\"Creates a `Pipeline` instance with the provided LLMs for a given task, which is useful\n    whenever you want to use a pre-defined `Pipeline` for a given task, or if you want to\n    create a custom `Pipeline` for a given task. Ideally one using this function over the `Pipeline`\n    class, don't want to worry about the details of the `labeller`, since it will come with a default\n    configuration based on the `task`, by default the LLM used for `labelling` will always be `gpt-3.5-turbo`\n    from OpenAI, as it's the one that provides the most consistent and fast results.\n\n    Args:\n        task (Literal[\"preference\", \"critique\"]): the task to be performed by the `Pipeline`.\n        subtask (Optional[str], optional): the subtask to be performed by the `Pipeline`.\n            Defaults to None.\n        generator (Optional[\"LLM\"], optional): the LLM to be used for generation. Defaults to None.\n        labeller (Optional[\"LLM\"], optional): the LLM to be used for labelling. Defaults to None.\n        **kwargs: the keyword arguments to be passed to the `task` and `subtask` classes.\n\n    Raises:\n        ValueError: if an invalid task is provided.\n\n    Returns:\n        Pipeline: the `Pipeline` instance.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.llm.huggingface import TransformersLLM\n        &gt;&gt;&gt; from distilabel.tasks.text_generation.llama import Llama2TextGenerationTask\n        &gt;&gt;&gt; from distilabel.pipeline import pipeline\n\n        &gt;&gt;&gt; generator = TransformersLLM(\n        ...     model=\"meta-llama/Llama-2-7b-chat-hf\",\n        ...     tokenizer=\"meta-llama/Llama-2-7b-chat-hf\",\n        ...     task=Llama2TextGenerationTask(),\n        ... )\n        &gt;&gt;&gt; pipeline = pipeline(\n        ...     task=\"preference\",\n        ...     subtask=\"text-quality\",\n        ...     generator=generator,\n        ... )\n    \"\"\"\n    if task == \"preference\":\n        if labeller is None:\n            from dataclasses import fields\n\n            from distilabel.llm.openai import OpenAILLM\n            from distilabel.tasks.preference.ultrafeedback import UltraFeedbackTask\n\n            task_cls = UltraFeedbackTask\n            task_kwargs = {\n                key: kwargs.get(key.name)\n                for key in fields(task_cls)\n                if key.name in kwargs and not key.name.startswith(\"__\")\n            }\n\n            # Dynamically call the appropriate classmethod using getattr\n            if subtask is not None:\n                if subtask not in task_cls.__subtasks__:\n                    raise ValueError(\n                        f\"Invalid subtask: {subtask}, available subtasks are {task_cls.__subtasks__}\"\n                    )\n                classmethod_name = f\"for_{subtask.lower().replace('-', '_')}\"\n                if hasattr(task_cls, classmethod_name):\n                    task_cls = getattr(task_cls, classmethod_name)\n\n            logger.info(\n                \"Since no `labeller` was provided, `OpenAILLM` will be used as the default labeller with `UltraFeedback`.\"\n            )\n\n            labeller = OpenAILLM(\n                model=kwargs.get(\"openai_model\") or \"gpt-3.5-turbo\",\n                task=task_cls(**task_kwargs),  # type: ignore\n                max_new_tokens=kwargs.get(\"max_new_tokens\") or 256,\n                num_threads=kwargs.get(\"num_threads\") or 4,\n                openai_api_key=kwargs.get(\"openai_api_key\")\n                or os.getenv(\"OPENAI_API_KEY\"),\n                temperature=kwargs.get(\"temperature\") or 0.0,\n            )\n        else:\n            from distilabel.tasks.preference.judgelm import JudgeLMTask\n            from distilabel.tasks.preference.ultrafeedback import UltraFeedbackTask\n            from distilabel.tasks.preference.ultrajudge import UltraJudgeTask\n\n            if not isinstance(\n                labeller.task, (UltraFeedbackTask, JudgeLMTask, UltraJudgeTask)\n            ):\n                warnings.warn(\n                    \"The `labeller` task for `preference` must be an instance of `UltraFeedbackTask`,\"\n                    f\" `JudgeLMTask` or `UltraJudge`, got {labeller.task.__class__.__name__}.\"\n                    \"If you are planning to use a custom `labeller` for a `preference` \"\n                    \"task, use it at your own risk.\",\n                    UserWarning,\n                    stacklevel=2,\n                )\n\n        if generator is not None:\n            assert (\n                generator.task.input_args_names + generator.task.output_args_names\n                == labeller.task.input_args_names\n            ), (\n                f\"`generator` outputs do not match `labeller` inputs: \"\n                f\"{generator.task.input_args_names + generator.task.output_args_names} != {labeller.task.input_args_names}\"\n            )\n    else:\n        raise ValueError(f\"Invalid task: {task}, available tasks are: `preference`.\")\n\n    return Pipeline(generator=generator, labeller=labeller)\n</code></pre>"},{"location":"reference/distilabel/progress_bar/","title":"progress_bar","text":""},{"location":"reference/distilabel/llm/","title":"llm","text":""},{"location":"reference/distilabel/llm/#distilabel.llm.InferenceEndpointsLLM","title":"<code>InferenceEndpointsLLM</code>","text":"<p>             Bases: <code>LLM</code></p> Source code in <code>src/distilabel/llm/huggingface/inference_endpoints.py</code> <pre><code>class InferenceEndpointsLLM(LLM):\n    def __init__(\n        self,\n        endpoint_name: str,\n        task: \"Task\",\n        endpoint_namespace: Union[str, None] = None,\n        token: Union[str, None] = None,\n        max_new_tokens: int = 128,\n        repetition_penalty: Union[float, None] = None,\n        seed: Union[int, None] = None,\n        do_sample: bool = False,\n        temperature: Union[float, None] = None,\n        top_k: Union[int, None] = None,\n        top_p: Union[float, None] = None,\n        typical_p: Union[float, None] = None,\n        num_threads: Union[int, None] = None,\n        prompt_format: Union[\"SupportedFormats\", None] = None,\n        prompt_formatting_fn: Union[Callable[..., str], None] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the InferenceEndpointsLLM class.\n\n        Args:\n            endpoint_name (str): The name of the endpoint.\n            task (Task): The task to be performed by the LLM.\n            endpoint_namespace (Union[str, None]): The namespace of the endpoint. Defaults to None.\n            token (Union[str, None]): The token for the endpoint. Defaults to None.\n            max_new_tokens (int): The maximum number of tokens to be generated. Defaults to 128.\n            repetition_penalty (Union[float, None]): The repetition penalty to be used for generation. Defaults to None.\n            seed (Union[int, None]): The seed for generation. Defaults to None.\n            do_sample (bool): Whether to do sampling. Defaults to False.\n            temperature (Union[float, None]): The temperature for generation. Defaults to None.\n            top_k (Union[int, None]): The top_k for generation. Defaults to None.\n            top_p (Union[float, None]): The top_p for generation. Defaults to None.\n            typical_p (Union[float, None]): The typical_p for generation. Defaults to None.\n            num_threads (Union[int, None]): The number of threads. Defaults to None.\n            prompt_format (Union[\"SupportedFormats\", None]): The format of the prompt. Defaults to None.\n            prompt_formatting_fn (Union[Callable[..., str], None]): The function for formatting the prompt. Defaults to None.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask as Task\n            &gt;&gt;&gt; from distilabel.llm import InferenceEndpointsLLM\n            &gt;&gt;&gt; task = Task()\n            &gt;&gt;&gt; llm = InferenceEndpointsLLM(\n            ...     endpoint_name=\"&lt;INFERENCE_ENDPOINT_NAME&gt;\",\n            ...     task=task,\n            ... )\n        \"\"\"\n        super().__init__(\n            task=task,\n            num_threads=num_threads,\n            prompt_format=prompt_format,\n            prompt_formatting_fn=prompt_formatting_fn,\n        )\n\n        if not _HUGGINGFACE_HUB_AVAILABLE:\n            raise ImportError(\n                \"`InferenceEndpointsLLM` cannot be used as `huggingface-hub` is not \"\n                \"installed, please install it with `pip install huggingface-hub`.\"\n            )\n\n        self.do_sample = do_sample\n        self.max_new_tokens = max_new_tokens\n        self.repetition_penalty = repetition_penalty\n        self.seed = seed\n        self.temperature = temperature\n        self.top_k = top_k\n        self.top_p = top_p\n        self.typical_p = typical_p\n\n        self.inference_endpoint = get_inference_endpoint(\n            name=endpoint_name, namespace=endpoint_namespace, token=token\n        )\n        self.inference_endpoint.wait(timeout=30)\n\n    def __rich_repr__(self) -&gt; Generator[Any, None, None]:\n        yield from super().__rich_repr__()\n        yield (\n            \"parameters\",\n            {\n                \"do_sample\": self.do_sample,\n                \"max_new_tokens\": self.max_new_tokens,\n                \"repetition_penalty\": self.repetition_penalty,\n                \"seed\": self.seed,\n                \"temperature\": self.temperature,\n                \"top_k\": self.top_k,\n                \"top_p\": self.top_p,\n                \"typical_p\": self.typical_p,\n            },\n        )\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name of the endpoint.\"\"\"\n        return self.inference_endpoint.repository\n\n    @retry(\n        retry=retry_if_exception_type(_INFERENCE_ENDPOINTS_API_RETRY_ON_EXCEPTIONS),\n        stop=stop_after_attempt(_INFERENCE_ENDPOINTS_API_STOP_AFTER_ATTEMPT),\n        wait=wait_random_exponential(\n            multiplier=_INFERENCE_ENDPOINTS_API_WAIT_RANDOM_EXPONENTIAL_MULTIPLIER,\n            max=_INFERENCE_ENDPOINTS_API_WAIT_RANDOM_EXPONENTIAL_MAX,\n        ),\n        before_sleep=before_sleep_log(logger, logging.INFO),\n        after=after_log(logger, logging.INFO),\n    )\n    def _text_generation_with_backoff(self, **kwargs: Any) -&gt; Any:\n        \"\"\"Performs text generation with backoff in case of an error.\"\"\"\n        return self.inference_endpoint.client.text_generation(**kwargs)  # type: ignore\n\n    def _generate(\n        self, inputs: List[Dict[str, Any]], num_generations: int = 1\n    ) -&gt; List[List[LLMOutput]]:\n        \"\"\"Generates `num_generations` for each input in `inputs`.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n            num_generations (int, optional): the number of generations to be performed for each\n                input. Defaults to 1.\n\n        Returns:\n            List[List[LLMOutput]]: the outputs of the LLM.\n        \"\"\"\n        prompts = self._generate_prompts(\n            inputs, default_format=None, expected_output_type=str\n        )\n        outputs = []\n        for prompt in prompts:\n            raw_responses = [\n                self._text_generation_with_backoff(\n                    prompt=prompt,\n                    do_sample=self.do_sample,\n                    max_new_tokens=self.max_new_tokens,\n                    repetition_penalty=self.repetition_penalty,\n                    seed=self.seed,\n                    temperature=self.temperature,\n                    top_k=self.top_k,\n                    top_p=self.top_p,\n                    typical_p=self.typical_p,\n                )\n                for _ in range(num_generations)\n            ]\n            output = []\n            for raw_response in raw_responses:\n                try:\n                    parsed_response = self.task.parse_output(raw_response)\n                except Exception as e:\n                    logger.error(f\"Error parsing Inference Endpoints output: {e}\")\n                    parsed_response = None\n                output.append(\n                    LLMOutput(\n                        model_name=self.model_name,\n                        prompt_used=prompt,\n                        raw_output=raw_response,\n                        parsed_output=parsed_response,\n                    )\n                )\n            outputs.append(output)\n        return outputs\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.InferenceEndpointsLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the model name of the endpoint.</p>"},{"location":"reference/distilabel/llm/#distilabel.llm.InferenceEndpointsLLM.__init__","title":"<code>__init__(endpoint_name, task, endpoint_namespace=None, token=None, max_new_tokens=128, repetition_penalty=None, seed=None, do_sample=False, temperature=None, top_k=None, top_p=None, typical_p=None, num_threads=None, prompt_format=None, prompt_formatting_fn=None)</code>","text":"<p>Initializes the InferenceEndpointsLLM class.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint_name</code> <code>str</code> <p>The name of the endpoint.</p> required <code>task</code> <code>Task</code> <p>The task to be performed by the LLM.</p> required <code>endpoint_namespace</code> <code>Union[str, None]</code> <p>The namespace of the endpoint. Defaults to None.</p> <code>None</code> <code>token</code> <code>Union[str, None]</code> <p>The token for the endpoint. Defaults to None.</p> <code>None</code> <code>max_new_tokens</code> <code>int</code> <p>The maximum number of tokens to be generated. Defaults to 128.</p> <code>128</code> <code>repetition_penalty</code> <code>Union[float, None]</code> <p>The repetition penalty to be used for generation. Defaults to None.</p> <code>None</code> <code>seed</code> <code>Union[int, None]</code> <p>The seed for generation. Defaults to None.</p> <code>None</code> <code>do_sample</code> <code>bool</code> <p>Whether to do sampling. Defaults to False.</p> <code>False</code> <code>temperature</code> <code>Union[float, None]</code> <p>The temperature for generation. Defaults to None.</p> <code>None</code> <code>top_k</code> <code>Union[int, None]</code> <p>The top_k for generation. Defaults to None.</p> <code>None</code> <code>top_p</code> <code>Union[float, None]</code> <p>The top_p for generation. Defaults to None.</p> <code>None</code> <code>typical_p</code> <code>Union[float, None]</code> <p>The typical_p for generation. Defaults to None.</p> <code>None</code> <code>num_threads</code> <code>Union[int, None]</code> <p>The number of threads. Defaults to None.</p> <code>None</code> <code>prompt_format</code> <code>Union[SupportedFormats, None]</code> <p>The format of the prompt. Defaults to None.</p> <code>None</code> <code>prompt_formatting_fn</code> <code>Union[Callable[..., str], None]</code> <p>The function for formatting the prompt. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask as Task\n&gt;&gt;&gt; from distilabel.llm import InferenceEndpointsLLM\n&gt;&gt;&gt; task = Task()\n&gt;&gt;&gt; llm = InferenceEndpointsLLM(\n...     endpoint_name=\"&lt;INFERENCE_ENDPOINT_NAME&gt;\",\n...     task=task,\n... )\n</code></pre> Source code in <code>src/distilabel/llm/huggingface/inference_endpoints.py</code> <pre><code>def __init__(\n    self,\n    endpoint_name: str,\n    task: \"Task\",\n    endpoint_namespace: Union[str, None] = None,\n    token: Union[str, None] = None,\n    max_new_tokens: int = 128,\n    repetition_penalty: Union[float, None] = None,\n    seed: Union[int, None] = None,\n    do_sample: bool = False,\n    temperature: Union[float, None] = None,\n    top_k: Union[int, None] = None,\n    top_p: Union[float, None] = None,\n    typical_p: Union[float, None] = None,\n    num_threads: Union[int, None] = None,\n    prompt_format: Union[\"SupportedFormats\", None] = None,\n    prompt_formatting_fn: Union[Callable[..., str], None] = None,\n) -&gt; None:\n    \"\"\"Initializes the InferenceEndpointsLLM class.\n\n    Args:\n        endpoint_name (str): The name of the endpoint.\n        task (Task): The task to be performed by the LLM.\n        endpoint_namespace (Union[str, None]): The namespace of the endpoint. Defaults to None.\n        token (Union[str, None]): The token for the endpoint. Defaults to None.\n        max_new_tokens (int): The maximum number of tokens to be generated. Defaults to 128.\n        repetition_penalty (Union[float, None]): The repetition penalty to be used for generation. Defaults to None.\n        seed (Union[int, None]): The seed for generation. Defaults to None.\n        do_sample (bool): Whether to do sampling. Defaults to False.\n        temperature (Union[float, None]): The temperature for generation. Defaults to None.\n        top_k (Union[int, None]): The top_k for generation. Defaults to None.\n        top_p (Union[float, None]): The top_p for generation. Defaults to None.\n        typical_p (Union[float, None]): The typical_p for generation. Defaults to None.\n        num_threads (Union[int, None]): The number of threads. Defaults to None.\n        prompt_format (Union[\"SupportedFormats\", None]): The format of the prompt. Defaults to None.\n        prompt_formatting_fn (Union[Callable[..., str], None]): The function for formatting the prompt. Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask as Task\n        &gt;&gt;&gt; from distilabel.llm import InferenceEndpointsLLM\n        &gt;&gt;&gt; task = Task()\n        &gt;&gt;&gt; llm = InferenceEndpointsLLM(\n        ...     endpoint_name=\"&lt;INFERENCE_ENDPOINT_NAME&gt;\",\n        ...     task=task,\n        ... )\n    \"\"\"\n    super().__init__(\n        task=task,\n        num_threads=num_threads,\n        prompt_format=prompt_format,\n        prompt_formatting_fn=prompt_formatting_fn,\n    )\n\n    if not _HUGGINGFACE_HUB_AVAILABLE:\n        raise ImportError(\n            \"`InferenceEndpointsLLM` cannot be used as `huggingface-hub` is not \"\n            \"installed, please install it with `pip install huggingface-hub`.\"\n        )\n\n    self.do_sample = do_sample\n    self.max_new_tokens = max_new_tokens\n    self.repetition_penalty = repetition_penalty\n    self.seed = seed\n    self.temperature = temperature\n    self.top_k = top_k\n    self.top_p = top_p\n    self.typical_p = typical_p\n\n    self.inference_endpoint = get_inference_endpoint(\n        name=endpoint_name, namespace=endpoint_namespace, token=token\n    )\n    self.inference_endpoint.wait(timeout=30)\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.LlamaCppLLM","title":"<code>LlamaCppLLM</code>","text":"<p>             Bases: <code>LLM</code></p> Source code in <code>src/distilabel/llm/llama_cpp.py</code> <pre><code>class LlamaCppLLM(LLM):\n    def __init__(\n        self,\n        model: \"Llama\",\n        task: \"Task\",\n        max_new_tokens: int = 128,\n        temperature: float = 0.8,\n        top_p: float = 0.95,\n        top_k: int = 40,\n        repeat_penalty: float = 1.1,\n        prompt_format: Union[SupportedFormats, None] = None,\n        prompt_formatting_fn: Union[Callable[..., str], None] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the LlamaCppLLM class.\n\n        Args:\n            model (Llama): the llama-cpp model to be used.\n            task (Task): the task to be performed by the LLM.\n            max_new_tokens (int, optional): the maximum number of tokens to be generated.\n                Defaults to 128.\n            temperature (float, optional): the temperature to be used for generation.\n                Defaults to 0.8.\n            top_p (float, optional): the top-p value to be used for generation.\n                Defaults to 0.95.\n            top_k (int, optional): the top-k value to be used for generation.\n                Defaults to 40.\n            repeat_penalty (float, optional): the repeat penalty to be used for generation.\n                Defaults to 1.1.\n            prompt_format (Union[SupportedFormats, None], optional): the format to be used\n                for the prompt. If `None`, the default format of the task will be used, available\n                formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n                but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n                will be used if no `prompt_formatting_fn` is provided.\n            prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n                applied to the prompt before generation. If `None`, no formatting will be applied.\n                Defaults to `None`.\n\n        Examples:\n            &gt;&gt;&gt; from llama_cpp import Llama\n            &gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask as Task\n            &gt;&gt;&gt; from distilabel.llm import LlamaCppLLM\n            &gt;&gt;&gt; model = Llama(model_path=\"path/to/model\")\n            &gt;&gt;&gt; task = Task()\n            &gt;&gt;&gt; llm = LlamaCppLLM(model=model, task=task)\n        \"\"\"\n        super().__init__(\n            task=task,\n            prompt_format=prompt_format,\n            prompt_formatting_fn=prompt_formatting_fn,\n        )\n\n        if not _LLAMA_CPP_AVAILABLE:\n            raise ImportError(\n                \"`LlamaCppLLM` cannot be used as `llama_cpp` is not installed, please \"\n                \" install it with `pip install llama-cpp-python`.\"\n            )\n\n        self.max_tokens = max_new_tokens\n        self.temperature = temperature\n        self.top_p = top_p\n        self.top_k = top_k\n        self.repeat_penalty = repeat_penalty\n\n        self.model = model\n\n    def __rich_repr__(self) -&gt; Generator[Any, None, None]:\n        yield from super().__rich_repr__()\n        yield (\n            \"parameters\",\n            {\n                \"max_new_tokens\": self.max_tokens,\n                \"temperature\": self.temperature,\n                \"top_p\": self.top_p,\n                \"top_k\": self.top_k,\n                \"repeat_penalty\": self.repeat_penalty,\n            },\n        )\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the name of the llama-cpp model, which is the same as the model path.\"\"\"\n        return self.model.model_path\n\n    def _generate(\n        self, inputs: List[Dict[str, Any]], num_generations: int = 1\n    ) -&gt; List[List[LLMOutput]]:\n        \"\"\"Generates `num_generations` for each input in `inputs`.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n            num_generations (int, optional): the number of generations to be performed for each\n                input. Defaults to 1.\n\n        Returns:\n            List[List[LLMOutput]]: the generated outputs.\n        \"\"\"\n        prompts = self._generate_prompts(\n            inputs, default_format=None, expected_output_type=str\n        )\n        outputs = []\n        for prompt in prompts:\n            output = []\n            for _ in range(num_generations):\n                raw_output = self.model.create_completion(\n                    prompt,\n                    max_tokens=self.max_tokens,\n                    temperature=self.temperature,\n                    top_p=self.top_p,\n                    top_k=self.top_k,\n                    repeat_penalty=self.repeat_penalty,\n                )\n                try:\n                    parsed_output = self.task.parse_output(\n                        raw_output[\"choices\"][0][\"text\"].strip()\n                    )\n                except Exception as e:\n                    logger.error(f\"Error parsing llama-cpp output: {e}\")\n                    parsed_output = None\n                output.append(\n                    LLMOutput(\n                        model_name=self.model_name,\n                        prompt_used=prompt,\n                        raw_output=raw_output,\n                        parsed_output=parsed_output,\n                    )\n                )\n            outputs.append(output)\n        return outputs\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.LlamaCppLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the name of the llama-cpp model, which is the same as the model path.</p>"},{"location":"reference/distilabel/llm/#distilabel.llm.LlamaCppLLM.__init__","title":"<code>__init__(model, task, max_new_tokens=128, temperature=0.8, top_p=0.95, top_k=40, repeat_penalty=1.1, prompt_format=None, prompt_formatting_fn=None)</code>","text":"<p>Initializes the LlamaCppLLM class.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Llama</code> <p>the llama-cpp model to be used.</p> required <code>task</code> <code>Task</code> <p>the task to be performed by the LLM.</p> required <code>max_new_tokens</code> <code>int</code> <p>the maximum number of tokens to be generated. Defaults to 128.</p> <code>128</code> <code>temperature</code> <code>float</code> <p>the temperature to be used for generation. Defaults to 0.8.</p> <code>0.8</code> <code>top_p</code> <code>float</code> <p>the top-p value to be used for generation. Defaults to 0.95.</p> <code>0.95</code> <code>top_k</code> <code>int</code> <p>the top-k value to be used for generation. Defaults to 40.</p> <code>40</code> <code>repeat_penalty</code> <code>float</code> <p>the repeat penalty to be used for generation. Defaults to 1.1.</p> <code>1.1</code> <code>prompt_format</code> <code>Union[SupportedFormats, None]</code> <p>the format to be used for the prompt. If <code>None</code>, the default format of the task will be used, available formats are <code>openai</code>, <code>chatml</code>, <code>llama2</code>, <code>zephyr</code>, and <code>default</code>. Defaults to <code>None</code>, but <code>default</code> (concatenation of <code>system_prompt</code> and <code>formatted_prompt</code> with a line-break) will be used if no <code>prompt_formatting_fn</code> is provided.</p> <code>None</code> <code>prompt_formatting_fn</code> <code>Union[Callable[..., str], None]</code> <p>a function to be applied to the prompt before generation. If <code>None</code>, no formatting will be applied. Defaults to <code>None</code>.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from llama_cpp import Llama\n&gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask as Task\n&gt;&gt;&gt; from distilabel.llm import LlamaCppLLM\n&gt;&gt;&gt; model = Llama(model_path=\"path/to/model\")\n&gt;&gt;&gt; task = Task()\n&gt;&gt;&gt; llm = LlamaCppLLM(model=model, task=task)\n</code></pre> Source code in <code>src/distilabel/llm/llama_cpp.py</code> <pre><code>def __init__(\n    self,\n    model: \"Llama\",\n    task: \"Task\",\n    max_new_tokens: int = 128,\n    temperature: float = 0.8,\n    top_p: float = 0.95,\n    top_k: int = 40,\n    repeat_penalty: float = 1.1,\n    prompt_format: Union[SupportedFormats, None] = None,\n    prompt_formatting_fn: Union[Callable[..., str], None] = None,\n) -&gt; None:\n    \"\"\"Initializes the LlamaCppLLM class.\n\n    Args:\n        model (Llama): the llama-cpp model to be used.\n        task (Task): the task to be performed by the LLM.\n        max_new_tokens (int, optional): the maximum number of tokens to be generated.\n            Defaults to 128.\n        temperature (float, optional): the temperature to be used for generation.\n            Defaults to 0.8.\n        top_p (float, optional): the top-p value to be used for generation.\n            Defaults to 0.95.\n        top_k (int, optional): the top-k value to be used for generation.\n            Defaults to 40.\n        repeat_penalty (float, optional): the repeat penalty to be used for generation.\n            Defaults to 1.1.\n        prompt_format (Union[SupportedFormats, None], optional): the format to be used\n            for the prompt. If `None`, the default format of the task will be used, available\n            formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n            but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n            will be used if no `prompt_formatting_fn` is provided.\n        prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n            applied to the prompt before generation. If `None`, no formatting will be applied.\n            Defaults to `None`.\n\n    Examples:\n        &gt;&gt;&gt; from llama_cpp import Llama\n        &gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask as Task\n        &gt;&gt;&gt; from distilabel.llm import LlamaCppLLM\n        &gt;&gt;&gt; model = Llama(model_path=\"path/to/model\")\n        &gt;&gt;&gt; task = Task()\n        &gt;&gt;&gt; llm = LlamaCppLLM(model=model, task=task)\n    \"\"\"\n    super().__init__(\n        task=task,\n        prompt_format=prompt_format,\n        prompt_formatting_fn=prompt_formatting_fn,\n    )\n\n    if not _LLAMA_CPP_AVAILABLE:\n        raise ImportError(\n            \"`LlamaCppLLM` cannot be used as `llama_cpp` is not installed, please \"\n            \" install it with `pip install llama-cpp-python`.\"\n        )\n\n    self.max_tokens = max_new_tokens\n    self.temperature = temperature\n    self.top_p = top_p\n    self.top_k = top_k\n    self.repeat_penalty = repeat_penalty\n\n    self.model = model\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.OpenAILLM","title":"<code>OpenAILLM</code>","text":"<p>             Bases: <code>LLM</code></p> Source code in <code>src/distilabel/llm/openai.py</code> <pre><code>class OpenAILLM(LLM):\n    def __init__(\n        self,\n        task: \"Task\",\n        model: str = \"gpt-3.5-turbo\",\n        client: Union[\"OpenAI\", None] = None,\n        openai_api_key: Union[str, None] = None,\n        max_new_tokens: int = 128,\n        frequency_penalty: float = 0.0,\n        presence_penalty: float = 0.0,\n        temperature: float = 1.0,\n        top_p: float = 1.0,\n        num_threads: Union[int, None] = None,\n        prompt_format: Union[\"SupportedFormats\", None] = None,\n        prompt_formatting_fn: Union[Callable[..., str], None] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the OpenAILLM class.\n\n        Args:\n            task (Task): the task to be performed by the LLM.\n            model (str, optional): the model to be used for generation. Defaults to \"gpt-3.5-turbo\".\n            client (Union[OpenAI, None], optional): an OpenAI client to be used for generation.\n                If `None`, a new client will be created. Defaults to `None`.\n            openai_api_key (Union[str, None], optional): the OpenAI API key to be used for generation.\n                If `None`, the `OPENAI_API_KEY` environment variable will be used. Defaults to `None`.\n            max_new_tokens (int, optional): the maximum number of tokens to be generated.\n                Defaults to 128.\n            frequency_penalty (float, optional): the frequency penalty to be used for generation.\n                Defaults to 0.0.\n            presence_penalty (float, optional): the presence penalty to be used for generation.\n                Defaults to 0.0.\n            temperature (float, optional): the temperature to be used for generation.\n                Defaults to 1.0.\n            top_p (float, optional): the top-p value to be used for generation.\n                Defaults to 1.0.\n            num_threads (Union[int, None], optional): the number of threads to be used\n                for parallel generation. If `None`, no parallel generation will be performed.\n                Defaults to `None`.\n            prompt_format (Union[SupportedFormats, None], optional): the format to be used\n                for the prompt. If `None`, the default format of the task will be used, available\n                formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n                but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n                will be used if no `prompt_formatting_fn` is provided.\n            prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n                applied to the prompt before generation. If `None`, no formatting will be applied.\n                Defaults to `None`.\n\n        Raises:\n            AssertionError: if the provided `model` is not available in your OpenAI account.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask as Task\n            &gt;&gt;&gt; from distilabel.llm import OpenAILLM\n            &gt;&gt;&gt; task = Task()\n            &gt;&gt;&gt; llm = OpenAILLM(model=\"gpt-3.5-turbo\", task=task)\n        \"\"\"\n        super().__init__(\n            task=task,\n            num_threads=num_threads,\n            prompt_format=prompt_format,\n            prompt_formatting_fn=prompt_formatting_fn,\n        )\n\n        if not _OPENAI_AVAILABLE:\n            raise ImportError(\n                \"`OpenAILLM` cannot be used as `openai` is not installed, please \"\n                \" install it with `pip install openai`.\"\n            )\n\n        self.max_tokens = max_new_tokens\n        self.frequency_penalty = frequency_penalty\n        self.presence_penalty = presence_penalty\n        self.temperature = temperature\n        self.top_p = top_p\n\n        self.client = client or OpenAI(api_key=openai_api_key, max_retries=6)\n\n        assert (\n            model in self.available_models\n        ), f\"Provided `model` is not available in your OpenAI account, available models are {self.available_models}\"\n        self.model = model\n\n    def __rich_repr__(self) -&gt; Generator[Any, None, None]:\n        yield from super().__rich_repr__()\n        yield (\n            \"parameters\",\n            {\n                \"max_tokens\": self.max_tokens,\n                \"frequency_penalty\": self.frequency_penalty,\n                \"presence_penalty\": self.presence_penalty,\n                \"temperature\": self.temperature,\n                \"top_p\": self.top_p,\n            },\n        )\n\n    @cached_property\n    def available_models(self) -&gt; List[str]:\n        \"\"\"Returns the list of available models in your OpenAI account.\"\"\"\n        return [model.id for model in self.client.models.list().data]\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the name of the OpenAI model.\"\"\"\n        return self.model\n\n    def _generate(\n        self,\n        inputs: List[Dict[str, Any]],\n        num_generations: int = 1,\n    ) -&gt; List[List[LLMOutput]]:\n        \"\"\"Generates `num_generations` for each input in `inputs`.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n            num_generations (int, optional): the number of generations to be performed for each\n                input. Defaults to 1.\n\n        Returns:\n            List[List[LLMOutput]]: the generated outputs.\n        \"\"\"\n        prompts = self._generate_prompts(\n            inputs, default_format=\"openai\", expected_output_type=list\n        )\n        outputs = []\n        for prompt in prompts:\n            chat_completions = self.client.chat.completions.create(\n                messages=prompt,\n                model=self.model,\n                n=num_generations,\n                max_tokens=self.max_tokens,\n                frequency_penalty=self.frequency_penalty,\n                presence_penalty=self.presence_penalty,\n                temperature=self.temperature,\n                top_p=self.top_p,\n                timeout=50,\n            )\n\n            output = []\n            for chat_completion in chat_completions.choices:\n                try:\n                    parsed_response = self.task.parse_output(\n                        chat_completion.message.content.strip()\n                    )\n                except Exception as e:\n                    logger.error(f\"Error parsing OpenAI response: {e}\")\n                    parsed_response = None\n                output.append(\n                    LLMOutput(\n                        model_name=self.model_name,\n                        prompt_used=prompt,\n                        raw_output=chat_completion.message.content,\n                        parsed_output=parsed_response,\n                    )\n                )\n            outputs.append(output)\n        return outputs\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.OpenAILLM.available_models","title":"<code>available_models: List[str]</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the list of available models in your OpenAI account.</p>"},{"location":"reference/distilabel/llm/#distilabel.llm.OpenAILLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the name of the OpenAI model.</p>"},{"location":"reference/distilabel/llm/#distilabel.llm.OpenAILLM.__init__","title":"<code>__init__(task, model='gpt-3.5-turbo', client=None, openai_api_key=None, max_new_tokens=128, frequency_penalty=0.0, presence_penalty=0.0, temperature=1.0, top_p=1.0, num_threads=None, prompt_format=None, prompt_formatting_fn=None)</code>","text":"<p>Initializes the OpenAILLM class.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task</code> <p>the task to be performed by the LLM.</p> required <code>model</code> <code>str</code> <p>the model to be used for generation. Defaults to \"gpt-3.5-turbo\".</p> <code>'gpt-3.5-turbo'</code> <code>client</code> <code>Union[OpenAI, None]</code> <p>an OpenAI client to be used for generation. If <code>None</code>, a new client will be created. Defaults to <code>None</code>.</p> <code>None</code> <code>openai_api_key</code> <code>Union[str, None]</code> <p>the OpenAI API key to be used for generation. If <code>None</code>, the <code>OPENAI_API_KEY</code> environment variable will be used. Defaults to <code>None</code>.</p> <code>None</code> <code>max_new_tokens</code> <code>int</code> <p>the maximum number of tokens to be generated. Defaults to 128.</p> <code>128</code> <code>frequency_penalty</code> <code>float</code> <p>the frequency penalty to be used for generation. Defaults to 0.0.</p> <code>0.0</code> <code>presence_penalty</code> <code>float</code> <p>the presence penalty to be used for generation. Defaults to 0.0.</p> <code>0.0</code> <code>temperature</code> <code>float</code> <p>the temperature to be used for generation. Defaults to 1.0.</p> <code>1.0</code> <code>top_p</code> <code>float</code> <p>the top-p value to be used for generation. Defaults to 1.0.</p> <code>1.0</code> <code>num_threads</code> <code>Union[int, None]</code> <p>the number of threads to be used for parallel generation. If <code>None</code>, no parallel generation will be performed. Defaults to <code>None</code>.</p> <code>None</code> <code>prompt_format</code> <code>Union[SupportedFormats, None]</code> <p>the format to be used for the prompt. If <code>None</code>, the default format of the task will be used, available formats are <code>openai</code>, <code>chatml</code>, <code>llama2</code>, <code>zephyr</code>, and <code>default</code>. Defaults to <code>None</code>, but <code>default</code> (concatenation of <code>system_prompt</code> and <code>formatted_prompt</code> with a line-break) will be used if no <code>prompt_formatting_fn</code> is provided.</p> <code>None</code> <code>prompt_formatting_fn</code> <code>Union[Callable[..., str], None]</code> <p>a function to be applied to the prompt before generation. If <code>None</code>, no formatting will be applied. Defaults to <code>None</code>.</p> <code>None</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>if the provided <code>model</code> is not available in your OpenAI account.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask as Task\n&gt;&gt;&gt; from distilabel.llm import OpenAILLM\n&gt;&gt;&gt; task = Task()\n&gt;&gt;&gt; llm = OpenAILLM(model=\"gpt-3.5-turbo\", task=task)\n</code></pre> Source code in <code>src/distilabel/llm/openai.py</code> <pre><code>def __init__(\n    self,\n    task: \"Task\",\n    model: str = \"gpt-3.5-turbo\",\n    client: Union[\"OpenAI\", None] = None,\n    openai_api_key: Union[str, None] = None,\n    max_new_tokens: int = 128,\n    frequency_penalty: float = 0.0,\n    presence_penalty: float = 0.0,\n    temperature: float = 1.0,\n    top_p: float = 1.0,\n    num_threads: Union[int, None] = None,\n    prompt_format: Union[\"SupportedFormats\", None] = None,\n    prompt_formatting_fn: Union[Callable[..., str], None] = None,\n) -&gt; None:\n    \"\"\"Initializes the OpenAILLM class.\n\n    Args:\n        task (Task): the task to be performed by the LLM.\n        model (str, optional): the model to be used for generation. Defaults to \"gpt-3.5-turbo\".\n        client (Union[OpenAI, None], optional): an OpenAI client to be used for generation.\n            If `None`, a new client will be created. Defaults to `None`.\n        openai_api_key (Union[str, None], optional): the OpenAI API key to be used for generation.\n            If `None`, the `OPENAI_API_KEY` environment variable will be used. Defaults to `None`.\n        max_new_tokens (int, optional): the maximum number of tokens to be generated.\n            Defaults to 128.\n        frequency_penalty (float, optional): the frequency penalty to be used for generation.\n            Defaults to 0.0.\n        presence_penalty (float, optional): the presence penalty to be used for generation.\n            Defaults to 0.0.\n        temperature (float, optional): the temperature to be used for generation.\n            Defaults to 1.0.\n        top_p (float, optional): the top-p value to be used for generation.\n            Defaults to 1.0.\n        num_threads (Union[int, None], optional): the number of threads to be used\n            for parallel generation. If `None`, no parallel generation will be performed.\n            Defaults to `None`.\n        prompt_format (Union[SupportedFormats, None], optional): the format to be used\n            for the prompt. If `None`, the default format of the task will be used, available\n            formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n            but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n            will be used if no `prompt_formatting_fn` is provided.\n        prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n            applied to the prompt before generation. If `None`, no formatting will be applied.\n            Defaults to `None`.\n\n    Raises:\n        AssertionError: if the provided `model` is not available in your OpenAI account.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask as Task\n        &gt;&gt;&gt; from distilabel.llm import OpenAILLM\n        &gt;&gt;&gt; task = Task()\n        &gt;&gt;&gt; llm = OpenAILLM(model=\"gpt-3.5-turbo\", task=task)\n    \"\"\"\n    super().__init__(\n        task=task,\n        num_threads=num_threads,\n        prompt_format=prompt_format,\n        prompt_formatting_fn=prompt_formatting_fn,\n    )\n\n    if not _OPENAI_AVAILABLE:\n        raise ImportError(\n            \"`OpenAILLM` cannot be used as `openai` is not installed, please \"\n            \" install it with `pip install openai`.\"\n        )\n\n    self.max_tokens = max_new_tokens\n    self.frequency_penalty = frequency_penalty\n    self.presence_penalty = presence_penalty\n    self.temperature = temperature\n    self.top_p = top_p\n\n    self.client = client or OpenAI(api_key=openai_api_key, max_retries=6)\n\n    assert (\n        model in self.available_models\n    ), f\"Provided `model` is not available in your OpenAI account, available models are {self.available_models}\"\n    self.model = model\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.TransformersLLM","title":"<code>TransformersLLM</code>","text":"<p>             Bases: <code>LLM</code></p> Source code in <code>src/distilabel/llm/huggingface/transformers.py</code> <pre><code>class TransformersLLM(LLM):\n    def __init__(\n        self,\n        model: \"PreTrainedModel\",\n        tokenizer: \"PreTrainedTokenizer\",\n        task: \"Task\",\n        max_new_tokens: int = 128,\n        do_sample: bool = False,\n        temperature: float = 1.0,\n        top_k: int = 50,\n        top_p: float = 1.0,\n        typical_p: float = 1.0,\n        num_threads: Union[int, None] = None,\n        prompt_format: Union[\"SupportedFormats\", None] = None,\n        prompt_formatting_fn: Union[Callable[..., str], None] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the TransformersLLM class.\n\n        Args:\n            model (PreTrainedModel): the model to be used for generation.\n            tokenizer (PreTrainedTokenizer): the tokenizer to be used for generation.\n            task (Task): the task to be performed by the LLM.\n            max_new_tokens (int, optional): the maximum number of tokens to be generated.\n                Defaults to 128.\n            do_sample (bool, optional): whether to sample from the model or not.\n                Defaults to False.\n            temperature (float, optional): the temperature to be used for generation.\n                Defaults to 1.0.\n            top_k (int, optional): the top-k value to be used for generation.\n                Defaults to 50.\n            top_p (float, optional): the top-p value to be used for generation.\n                Defaults to 1.0.\n            typical_p (float, optional): the typical-p value to be used for generation.\n                Defaults to 1.0.\n            num_threads (Union[int, None], optional): the number of threads to be used for generation.\n                If `None`, the number of threads will be set to the number of available CPUs.\n                Defaults to `None`.\n            prompt_format (Union[SupportedFormats, None], optional): the format to be used\n                for formatting the prompts. If `None`, the prompts will not be formatted.\n                Defaults to `None`.\n            prompt_formatting_fn (Union[Callable[..., str], None], optional): the function to be used\n                for formatting the prompts. If `None`, the prompts will not be formatted.\n\n        Examples:\n            &gt;&gt;&gt; from transformers import AutoModelForCausalLM, AutoTokenizer\n            &gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask as Task\n            &gt;&gt;&gt; from distilabel.llm import TransformersLLM\n            &gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n            &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n            &gt;&gt;&gt; task = Task()\n            &gt;&gt;&gt; llm = TransformersLLM(\n            ...     model=model,\n            ...     tokenizer=tokenizer,\n            ...     task=task,\n            ... )\n        \"\"\"\n        super().__init__(\n            task=task,\n            num_threads=num_threads,\n            prompt_format=prompt_format,\n            prompt_formatting_fn=prompt_formatting_fn,\n        )\n\n        self.max_new_tokens = max_new_tokens\n        self.do_sample = do_sample\n        self.temperature = temperature\n        self.top_k = top_k\n        self.top_p = top_p\n        self.typical_p = typical_p\n\n        self.model = model\n        if self.device != \"cpu\":\n            self.model.to(self.device)\n\n        self.tokenizer = tokenizer\n        self.tokenizer.padding_side = \"left\"\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n        if (\n            hasattr(self.tokenizer, \"use_default_system_prompt\")\n            and self.tokenizer.use_default_system_prompt  # type: ignore\n        ):\n            # The `tokenizer` also has a method named `apply_chat_template` that expects a `Conversation` as OpenAI does with the ChatML format\n            warnings.warn(\n                \"The provided `tokenizer` has `use_default_system_prompt=True` which means that the default system prompt will be used, which may collide with the `task` provided as an arg to this class.\",\n                UserWarning,\n                stacklevel=2,\n            )\n\n    def __rich_repr__(self) -&gt; Generator[Any, None, None]:\n        yield from super().__rich_repr__()\n        yield (\n            \"parameters\",\n            {\n                \"max_new_tokens\": self.max_new_tokens,\n                \"do_sample\": self.do_sample,\n                \"temperature\": self.temperature,\n                \"top_k\": self.top_k,\n                \"top_p\": self.top_p,\n                \"typical_p\": self.typical_p,\n            },\n        )\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the name of the Transformers model.\"\"\"\n        return self.model.config.name_or_path\n\n    @cached_property\n    def device(self) -&gt; \"device\":\n        \"\"\"Returns the device to be used for generation.\"\"\"\n        if torch.cuda.is_available():\n            return torch.device(\"cuda\")\n        if torch.backends.mps.is_available() and torch.backends.mps.is_built():  # type: ignore\n            return torch.device(\"mps\")\n        return torch.device(\"cpu\")\n\n    def _generate(\n        self, inputs: List[Dict[str, Any]], num_generations: int = 1\n    ) -&gt; List[List[LLMOutput]]:\n        \"\"\"Generates `num_generations` for each input in `inputs`.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n            num_generations (int, optional): the number of generations to be performed for each\n                input. Defaults to 1.\n\n        Returns:\n            List[List[LLMOutput]]: the outputs of the LLM.\n        \"\"\"\n        prompts = self._generate_prompts(\n            inputs, default_format=None, expected_output_type=str\n        )\n        encodings = self.tokenizer(prompts, padding=True, return_tensors=\"pt\")\n        if self.device != \"cpu\":\n            encodings = encodings.to(self.device)\n        with torch.inference_mode():\n            generated_ids = self.model.generate(\n                **encodings,  # type: ignore\n                pad_token_id=self.tokenizer.eos_token_id,\n                generation_config=GenerationConfig(\n                    do_sample=self.do_sample,\n                    temperature=self.temperature,\n                    max_new_tokens=self.max_new_tokens,\n                    top_k=self.top_k,\n                    top_p=self.top_p,\n                    typical_p=self.typical_p,\n                    num_return_sequences=num_generations,\n                ),\n            )\n        raw_outputs = self.tokenizer.batch_decode(\n            generated_ids[:, encodings.input_ids.shape[1] :],\n            skip_special_tokens=True,\n            clean_up_tokenization_spaces=True,\n        )\n        outputs = []\n        for prompt, i in zip(prompts, range(0, len(raw_outputs), num_generations)):\n            output = []\n            for raw_output in raw_outputs[i : i + num_generations]:\n                try:\n                    parsed_output = self.task.parse_output(raw_output)\n                except Exception as e:\n                    logger.error(f\"Error parsing Transformers output: {e}\")\n                    parsed_output = None\n                output.append(\n                    LLMOutput(\n                        model_name=self.model_name,\n                        prompt_used=prompt,\n                        raw_output=raw_output,\n                        parsed_output=parsed_output,\n                    )\n                )\n            outputs.append(output)\n        return outputs\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.TransformersLLM.device","title":"<code>device: device</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the device to be used for generation.</p>"},{"location":"reference/distilabel/llm/#distilabel.llm.TransformersLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the name of the Transformers model.</p>"},{"location":"reference/distilabel/llm/#distilabel.llm.TransformersLLM.__init__","title":"<code>__init__(model, tokenizer, task, max_new_tokens=128, do_sample=False, temperature=1.0, top_k=50, top_p=1.0, typical_p=1.0, num_threads=None, prompt_format=None, prompt_formatting_fn=None)</code>","text":"<p>Initializes the TransformersLLM class.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PreTrainedModel</code> <p>the model to be used for generation.</p> required <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>the tokenizer to be used for generation.</p> required <code>task</code> <code>Task</code> <p>the task to be performed by the LLM.</p> required <code>max_new_tokens</code> <code>int</code> <p>the maximum number of tokens to be generated. Defaults to 128.</p> <code>128</code> <code>do_sample</code> <code>bool</code> <p>whether to sample from the model or not. Defaults to False.</p> <code>False</code> <code>temperature</code> <code>float</code> <p>the temperature to be used for generation. Defaults to 1.0.</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>the top-k value to be used for generation. Defaults to 50.</p> <code>50</code> <code>top_p</code> <code>float</code> <p>the top-p value to be used for generation. Defaults to 1.0.</p> <code>1.0</code> <code>typical_p</code> <code>float</code> <p>the typical-p value to be used for generation. Defaults to 1.0.</p> <code>1.0</code> <code>num_threads</code> <code>Union[int, None]</code> <p>the number of threads to be used for generation. If <code>None</code>, the number of threads will be set to the number of available CPUs. Defaults to <code>None</code>.</p> <code>None</code> <code>prompt_format</code> <code>Union[SupportedFormats, None]</code> <p>the format to be used for formatting the prompts. If <code>None</code>, the prompts will not be formatted. Defaults to <code>None</code>.</p> <code>None</code> <code>prompt_formatting_fn</code> <code>Union[Callable[..., str], None]</code> <p>the function to be used for formatting the prompts. If <code>None</code>, the prompts will not be formatted.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from transformers import AutoModelForCausalLM, AutoTokenizer\n&gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask as Task\n&gt;&gt;&gt; from distilabel.llm import TransformersLLM\n&gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n&gt;&gt;&gt; task = Task()\n&gt;&gt;&gt; llm = TransformersLLM(\n...     model=model,\n...     tokenizer=tokenizer,\n...     task=task,\n... )\n</code></pre> Source code in <code>src/distilabel/llm/huggingface/transformers.py</code> <pre><code>def __init__(\n    self,\n    model: \"PreTrainedModel\",\n    tokenizer: \"PreTrainedTokenizer\",\n    task: \"Task\",\n    max_new_tokens: int = 128,\n    do_sample: bool = False,\n    temperature: float = 1.0,\n    top_k: int = 50,\n    top_p: float = 1.0,\n    typical_p: float = 1.0,\n    num_threads: Union[int, None] = None,\n    prompt_format: Union[\"SupportedFormats\", None] = None,\n    prompt_formatting_fn: Union[Callable[..., str], None] = None,\n) -&gt; None:\n    \"\"\"Initializes the TransformersLLM class.\n\n    Args:\n        model (PreTrainedModel): the model to be used for generation.\n        tokenizer (PreTrainedTokenizer): the tokenizer to be used for generation.\n        task (Task): the task to be performed by the LLM.\n        max_new_tokens (int, optional): the maximum number of tokens to be generated.\n            Defaults to 128.\n        do_sample (bool, optional): whether to sample from the model or not.\n            Defaults to False.\n        temperature (float, optional): the temperature to be used for generation.\n            Defaults to 1.0.\n        top_k (int, optional): the top-k value to be used for generation.\n            Defaults to 50.\n        top_p (float, optional): the top-p value to be used for generation.\n            Defaults to 1.0.\n        typical_p (float, optional): the typical-p value to be used for generation.\n            Defaults to 1.0.\n        num_threads (Union[int, None], optional): the number of threads to be used for generation.\n            If `None`, the number of threads will be set to the number of available CPUs.\n            Defaults to `None`.\n        prompt_format (Union[SupportedFormats, None], optional): the format to be used\n            for formatting the prompts. If `None`, the prompts will not be formatted.\n            Defaults to `None`.\n        prompt_formatting_fn (Union[Callable[..., str], None], optional): the function to be used\n            for formatting the prompts. If `None`, the prompts will not be formatted.\n\n    Examples:\n        &gt;&gt;&gt; from transformers import AutoModelForCausalLM, AutoTokenizer\n        &gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask as Task\n        &gt;&gt;&gt; from distilabel.llm import TransformersLLM\n        &gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n        &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n        &gt;&gt;&gt; task = Task()\n        &gt;&gt;&gt; llm = TransformersLLM(\n        ...     model=model,\n        ...     tokenizer=tokenizer,\n        ...     task=task,\n        ... )\n    \"\"\"\n    super().__init__(\n        task=task,\n        num_threads=num_threads,\n        prompt_format=prompt_format,\n        prompt_formatting_fn=prompt_formatting_fn,\n    )\n\n    self.max_new_tokens = max_new_tokens\n    self.do_sample = do_sample\n    self.temperature = temperature\n    self.top_k = top_k\n    self.top_p = top_p\n    self.typical_p = typical_p\n\n    self.model = model\n    if self.device != \"cpu\":\n        self.model.to(self.device)\n\n    self.tokenizer = tokenizer\n    self.tokenizer.padding_side = \"left\"\n    if self.tokenizer.pad_token is None:\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n    if (\n        hasattr(self.tokenizer, \"use_default_system_prompt\")\n        and self.tokenizer.use_default_system_prompt  # type: ignore\n    ):\n        # The `tokenizer` also has a method named `apply_chat_template` that expects a `Conversation` as OpenAI does with the ChatML format\n        warnings.warn(\n            \"The provided `tokenizer` has `use_default_system_prompt=True` which means that the default system prompt will be used, which may collide with the `task` provided as an arg to this class.\",\n            UserWarning,\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.vLLM","title":"<code>vLLM</code>","text":"<p>             Bases: <code>LLM</code></p> Source code in <code>src/distilabel/llm/vllm.py</code> <pre><code>class vLLM(LLM):\n    def __init__(\n        self,\n        vllm: \"_vLLM\",\n        task: \"Task\",\n        max_new_tokens: int = 128,\n        presence_penalty: float = 0.0,\n        frequency_penalty: float = 0.0,\n        temperature: float = 1.0,\n        top_p: float = 1.0,\n        top_k: int = -1,\n        prompt_format: Union[\"SupportedFormats\", None] = None,\n        prompt_formatting_fn: Union[Callable[..., str], None] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the vLLM class.\n\n        Args:\n            vllm (_vLLM): the vLLM model to be used.\n            task (Task): the task to be performed by the LLM.\n            max_new_tokens (int, optional): the maximum number of tokens to be generated.\n                Defaults to 128.\n            presence_penalty (float, optional): the presence penalty to be used for generation.\n                Defaults to 0.0.\n            frequency_penalty (float, optional): the frequency penalty to be used for generation.\n                Defaults to 0.0.\n            temperature (float, optional): the temperature to be used for generation.\n                Defaults to 1.0.\n            top_p (float, optional): the top-p value to be used for generation.\n                Defaults to 1.0.\n            top_k (int, optional): the top-k value to be used for generation.\n                Defaults to -1.\n            prompt_format (Union[SupportedFormats, None], optional): the format to be used\n                for the prompt. If `None`, the default format of the task will be used, available\n                formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n                but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n                will be used if no `prompt_formatting_fn` is provided.\n            prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n                applied to the prompt before generation. If `None`, no formatting will be applied.\n\n        Examples:\n            &gt;&gt;&gt; from vllm import LLM\n            &gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask as Task\n            &gt;&gt;&gt; from distilabel.llm import vLLM\n            &gt;&gt;&gt; model = LLM(model=\"gpt2\")\n            &gt;&gt;&gt; task = Task()\n            &gt;&gt;&gt; llm = vLLM(model=model, task=task)\n        \"\"\"\n        super().__init__(\n            task=task,\n            prompt_format=prompt_format,\n            prompt_formatting_fn=prompt_formatting_fn,\n        )\n\n        if not _VLLM_AVAILABLE:\n            raise ImportError(\n                \"`vLLM` cannot be used as `vllm` is not installed, please \"\n                \" install it with `pip install vllm`.\"\n            )\n\n        self.presence_penalty = presence_penalty\n        self.frequency_penalty = frequency_penalty\n        self.temperature = temperature\n        self.top_p = top_p\n        self.top_k = top_k\n        self.max_tokens = max_new_tokens\n\n        self.vllm = vllm\n\n    def __rich_repr__(self) -&gt; Generator[Any, None, None]:\n        yield from super().__rich_repr__()\n        yield (\n            \"parameters\",\n            {\n                \"max_tokens\": self.max_tokens,\n                \"presence_penalty\": self.presence_penalty,\n                \"frequency_penalty\": self.frequency_penalty,\n                \"temperature\": self.temperature,\n                \"top_p\": self.top_p,\n                \"top_k\": self.top_k,\n            },\n        )\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the name of the vLLM model.\"\"\"\n        return self.vllm.llm_engine.model_config.model  # type: ignore\n\n    def _generate(\n        self, inputs: List[Dict[str, Any]], num_generations: int = 1\n    ) -&gt; List[List[LLMOutput]]:\n        \"\"\"Generates `num_generations` for each input in `inputs`.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n            num_generations (int, optional): the number of generations to be performed for each\n                input. Defaults to 1.\n\n        Returns:\n            List[List[LLMOutput]]: the outputs of the LLM.\n        \"\"\"\n        prompts = self._generate_prompts(\n            inputs, default_format=None, expected_output_type=str\n        )\n        requests = self.vllm.generate(\n            prompts,\n            SamplingParams(  # type: ignore\n                n=num_generations,\n                presence_penalty=self.presence_penalty,\n                frequency_penalty=self.frequency_penalty,\n                temperature=self.temperature,\n                top_p=self.top_p,\n                top_k=self.top_k,\n                max_tokens=self.max_tokens,\n            ),\n            use_tqdm=False,  # type: ignore\n        )\n        outputs = []\n        for request, prompt in zip(requests, prompts):\n            output = []\n            for request_output in request.outputs:\n                try:\n                    parsed_output = self.task.parse_output(request_output.text)\n                except Exception as e:\n                    logger.error(f\"Error parsing vLLM output: {e}\")\n                    parsed_output = None\n                output.append(\n                    LLMOutput(\n                        model_name=self.model_name,\n                        prompt_used=prompt,\n                        raw_output=request_output.text,\n                        parsed_output=parsed_output,\n                    )\n                )\n            outputs.append(output)\n        return outputs\n</code></pre>"},{"location":"reference/distilabel/llm/#distilabel.llm.vLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the name of the vLLM model.</p>"},{"location":"reference/distilabel/llm/#distilabel.llm.vLLM.__init__","title":"<code>__init__(vllm, task, max_new_tokens=128, presence_penalty=0.0, frequency_penalty=0.0, temperature=1.0, top_p=1.0, top_k=-1, prompt_format=None, prompt_formatting_fn=None)</code>","text":"<p>Initializes the vLLM class.</p> <p>Parameters:</p> Name Type Description Default <code>vllm</code> <code>LLM</code> <p>the vLLM model to be used.</p> required <code>task</code> <code>Task</code> <p>the task to be performed by the LLM.</p> required <code>max_new_tokens</code> <code>int</code> <p>the maximum number of tokens to be generated. Defaults to 128.</p> <code>128</code> <code>presence_penalty</code> <code>float</code> <p>the presence penalty to be used for generation. Defaults to 0.0.</p> <code>0.0</code> <code>frequency_penalty</code> <code>float</code> <p>the frequency penalty to be used for generation. Defaults to 0.0.</p> <code>0.0</code> <code>temperature</code> <code>float</code> <p>the temperature to be used for generation. Defaults to 1.0.</p> <code>1.0</code> <code>top_p</code> <code>float</code> <p>the top-p value to be used for generation. Defaults to 1.0.</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>the top-k value to be used for generation. Defaults to -1.</p> <code>-1</code> <code>prompt_format</code> <code>Union[SupportedFormats, None]</code> <p>the format to be used for the prompt. If <code>None</code>, the default format of the task will be used, available formats are <code>openai</code>, <code>chatml</code>, <code>llama2</code>, <code>zephyr</code>, and <code>default</code>. Defaults to <code>None</code>, but <code>default</code> (concatenation of <code>system_prompt</code> and <code>formatted_prompt</code> with a line-break) will be used if no <code>prompt_formatting_fn</code> is provided.</p> <code>None</code> <code>prompt_formatting_fn</code> <code>Union[Callable[..., str], None]</code> <p>a function to be applied to the prompt before generation. If <code>None</code>, no formatting will be applied.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from vllm import LLM\n&gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask as Task\n&gt;&gt;&gt; from distilabel.llm import vLLM\n&gt;&gt;&gt; model = LLM(model=\"gpt2\")\n&gt;&gt;&gt; task = Task()\n&gt;&gt;&gt; llm = vLLM(model=model, task=task)\n</code></pre> Source code in <code>src/distilabel/llm/vllm.py</code> <pre><code>def __init__(\n    self,\n    vllm: \"_vLLM\",\n    task: \"Task\",\n    max_new_tokens: int = 128,\n    presence_penalty: float = 0.0,\n    frequency_penalty: float = 0.0,\n    temperature: float = 1.0,\n    top_p: float = 1.0,\n    top_k: int = -1,\n    prompt_format: Union[\"SupportedFormats\", None] = None,\n    prompt_formatting_fn: Union[Callable[..., str], None] = None,\n) -&gt; None:\n    \"\"\"Initializes the vLLM class.\n\n    Args:\n        vllm (_vLLM): the vLLM model to be used.\n        task (Task): the task to be performed by the LLM.\n        max_new_tokens (int, optional): the maximum number of tokens to be generated.\n            Defaults to 128.\n        presence_penalty (float, optional): the presence penalty to be used for generation.\n            Defaults to 0.0.\n        frequency_penalty (float, optional): the frequency penalty to be used for generation.\n            Defaults to 0.0.\n        temperature (float, optional): the temperature to be used for generation.\n            Defaults to 1.0.\n        top_p (float, optional): the top-p value to be used for generation.\n            Defaults to 1.0.\n        top_k (int, optional): the top-k value to be used for generation.\n            Defaults to -1.\n        prompt_format (Union[SupportedFormats, None], optional): the format to be used\n            for the prompt. If `None`, the default format of the task will be used, available\n            formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n            but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n            will be used if no `prompt_formatting_fn` is provided.\n        prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n            applied to the prompt before generation. If `None`, no formatting will be applied.\n\n    Examples:\n        &gt;&gt;&gt; from vllm import LLM\n        &gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask as Task\n        &gt;&gt;&gt; from distilabel.llm import vLLM\n        &gt;&gt;&gt; model = LLM(model=\"gpt2\")\n        &gt;&gt;&gt; task = Task()\n        &gt;&gt;&gt; llm = vLLM(model=model, task=task)\n    \"\"\"\n    super().__init__(\n        task=task,\n        prompt_format=prompt_format,\n        prompt_formatting_fn=prompt_formatting_fn,\n    )\n\n    if not _VLLM_AVAILABLE:\n        raise ImportError(\n            \"`vLLM` cannot be used as `vllm` is not installed, please \"\n            \" install it with `pip install vllm`.\"\n        )\n\n    self.presence_penalty = presence_penalty\n    self.frequency_penalty = frequency_penalty\n    self.temperature = temperature\n    self.top_p = top_p\n    self.top_k = top_k\n    self.max_tokens = max_new_tokens\n\n    self.vllm = vllm\n</code></pre>"},{"location":"reference/distilabel/llm/base/","title":"base","text":""},{"location":"reference/distilabel/llm/base/#distilabel.llm.base.LLM","title":"<code>LLM</code>","text":"<p>             Bases: <code>ABC</code></p> Source code in <code>src/distilabel/llm/base.py</code> <pre><code>class LLM(ABC):\n    def __init__(\n        self,\n        task: Task,\n        num_threads: Union[int, None] = None,\n        prompt_format: Union[\"SupportedFormats\", None] = None,\n        prompt_formatting_fn: Union[Callable[..., str], None] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the LLM base class.\n\n        Note:\n            This class is intended to be used internally, but you anyone can still create\n            a subclass, implement the `abstractmethod`s and use it.\n\n        Args:\n            task (Task): the task to be performed by the LLM.\n            num_threads (Union[int, None], optional): the number of threads to be used\n                for parallel generation. If `None`, no parallel generation will be performed.\n                Defaults to `None`.\n            prompt_format (Union[\"SupportedFormats\", None], optional): the format to be used\n                for the prompt. If `None`, the default format of the task will be used, available\n                formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n                but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n                will be used if no `prompt_formatting_fn` is provided.\n            prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n                applied to the prompt before generation. If `None`, no formatting will be applied.\n                Defaults to `None`.\n        \"\"\"\n        self.task = task\n\n        self.thread_pool_executor = (\n            ThreadPoolExecutor(max_workers=num_threads)\n            if num_threads is not None\n            else None\n        )\n\n        self.prompt_format = prompt_format\n        self.prompt_formatting_fn = prompt_formatting_fn\n\n    def __del__(self) -&gt; None:\n        \"\"\"Shuts down the thread pool executor if it is not `None`.\"\"\"\n        if self.thread_pool_executor is not None:\n            self.thread_pool_executor.shutdown()\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}(task={self.task.__class__.__name__}, num_threads={self.thread_pool_executor._max_workers}, promp_format='{self.prompt_format}', model='{self.model_name}')\"\n\n    def __rich_repr__(self) -&gt; Generator[Any, None, None]:\n        yield \"task\", self.task\n        yield \"num_threads\", self.thread_pool_executor._max_workers\n        yield \"prompt_format\", self.prompt_format\n        if self.prompt_formatting_fn is not None:\n            args = f\"({', '.join(self.prompt_formatting_fn.__code__.co_varnames)})\"\n            representation = self.prompt_formatting_fn.__name__ + args\n            yield \"prompt_formatting_fn\", representation\n        yield \"model\", self.model_name\n\n    @property\n    @abstractmethod\n    def model_name(self) -&gt; str:\n        pass\n\n    def _generate_prompts(\n        self,\n        inputs: List[Dict[str, Any]],\n        default_format: Union[\"SupportedFormats\", None] = None,\n        expected_output_type: Type = str,\n    ) -&gt; List[Any]:\n        \"\"\"Generates the prompts to be used for generation.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n            default_format (Union[\"SupportedFormats\", None], optional): the default format to be used\n                for the prompt if no `prompt_format` is specified. Defaults to `None`.\n            expected_output_type (Type, optional): the expected type of the prompt. Defaults to `str`.\n\n        Returns:\n            List[Any]: the generated prompts.\n\n        Raises:\n            ValueError: if the generated prompt is not of the expected type.\n        \"\"\"\n        prompts = []\n        for input in inputs:\n            prompt = self.task.generate_prompt(**input)\n            if not isinstance(prompt, Prompt) and self.prompt_formatting_fn is not None:\n                warnings.warn(\n                    \"The method `generate_prompt` is not returning a `Prompt` class but a prompt\"\n                    f\" of `type={type(prompt)}`, meaning that a pre-formatting has already been\"\n                    \" applied in the `task.generate_prompt` method, so the usage of a `prompt_formatting_fn`\"\n                    \" is discouraged.\",\n                    UserWarning,\n                    stacklevel=2,\n                )\n                prompt = self.prompt_formatting_fn(prompt)\n            elif isinstance(prompt, Prompt) and self.prompt_formatting_fn is None:\n                if self.prompt_format is not None or default_format is not None:\n                    prompt = prompt.format_as(\n                        format=self.prompt_format or default_format  # type: ignore\n                    )\n                else:\n                    warnings.warn(\n                        \"No `prompt_format` has been specified and no `default_format` is set, so\"\n                        \" the prompt will be concatenated with a line-break and no specific formatting\"\n                        \" by default.\",\n                        UserWarning,\n                        stacklevel=2,\n                    )\n                    prompt = prompt.format_as(format=\"default\")\n            if not isinstance(prompt, expected_output_type):\n                raise ValueError(\n                    f\"The provided `prompt={prompt}` is of `type={type(prompt)}`, but it must be of\"\n                    f\" `type={expected_output_type}`, so make sure that `task.generate_prompt` returns\"\n                    f\" a `{expected_output_type}` or that the `formatting_fn` formats the prompt as a \"\n                    f\" `{expected_output_type}`.\"\n                )\n            prompts.append(prompt)\n        return prompts\n\n    @abstractmethod\n    def _generate(\n        self, inputs: List[Dict[str, Any]], num_generations: int = 1\n    ) -&gt; List[List[\"LLMOutput\"]]:\n        pass\n\n    def generate(\n        self,\n        inputs: List[Dict[str, Any]],\n        num_generations: int = 1,\n        progress_callback_func: Union[Callable, None] = None,\n    ) -&gt; Union[List[Future[List[\"LLMOutput\"]]], List[List[\"LLMOutput\"]]]:\n        \"\"\"Generates the outputs for the given inputs using the LLM.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n            num_generations (int, optional): the number of generations to be performed for each input.\n                Defaults to `1`.\n            progress_callback_func (Union[Callable, None], optional): a function to be called at each\n                generation step. Defaults to `None`.\n\n        Returns:\n            Union[List[Future[List[\"LLMOutput\"]]], List[List[\"LLMOutput\"]]]: the generated outputs.\n        \"\"\"\n\n        def _progress():\n            if progress_callback_func is not None:\n                advance = (\n                    num_generations * len(inputs)\n                    if not self.return_futures\n                    else num_generations\n                )\n                progress_callback_func(advance=advance)\n\n        if self.thread_pool_executor is not None:\n            futures = []\n            for input in inputs:\n                future = self.thread_pool_executor.submit(\n                    self._generate, [input], num_generations\n                )\n                future.add_done_callback(lambda _: _progress())\n                futures.append(future)\n            return futures\n\n        generations = self._generate(inputs, num_generations)\n        _progress()\n        return generations\n\n    @property\n    def return_futures(self) -&gt; bool:\n        \"\"\"Returns whether the LLM returns futures or not.\"\"\"\n        return self.thread_pool_executor is not None\n</code></pre>"},{"location":"reference/distilabel/llm/base/#distilabel.llm.base.LLM.return_futures","title":"<code>return_futures: bool</code>  <code>property</code>","text":"<p>Returns whether the LLM returns futures or not.</p>"},{"location":"reference/distilabel/llm/base/#distilabel.llm.base.LLM.__del__","title":"<code>__del__()</code>","text":"<p>Shuts down the thread pool executor if it is not <code>None</code>.</p> Source code in <code>src/distilabel/llm/base.py</code> <pre><code>def __del__(self) -&gt; None:\n    \"\"\"Shuts down the thread pool executor if it is not `None`.\"\"\"\n    if self.thread_pool_executor is not None:\n        self.thread_pool_executor.shutdown()\n</code></pre>"},{"location":"reference/distilabel/llm/base/#distilabel.llm.base.LLM.__init__","title":"<code>__init__(task, num_threads=None, prompt_format=None, prompt_formatting_fn=None)</code>","text":"<p>Initializes the LLM base class.</p> Note <p>This class is intended to be used internally, but you anyone can still create a subclass, implement the <code>abstractmethod</code>s and use it.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task</code> <p>the task to be performed by the LLM.</p> required <code>num_threads</code> <code>Union[int, None]</code> <p>the number of threads to be used for parallel generation. If <code>None</code>, no parallel generation will be performed. Defaults to <code>None</code>.</p> <code>None</code> <code>prompt_format</code> <code>Union['SupportedFormats', None]</code> <p>the format to be used for the prompt. If <code>None</code>, the default format of the task will be used, available formats are <code>openai</code>, <code>chatml</code>, <code>llama2</code>, <code>zephyr</code>, and <code>default</code>. Defaults to <code>None</code>, but <code>default</code> (concatenation of <code>system_prompt</code> and <code>formatted_prompt</code> with a line-break) will be used if no <code>prompt_formatting_fn</code> is provided.</p> <code>None</code> <code>prompt_formatting_fn</code> <code>Union[Callable[..., str], None]</code> <p>a function to be applied to the prompt before generation. If <code>None</code>, no formatting will be applied. Defaults to <code>None</code>.</p> <code>None</code> Source code in <code>src/distilabel/llm/base.py</code> <pre><code>def __init__(\n    self,\n    task: Task,\n    num_threads: Union[int, None] = None,\n    prompt_format: Union[\"SupportedFormats\", None] = None,\n    prompt_formatting_fn: Union[Callable[..., str], None] = None,\n) -&gt; None:\n    \"\"\"Initializes the LLM base class.\n\n    Note:\n        This class is intended to be used internally, but you anyone can still create\n        a subclass, implement the `abstractmethod`s and use it.\n\n    Args:\n        task (Task): the task to be performed by the LLM.\n        num_threads (Union[int, None], optional): the number of threads to be used\n            for parallel generation. If `None`, no parallel generation will be performed.\n            Defaults to `None`.\n        prompt_format (Union[\"SupportedFormats\", None], optional): the format to be used\n            for the prompt. If `None`, the default format of the task will be used, available\n            formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n            but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n            will be used if no `prompt_formatting_fn` is provided.\n        prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n            applied to the prompt before generation. If `None`, no formatting will be applied.\n            Defaults to `None`.\n    \"\"\"\n    self.task = task\n\n    self.thread_pool_executor = (\n        ThreadPoolExecutor(max_workers=num_threads)\n        if num_threads is not None\n        else None\n    )\n\n    self.prompt_format = prompt_format\n    self.prompt_formatting_fn = prompt_formatting_fn\n</code></pre>"},{"location":"reference/distilabel/llm/base/#distilabel.llm.base.LLM.generate","title":"<code>generate(inputs, num_generations=1, progress_callback_func=None)</code>","text":"<p>Generates the outputs for the given inputs using the LLM.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[Dict[str, Any]]</code> <p>the inputs to be used for generation.</p> required <code>num_generations</code> <code>int</code> <p>the number of generations to be performed for each input. Defaults to <code>1</code>.</p> <code>1</code> <code>progress_callback_func</code> <code>Union[Callable, None]</code> <p>a function to be called at each generation step. Defaults to <code>None</code>.</p> <code>None</code> <p>Returns:</p> Type Description <code>Union[List[Future[List['LLMOutput']]], List[List['LLMOutput']]]</code> <p>Union[List[Future[List[\"LLMOutput\"]]], List[List[\"LLMOutput\"]]]: the generated outputs.</p> Source code in <code>src/distilabel/llm/base.py</code> <pre><code>def generate(\n    self,\n    inputs: List[Dict[str, Any]],\n    num_generations: int = 1,\n    progress_callback_func: Union[Callable, None] = None,\n) -&gt; Union[List[Future[List[\"LLMOutput\"]]], List[List[\"LLMOutput\"]]]:\n    \"\"\"Generates the outputs for the given inputs using the LLM.\n\n    Args:\n        inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n        num_generations (int, optional): the number of generations to be performed for each input.\n            Defaults to `1`.\n        progress_callback_func (Union[Callable, None], optional): a function to be called at each\n            generation step. Defaults to `None`.\n\n    Returns:\n        Union[List[Future[List[\"LLMOutput\"]]], List[List[\"LLMOutput\"]]]: the generated outputs.\n    \"\"\"\n\n    def _progress():\n        if progress_callback_func is not None:\n            advance = (\n                num_generations * len(inputs)\n                if not self.return_futures\n                else num_generations\n            )\n            progress_callback_func(advance=advance)\n\n    if self.thread_pool_executor is not None:\n        futures = []\n        for input in inputs:\n            future = self.thread_pool_executor.submit(\n                self._generate, [input], num_generations\n            )\n            future.add_done_callback(lambda _: _progress())\n            futures.append(future)\n        return futures\n\n    generations = self._generate(inputs, num_generations)\n    _progress()\n    return generations\n</code></pre>"},{"location":"reference/distilabel/llm/llama_cpp/","title":"llama_cpp","text":""},{"location":"reference/distilabel/llm/llama_cpp/#distilabel.llm.llama_cpp.LlamaCppLLM","title":"<code>LlamaCppLLM</code>","text":"<p>             Bases: <code>LLM</code></p> Source code in <code>src/distilabel/llm/llama_cpp.py</code> <pre><code>class LlamaCppLLM(LLM):\n    def __init__(\n        self,\n        model: \"Llama\",\n        task: \"Task\",\n        max_new_tokens: int = 128,\n        temperature: float = 0.8,\n        top_p: float = 0.95,\n        top_k: int = 40,\n        repeat_penalty: float = 1.1,\n        prompt_format: Union[SupportedFormats, None] = None,\n        prompt_formatting_fn: Union[Callable[..., str], None] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the LlamaCppLLM class.\n\n        Args:\n            model (Llama): the llama-cpp model to be used.\n            task (Task): the task to be performed by the LLM.\n            max_new_tokens (int, optional): the maximum number of tokens to be generated.\n                Defaults to 128.\n            temperature (float, optional): the temperature to be used for generation.\n                Defaults to 0.8.\n            top_p (float, optional): the top-p value to be used for generation.\n                Defaults to 0.95.\n            top_k (int, optional): the top-k value to be used for generation.\n                Defaults to 40.\n            repeat_penalty (float, optional): the repeat penalty to be used for generation.\n                Defaults to 1.1.\n            prompt_format (Union[SupportedFormats, None], optional): the format to be used\n                for the prompt. If `None`, the default format of the task will be used, available\n                formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n                but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n                will be used if no `prompt_formatting_fn` is provided.\n            prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n                applied to the prompt before generation. If `None`, no formatting will be applied.\n                Defaults to `None`.\n\n        Examples:\n            &gt;&gt;&gt; from llama_cpp import Llama\n            &gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask as Task\n            &gt;&gt;&gt; from distilabel.llm import LlamaCppLLM\n            &gt;&gt;&gt; model = Llama(model_path=\"path/to/model\")\n            &gt;&gt;&gt; task = Task()\n            &gt;&gt;&gt; llm = LlamaCppLLM(model=model, task=task)\n        \"\"\"\n        super().__init__(\n            task=task,\n            prompt_format=prompt_format,\n            prompt_formatting_fn=prompt_formatting_fn,\n        )\n\n        if not _LLAMA_CPP_AVAILABLE:\n            raise ImportError(\n                \"`LlamaCppLLM` cannot be used as `llama_cpp` is not installed, please \"\n                \" install it with `pip install llama-cpp-python`.\"\n            )\n\n        self.max_tokens = max_new_tokens\n        self.temperature = temperature\n        self.top_p = top_p\n        self.top_k = top_k\n        self.repeat_penalty = repeat_penalty\n\n        self.model = model\n\n    def __rich_repr__(self) -&gt; Generator[Any, None, None]:\n        yield from super().__rich_repr__()\n        yield (\n            \"parameters\",\n            {\n                \"max_new_tokens\": self.max_tokens,\n                \"temperature\": self.temperature,\n                \"top_p\": self.top_p,\n                \"top_k\": self.top_k,\n                \"repeat_penalty\": self.repeat_penalty,\n            },\n        )\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the name of the llama-cpp model, which is the same as the model path.\"\"\"\n        return self.model.model_path\n\n    def _generate(\n        self, inputs: List[Dict[str, Any]], num_generations: int = 1\n    ) -&gt; List[List[LLMOutput]]:\n        \"\"\"Generates `num_generations` for each input in `inputs`.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n            num_generations (int, optional): the number of generations to be performed for each\n                input. Defaults to 1.\n\n        Returns:\n            List[List[LLMOutput]]: the generated outputs.\n        \"\"\"\n        prompts = self._generate_prompts(\n            inputs, default_format=None, expected_output_type=str\n        )\n        outputs = []\n        for prompt in prompts:\n            output = []\n            for _ in range(num_generations):\n                raw_output = self.model.create_completion(\n                    prompt,\n                    max_tokens=self.max_tokens,\n                    temperature=self.temperature,\n                    top_p=self.top_p,\n                    top_k=self.top_k,\n                    repeat_penalty=self.repeat_penalty,\n                )\n                try:\n                    parsed_output = self.task.parse_output(\n                        raw_output[\"choices\"][0][\"text\"].strip()\n                    )\n                except Exception as e:\n                    logger.error(f\"Error parsing llama-cpp output: {e}\")\n                    parsed_output = None\n                output.append(\n                    LLMOutput(\n                        model_name=self.model_name,\n                        prompt_used=prompt,\n                        raw_output=raw_output,\n                        parsed_output=parsed_output,\n                    )\n                )\n            outputs.append(output)\n        return outputs\n</code></pre>"},{"location":"reference/distilabel/llm/llama_cpp/#distilabel.llm.llama_cpp.LlamaCppLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the name of the llama-cpp model, which is the same as the model path.</p>"},{"location":"reference/distilabel/llm/llama_cpp/#distilabel.llm.llama_cpp.LlamaCppLLM.__init__","title":"<code>__init__(model, task, max_new_tokens=128, temperature=0.8, top_p=0.95, top_k=40, repeat_penalty=1.1, prompt_format=None, prompt_formatting_fn=None)</code>","text":"<p>Initializes the LlamaCppLLM class.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Llama</code> <p>the llama-cpp model to be used.</p> required <code>task</code> <code>Task</code> <p>the task to be performed by the LLM.</p> required <code>max_new_tokens</code> <code>int</code> <p>the maximum number of tokens to be generated. Defaults to 128.</p> <code>128</code> <code>temperature</code> <code>float</code> <p>the temperature to be used for generation. Defaults to 0.8.</p> <code>0.8</code> <code>top_p</code> <code>float</code> <p>the top-p value to be used for generation. Defaults to 0.95.</p> <code>0.95</code> <code>top_k</code> <code>int</code> <p>the top-k value to be used for generation. Defaults to 40.</p> <code>40</code> <code>repeat_penalty</code> <code>float</code> <p>the repeat penalty to be used for generation. Defaults to 1.1.</p> <code>1.1</code> <code>prompt_format</code> <code>Union[SupportedFormats, None]</code> <p>the format to be used for the prompt. If <code>None</code>, the default format of the task will be used, available formats are <code>openai</code>, <code>chatml</code>, <code>llama2</code>, <code>zephyr</code>, and <code>default</code>. Defaults to <code>None</code>, but <code>default</code> (concatenation of <code>system_prompt</code> and <code>formatted_prompt</code> with a line-break) will be used if no <code>prompt_formatting_fn</code> is provided.</p> <code>None</code> <code>prompt_formatting_fn</code> <code>Union[Callable[..., str], None]</code> <p>a function to be applied to the prompt before generation. If <code>None</code>, no formatting will be applied. Defaults to <code>None</code>.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from llama_cpp import Llama\n&gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask as Task\n&gt;&gt;&gt; from distilabel.llm import LlamaCppLLM\n&gt;&gt;&gt; model = Llama(model_path=\"path/to/model\")\n&gt;&gt;&gt; task = Task()\n&gt;&gt;&gt; llm = LlamaCppLLM(model=model, task=task)\n</code></pre> Source code in <code>src/distilabel/llm/llama_cpp.py</code> <pre><code>def __init__(\n    self,\n    model: \"Llama\",\n    task: \"Task\",\n    max_new_tokens: int = 128,\n    temperature: float = 0.8,\n    top_p: float = 0.95,\n    top_k: int = 40,\n    repeat_penalty: float = 1.1,\n    prompt_format: Union[SupportedFormats, None] = None,\n    prompt_formatting_fn: Union[Callable[..., str], None] = None,\n) -&gt; None:\n    \"\"\"Initializes the LlamaCppLLM class.\n\n    Args:\n        model (Llama): the llama-cpp model to be used.\n        task (Task): the task to be performed by the LLM.\n        max_new_tokens (int, optional): the maximum number of tokens to be generated.\n            Defaults to 128.\n        temperature (float, optional): the temperature to be used for generation.\n            Defaults to 0.8.\n        top_p (float, optional): the top-p value to be used for generation.\n            Defaults to 0.95.\n        top_k (int, optional): the top-k value to be used for generation.\n            Defaults to 40.\n        repeat_penalty (float, optional): the repeat penalty to be used for generation.\n            Defaults to 1.1.\n        prompt_format (Union[SupportedFormats, None], optional): the format to be used\n            for the prompt. If `None`, the default format of the task will be used, available\n            formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n            but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n            will be used if no `prompt_formatting_fn` is provided.\n        prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n            applied to the prompt before generation. If `None`, no formatting will be applied.\n            Defaults to `None`.\n\n    Examples:\n        &gt;&gt;&gt; from llama_cpp import Llama\n        &gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask as Task\n        &gt;&gt;&gt; from distilabel.llm import LlamaCppLLM\n        &gt;&gt;&gt; model = Llama(model_path=\"path/to/model\")\n        &gt;&gt;&gt; task = Task()\n        &gt;&gt;&gt; llm = LlamaCppLLM(model=model, task=task)\n    \"\"\"\n    super().__init__(\n        task=task,\n        prompt_format=prompt_format,\n        prompt_formatting_fn=prompt_formatting_fn,\n    )\n\n    if not _LLAMA_CPP_AVAILABLE:\n        raise ImportError(\n            \"`LlamaCppLLM` cannot be used as `llama_cpp` is not installed, please \"\n            \" install it with `pip install llama-cpp-python`.\"\n        )\n\n    self.max_tokens = max_new_tokens\n    self.temperature = temperature\n    self.top_p = top_p\n    self.top_k = top_k\n    self.repeat_penalty = repeat_penalty\n\n    self.model = model\n</code></pre>"},{"location":"reference/distilabel/llm/openai/","title":"openai","text":""},{"location":"reference/distilabel/llm/openai/#distilabel.llm.openai.OpenAILLM","title":"<code>OpenAILLM</code>","text":"<p>             Bases: <code>LLM</code></p> Source code in <code>src/distilabel/llm/openai.py</code> <pre><code>class OpenAILLM(LLM):\n    def __init__(\n        self,\n        task: \"Task\",\n        model: str = \"gpt-3.5-turbo\",\n        client: Union[\"OpenAI\", None] = None,\n        openai_api_key: Union[str, None] = None,\n        max_new_tokens: int = 128,\n        frequency_penalty: float = 0.0,\n        presence_penalty: float = 0.0,\n        temperature: float = 1.0,\n        top_p: float = 1.0,\n        num_threads: Union[int, None] = None,\n        prompt_format: Union[\"SupportedFormats\", None] = None,\n        prompt_formatting_fn: Union[Callable[..., str], None] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the OpenAILLM class.\n\n        Args:\n            task (Task): the task to be performed by the LLM.\n            model (str, optional): the model to be used for generation. Defaults to \"gpt-3.5-turbo\".\n            client (Union[OpenAI, None], optional): an OpenAI client to be used for generation.\n                If `None`, a new client will be created. Defaults to `None`.\n            openai_api_key (Union[str, None], optional): the OpenAI API key to be used for generation.\n                If `None`, the `OPENAI_API_KEY` environment variable will be used. Defaults to `None`.\n            max_new_tokens (int, optional): the maximum number of tokens to be generated.\n                Defaults to 128.\n            frequency_penalty (float, optional): the frequency penalty to be used for generation.\n                Defaults to 0.0.\n            presence_penalty (float, optional): the presence penalty to be used for generation.\n                Defaults to 0.0.\n            temperature (float, optional): the temperature to be used for generation.\n                Defaults to 1.0.\n            top_p (float, optional): the top-p value to be used for generation.\n                Defaults to 1.0.\n            num_threads (Union[int, None], optional): the number of threads to be used\n                for parallel generation. If `None`, no parallel generation will be performed.\n                Defaults to `None`.\n            prompt_format (Union[SupportedFormats, None], optional): the format to be used\n                for the prompt. If `None`, the default format of the task will be used, available\n                formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n                but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n                will be used if no `prompt_formatting_fn` is provided.\n            prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n                applied to the prompt before generation. If `None`, no formatting will be applied.\n                Defaults to `None`.\n\n        Raises:\n            AssertionError: if the provided `model` is not available in your OpenAI account.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask as Task\n            &gt;&gt;&gt; from distilabel.llm import OpenAILLM\n            &gt;&gt;&gt; task = Task()\n            &gt;&gt;&gt; llm = OpenAILLM(model=\"gpt-3.5-turbo\", task=task)\n        \"\"\"\n        super().__init__(\n            task=task,\n            num_threads=num_threads,\n            prompt_format=prompt_format,\n            prompt_formatting_fn=prompt_formatting_fn,\n        )\n\n        if not _OPENAI_AVAILABLE:\n            raise ImportError(\n                \"`OpenAILLM` cannot be used as `openai` is not installed, please \"\n                \" install it with `pip install openai`.\"\n            )\n\n        self.max_tokens = max_new_tokens\n        self.frequency_penalty = frequency_penalty\n        self.presence_penalty = presence_penalty\n        self.temperature = temperature\n        self.top_p = top_p\n\n        self.client = client or OpenAI(api_key=openai_api_key, max_retries=6)\n\n        assert (\n            model in self.available_models\n        ), f\"Provided `model` is not available in your OpenAI account, available models are {self.available_models}\"\n        self.model = model\n\n    def __rich_repr__(self) -&gt; Generator[Any, None, None]:\n        yield from super().__rich_repr__()\n        yield (\n            \"parameters\",\n            {\n                \"max_tokens\": self.max_tokens,\n                \"frequency_penalty\": self.frequency_penalty,\n                \"presence_penalty\": self.presence_penalty,\n                \"temperature\": self.temperature,\n                \"top_p\": self.top_p,\n            },\n        )\n\n    @cached_property\n    def available_models(self) -&gt; List[str]:\n        \"\"\"Returns the list of available models in your OpenAI account.\"\"\"\n        return [model.id for model in self.client.models.list().data]\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the name of the OpenAI model.\"\"\"\n        return self.model\n\n    def _generate(\n        self,\n        inputs: List[Dict[str, Any]],\n        num_generations: int = 1,\n    ) -&gt; List[List[LLMOutput]]:\n        \"\"\"Generates `num_generations` for each input in `inputs`.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n            num_generations (int, optional): the number of generations to be performed for each\n                input. Defaults to 1.\n\n        Returns:\n            List[List[LLMOutput]]: the generated outputs.\n        \"\"\"\n        prompts = self._generate_prompts(\n            inputs, default_format=\"openai\", expected_output_type=list\n        )\n        outputs = []\n        for prompt in prompts:\n            chat_completions = self.client.chat.completions.create(\n                messages=prompt,\n                model=self.model,\n                n=num_generations,\n                max_tokens=self.max_tokens,\n                frequency_penalty=self.frequency_penalty,\n                presence_penalty=self.presence_penalty,\n                temperature=self.temperature,\n                top_p=self.top_p,\n                timeout=50,\n            )\n\n            output = []\n            for chat_completion in chat_completions.choices:\n                try:\n                    parsed_response = self.task.parse_output(\n                        chat_completion.message.content.strip()\n                    )\n                except Exception as e:\n                    logger.error(f\"Error parsing OpenAI response: {e}\")\n                    parsed_response = None\n                output.append(\n                    LLMOutput(\n                        model_name=self.model_name,\n                        prompt_used=prompt,\n                        raw_output=chat_completion.message.content,\n                        parsed_output=parsed_response,\n                    )\n                )\n            outputs.append(output)\n        return outputs\n</code></pre>"},{"location":"reference/distilabel/llm/openai/#distilabel.llm.openai.OpenAILLM.available_models","title":"<code>available_models: List[str]</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the list of available models in your OpenAI account.</p>"},{"location":"reference/distilabel/llm/openai/#distilabel.llm.openai.OpenAILLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the name of the OpenAI model.</p>"},{"location":"reference/distilabel/llm/openai/#distilabel.llm.openai.OpenAILLM.__init__","title":"<code>__init__(task, model='gpt-3.5-turbo', client=None, openai_api_key=None, max_new_tokens=128, frequency_penalty=0.0, presence_penalty=0.0, temperature=1.0, top_p=1.0, num_threads=None, prompt_format=None, prompt_formatting_fn=None)</code>","text":"<p>Initializes the OpenAILLM class.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>Task</code> <p>the task to be performed by the LLM.</p> required <code>model</code> <code>str</code> <p>the model to be used for generation. Defaults to \"gpt-3.5-turbo\".</p> <code>'gpt-3.5-turbo'</code> <code>client</code> <code>Union[OpenAI, None]</code> <p>an OpenAI client to be used for generation. If <code>None</code>, a new client will be created. Defaults to <code>None</code>.</p> <code>None</code> <code>openai_api_key</code> <code>Union[str, None]</code> <p>the OpenAI API key to be used for generation. If <code>None</code>, the <code>OPENAI_API_KEY</code> environment variable will be used. Defaults to <code>None</code>.</p> <code>None</code> <code>max_new_tokens</code> <code>int</code> <p>the maximum number of tokens to be generated. Defaults to 128.</p> <code>128</code> <code>frequency_penalty</code> <code>float</code> <p>the frequency penalty to be used for generation. Defaults to 0.0.</p> <code>0.0</code> <code>presence_penalty</code> <code>float</code> <p>the presence penalty to be used for generation. Defaults to 0.0.</p> <code>0.0</code> <code>temperature</code> <code>float</code> <p>the temperature to be used for generation. Defaults to 1.0.</p> <code>1.0</code> <code>top_p</code> <code>float</code> <p>the top-p value to be used for generation. Defaults to 1.0.</p> <code>1.0</code> <code>num_threads</code> <code>Union[int, None]</code> <p>the number of threads to be used for parallel generation. If <code>None</code>, no parallel generation will be performed. Defaults to <code>None</code>.</p> <code>None</code> <code>prompt_format</code> <code>Union[SupportedFormats, None]</code> <p>the format to be used for the prompt. If <code>None</code>, the default format of the task will be used, available formats are <code>openai</code>, <code>chatml</code>, <code>llama2</code>, <code>zephyr</code>, and <code>default</code>. Defaults to <code>None</code>, but <code>default</code> (concatenation of <code>system_prompt</code> and <code>formatted_prompt</code> with a line-break) will be used if no <code>prompt_formatting_fn</code> is provided.</p> <code>None</code> <code>prompt_formatting_fn</code> <code>Union[Callable[..., str], None]</code> <p>a function to be applied to the prompt before generation. If <code>None</code>, no formatting will be applied. Defaults to <code>None</code>.</p> <code>None</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>if the provided <code>model</code> is not available in your OpenAI account.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask as Task\n&gt;&gt;&gt; from distilabel.llm import OpenAILLM\n&gt;&gt;&gt; task = Task()\n&gt;&gt;&gt; llm = OpenAILLM(model=\"gpt-3.5-turbo\", task=task)\n</code></pre> Source code in <code>src/distilabel/llm/openai.py</code> <pre><code>def __init__(\n    self,\n    task: \"Task\",\n    model: str = \"gpt-3.5-turbo\",\n    client: Union[\"OpenAI\", None] = None,\n    openai_api_key: Union[str, None] = None,\n    max_new_tokens: int = 128,\n    frequency_penalty: float = 0.0,\n    presence_penalty: float = 0.0,\n    temperature: float = 1.0,\n    top_p: float = 1.0,\n    num_threads: Union[int, None] = None,\n    prompt_format: Union[\"SupportedFormats\", None] = None,\n    prompt_formatting_fn: Union[Callable[..., str], None] = None,\n) -&gt; None:\n    \"\"\"Initializes the OpenAILLM class.\n\n    Args:\n        task (Task): the task to be performed by the LLM.\n        model (str, optional): the model to be used for generation. Defaults to \"gpt-3.5-turbo\".\n        client (Union[OpenAI, None], optional): an OpenAI client to be used for generation.\n            If `None`, a new client will be created. Defaults to `None`.\n        openai_api_key (Union[str, None], optional): the OpenAI API key to be used for generation.\n            If `None`, the `OPENAI_API_KEY` environment variable will be used. Defaults to `None`.\n        max_new_tokens (int, optional): the maximum number of tokens to be generated.\n            Defaults to 128.\n        frequency_penalty (float, optional): the frequency penalty to be used for generation.\n            Defaults to 0.0.\n        presence_penalty (float, optional): the presence penalty to be used for generation.\n            Defaults to 0.0.\n        temperature (float, optional): the temperature to be used for generation.\n            Defaults to 1.0.\n        top_p (float, optional): the top-p value to be used for generation.\n            Defaults to 1.0.\n        num_threads (Union[int, None], optional): the number of threads to be used\n            for parallel generation. If `None`, no parallel generation will be performed.\n            Defaults to `None`.\n        prompt_format (Union[SupportedFormats, None], optional): the format to be used\n            for the prompt. If `None`, the default format of the task will be used, available\n            formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n            but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n            will be used if no `prompt_formatting_fn` is provided.\n        prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n            applied to the prompt before generation. If `None`, no formatting will be applied.\n            Defaults to `None`.\n\n    Raises:\n        AssertionError: if the provided `model` is not available in your OpenAI account.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask as Task\n        &gt;&gt;&gt; from distilabel.llm import OpenAILLM\n        &gt;&gt;&gt; task = Task()\n        &gt;&gt;&gt; llm = OpenAILLM(model=\"gpt-3.5-turbo\", task=task)\n    \"\"\"\n    super().__init__(\n        task=task,\n        num_threads=num_threads,\n        prompt_format=prompt_format,\n        prompt_formatting_fn=prompt_formatting_fn,\n    )\n\n    if not _OPENAI_AVAILABLE:\n        raise ImportError(\n            \"`OpenAILLM` cannot be used as `openai` is not installed, please \"\n            \" install it with `pip install openai`.\"\n        )\n\n    self.max_tokens = max_new_tokens\n    self.frequency_penalty = frequency_penalty\n    self.presence_penalty = presence_penalty\n    self.temperature = temperature\n    self.top_p = top_p\n\n    self.client = client or OpenAI(api_key=openai_api_key, max_retries=6)\n\n    assert (\n        model in self.available_models\n    ), f\"Provided `model` is not available in your OpenAI account, available models are {self.available_models}\"\n    self.model = model\n</code></pre>"},{"location":"reference/distilabel/llm/utils/","title":"utils","text":""},{"location":"reference/distilabel/llm/utils/#distilabel.llm.utils.LLMOutput","title":"<code>LLMOutput</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>A type for the output of an LLM.</p> Source code in <code>src/distilabel/llm/utils.py</code> <pre><code>class LLMOutput(TypedDict):\n    \"\"\"A type for the output of an LLM.\"\"\"\n\n    model_name: str\n    prompt_used: Any\n    raw_output: Any\n    parsed_output: Optional[Any]\n</code></pre>"},{"location":"reference/distilabel/llm/vllm/","title":"vllm","text":""},{"location":"reference/distilabel/llm/vllm/#distilabel.llm.vllm.vLLM","title":"<code>vLLM</code>","text":"<p>             Bases: <code>LLM</code></p> Source code in <code>src/distilabel/llm/vllm.py</code> <pre><code>class vLLM(LLM):\n    def __init__(\n        self,\n        vllm: \"_vLLM\",\n        task: \"Task\",\n        max_new_tokens: int = 128,\n        presence_penalty: float = 0.0,\n        frequency_penalty: float = 0.0,\n        temperature: float = 1.0,\n        top_p: float = 1.0,\n        top_k: int = -1,\n        prompt_format: Union[\"SupportedFormats\", None] = None,\n        prompt_formatting_fn: Union[Callable[..., str], None] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the vLLM class.\n\n        Args:\n            vllm (_vLLM): the vLLM model to be used.\n            task (Task): the task to be performed by the LLM.\n            max_new_tokens (int, optional): the maximum number of tokens to be generated.\n                Defaults to 128.\n            presence_penalty (float, optional): the presence penalty to be used for generation.\n                Defaults to 0.0.\n            frequency_penalty (float, optional): the frequency penalty to be used for generation.\n                Defaults to 0.0.\n            temperature (float, optional): the temperature to be used for generation.\n                Defaults to 1.0.\n            top_p (float, optional): the top-p value to be used for generation.\n                Defaults to 1.0.\n            top_k (int, optional): the top-k value to be used for generation.\n                Defaults to -1.\n            prompt_format (Union[SupportedFormats, None], optional): the format to be used\n                for the prompt. If `None`, the default format of the task will be used, available\n                formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n                but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n                will be used if no `prompt_formatting_fn` is provided.\n            prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n                applied to the prompt before generation. If `None`, no formatting will be applied.\n\n        Examples:\n            &gt;&gt;&gt; from vllm import LLM\n            &gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask as Task\n            &gt;&gt;&gt; from distilabel.llm import vLLM\n            &gt;&gt;&gt; model = LLM(model=\"gpt2\")\n            &gt;&gt;&gt; task = Task()\n            &gt;&gt;&gt; llm = vLLM(model=model, task=task)\n        \"\"\"\n        super().__init__(\n            task=task,\n            prompt_format=prompt_format,\n            prompt_formatting_fn=prompt_formatting_fn,\n        )\n\n        if not _VLLM_AVAILABLE:\n            raise ImportError(\n                \"`vLLM` cannot be used as `vllm` is not installed, please \"\n                \" install it with `pip install vllm`.\"\n            )\n\n        self.presence_penalty = presence_penalty\n        self.frequency_penalty = frequency_penalty\n        self.temperature = temperature\n        self.top_p = top_p\n        self.top_k = top_k\n        self.max_tokens = max_new_tokens\n\n        self.vllm = vllm\n\n    def __rich_repr__(self) -&gt; Generator[Any, None, None]:\n        yield from super().__rich_repr__()\n        yield (\n            \"parameters\",\n            {\n                \"max_tokens\": self.max_tokens,\n                \"presence_penalty\": self.presence_penalty,\n                \"frequency_penalty\": self.frequency_penalty,\n                \"temperature\": self.temperature,\n                \"top_p\": self.top_p,\n                \"top_k\": self.top_k,\n            },\n        )\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the name of the vLLM model.\"\"\"\n        return self.vllm.llm_engine.model_config.model  # type: ignore\n\n    def _generate(\n        self, inputs: List[Dict[str, Any]], num_generations: int = 1\n    ) -&gt; List[List[LLMOutput]]:\n        \"\"\"Generates `num_generations` for each input in `inputs`.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n            num_generations (int, optional): the number of generations to be performed for each\n                input. Defaults to 1.\n\n        Returns:\n            List[List[LLMOutput]]: the outputs of the LLM.\n        \"\"\"\n        prompts = self._generate_prompts(\n            inputs, default_format=None, expected_output_type=str\n        )\n        requests = self.vllm.generate(\n            prompts,\n            SamplingParams(  # type: ignore\n                n=num_generations,\n                presence_penalty=self.presence_penalty,\n                frequency_penalty=self.frequency_penalty,\n                temperature=self.temperature,\n                top_p=self.top_p,\n                top_k=self.top_k,\n                max_tokens=self.max_tokens,\n            ),\n            use_tqdm=False,  # type: ignore\n        )\n        outputs = []\n        for request, prompt in zip(requests, prompts):\n            output = []\n            for request_output in request.outputs:\n                try:\n                    parsed_output = self.task.parse_output(request_output.text)\n                except Exception as e:\n                    logger.error(f\"Error parsing vLLM output: {e}\")\n                    parsed_output = None\n                output.append(\n                    LLMOutput(\n                        model_name=self.model_name,\n                        prompt_used=prompt,\n                        raw_output=request_output.text,\n                        parsed_output=parsed_output,\n                    )\n                )\n            outputs.append(output)\n        return outputs\n</code></pre>"},{"location":"reference/distilabel/llm/vllm/#distilabel.llm.vllm.vLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the name of the vLLM model.</p>"},{"location":"reference/distilabel/llm/vllm/#distilabel.llm.vllm.vLLM.__init__","title":"<code>__init__(vllm, task, max_new_tokens=128, presence_penalty=0.0, frequency_penalty=0.0, temperature=1.0, top_p=1.0, top_k=-1, prompt_format=None, prompt_formatting_fn=None)</code>","text":"<p>Initializes the vLLM class.</p> <p>Parameters:</p> Name Type Description Default <code>vllm</code> <code>LLM</code> <p>the vLLM model to be used.</p> required <code>task</code> <code>Task</code> <p>the task to be performed by the LLM.</p> required <code>max_new_tokens</code> <code>int</code> <p>the maximum number of tokens to be generated. Defaults to 128.</p> <code>128</code> <code>presence_penalty</code> <code>float</code> <p>the presence penalty to be used for generation. Defaults to 0.0.</p> <code>0.0</code> <code>frequency_penalty</code> <code>float</code> <p>the frequency penalty to be used for generation. Defaults to 0.0.</p> <code>0.0</code> <code>temperature</code> <code>float</code> <p>the temperature to be used for generation. Defaults to 1.0.</p> <code>1.0</code> <code>top_p</code> <code>float</code> <p>the top-p value to be used for generation. Defaults to 1.0.</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>the top-k value to be used for generation. Defaults to -1.</p> <code>-1</code> <code>prompt_format</code> <code>Union[SupportedFormats, None]</code> <p>the format to be used for the prompt. If <code>None</code>, the default format of the task will be used, available formats are <code>openai</code>, <code>chatml</code>, <code>llama2</code>, <code>zephyr</code>, and <code>default</code>. Defaults to <code>None</code>, but <code>default</code> (concatenation of <code>system_prompt</code> and <code>formatted_prompt</code> with a line-break) will be used if no <code>prompt_formatting_fn</code> is provided.</p> <code>None</code> <code>prompt_formatting_fn</code> <code>Union[Callable[..., str], None]</code> <p>a function to be applied to the prompt before generation. If <code>None</code>, no formatting will be applied.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from vllm import LLM\n&gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask as Task\n&gt;&gt;&gt; from distilabel.llm import vLLM\n&gt;&gt;&gt; model = LLM(model=\"gpt2\")\n&gt;&gt;&gt; task = Task()\n&gt;&gt;&gt; llm = vLLM(model=model, task=task)\n</code></pre> Source code in <code>src/distilabel/llm/vllm.py</code> <pre><code>def __init__(\n    self,\n    vllm: \"_vLLM\",\n    task: \"Task\",\n    max_new_tokens: int = 128,\n    presence_penalty: float = 0.0,\n    frequency_penalty: float = 0.0,\n    temperature: float = 1.0,\n    top_p: float = 1.0,\n    top_k: int = -1,\n    prompt_format: Union[\"SupportedFormats\", None] = None,\n    prompt_formatting_fn: Union[Callable[..., str], None] = None,\n) -&gt; None:\n    \"\"\"Initializes the vLLM class.\n\n    Args:\n        vllm (_vLLM): the vLLM model to be used.\n        task (Task): the task to be performed by the LLM.\n        max_new_tokens (int, optional): the maximum number of tokens to be generated.\n            Defaults to 128.\n        presence_penalty (float, optional): the presence penalty to be used for generation.\n            Defaults to 0.0.\n        frequency_penalty (float, optional): the frequency penalty to be used for generation.\n            Defaults to 0.0.\n        temperature (float, optional): the temperature to be used for generation.\n            Defaults to 1.0.\n        top_p (float, optional): the top-p value to be used for generation.\n            Defaults to 1.0.\n        top_k (int, optional): the top-k value to be used for generation.\n            Defaults to -1.\n        prompt_format (Union[SupportedFormats, None], optional): the format to be used\n            for the prompt. If `None`, the default format of the task will be used, available\n            formats are `openai`, `chatml`, `llama2`, `zephyr`, and `default`. Defaults to `None`,\n            but `default` (concatenation of `system_prompt` and `formatted_prompt` with a line-break)\n            will be used if no `prompt_formatting_fn` is provided.\n        prompt_formatting_fn (Union[Callable[..., str], None], optional): a function to be\n            applied to the prompt before generation. If `None`, no formatting will be applied.\n\n    Examples:\n        &gt;&gt;&gt; from vllm import LLM\n        &gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask as Task\n        &gt;&gt;&gt; from distilabel.llm import vLLM\n        &gt;&gt;&gt; model = LLM(model=\"gpt2\")\n        &gt;&gt;&gt; task = Task()\n        &gt;&gt;&gt; llm = vLLM(model=model, task=task)\n    \"\"\"\n    super().__init__(\n        task=task,\n        prompt_format=prompt_format,\n        prompt_formatting_fn=prompt_formatting_fn,\n    )\n\n    if not _VLLM_AVAILABLE:\n        raise ImportError(\n            \"`vLLM` cannot be used as `vllm` is not installed, please \"\n            \" install it with `pip install vllm`.\"\n        )\n\n    self.presence_penalty = presence_penalty\n    self.frequency_penalty = frequency_penalty\n    self.temperature = temperature\n    self.top_p = top_p\n    self.top_k = top_k\n    self.max_tokens = max_new_tokens\n\n    self.vllm = vllm\n</code></pre>"},{"location":"reference/distilabel/llm/huggingface/","title":"huggingface","text":""},{"location":"reference/distilabel/llm/huggingface/inference_endpoints/","title":"inference_endpoints","text":""},{"location":"reference/distilabel/llm/huggingface/inference_endpoints/#distilabel.llm.huggingface.inference_endpoints.InferenceEndpointsLLM","title":"<code>InferenceEndpointsLLM</code>","text":"<p>             Bases: <code>LLM</code></p> Source code in <code>src/distilabel/llm/huggingface/inference_endpoints.py</code> <pre><code>class InferenceEndpointsLLM(LLM):\n    def __init__(\n        self,\n        endpoint_name: str,\n        task: \"Task\",\n        endpoint_namespace: Union[str, None] = None,\n        token: Union[str, None] = None,\n        max_new_tokens: int = 128,\n        repetition_penalty: Union[float, None] = None,\n        seed: Union[int, None] = None,\n        do_sample: bool = False,\n        temperature: Union[float, None] = None,\n        top_k: Union[int, None] = None,\n        top_p: Union[float, None] = None,\n        typical_p: Union[float, None] = None,\n        num_threads: Union[int, None] = None,\n        prompt_format: Union[\"SupportedFormats\", None] = None,\n        prompt_formatting_fn: Union[Callable[..., str], None] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the InferenceEndpointsLLM class.\n\n        Args:\n            endpoint_name (str): The name of the endpoint.\n            task (Task): The task to be performed by the LLM.\n            endpoint_namespace (Union[str, None]): The namespace of the endpoint. Defaults to None.\n            token (Union[str, None]): The token for the endpoint. Defaults to None.\n            max_new_tokens (int): The maximum number of tokens to be generated. Defaults to 128.\n            repetition_penalty (Union[float, None]): The repetition penalty to be used for generation. Defaults to None.\n            seed (Union[int, None]): The seed for generation. Defaults to None.\n            do_sample (bool): Whether to do sampling. Defaults to False.\n            temperature (Union[float, None]): The temperature for generation. Defaults to None.\n            top_k (Union[int, None]): The top_k for generation. Defaults to None.\n            top_p (Union[float, None]): The top_p for generation. Defaults to None.\n            typical_p (Union[float, None]): The typical_p for generation. Defaults to None.\n            num_threads (Union[int, None]): The number of threads. Defaults to None.\n            prompt_format (Union[\"SupportedFormats\", None]): The format of the prompt. Defaults to None.\n            prompt_formatting_fn (Union[Callable[..., str], None]): The function for formatting the prompt. Defaults to None.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask as Task\n            &gt;&gt;&gt; from distilabel.llm import InferenceEndpointsLLM\n            &gt;&gt;&gt; task = Task()\n            &gt;&gt;&gt; llm = InferenceEndpointsLLM(\n            ...     endpoint_name=\"&lt;INFERENCE_ENDPOINT_NAME&gt;\",\n            ...     task=task,\n            ... )\n        \"\"\"\n        super().__init__(\n            task=task,\n            num_threads=num_threads,\n            prompt_format=prompt_format,\n            prompt_formatting_fn=prompt_formatting_fn,\n        )\n\n        if not _HUGGINGFACE_HUB_AVAILABLE:\n            raise ImportError(\n                \"`InferenceEndpointsLLM` cannot be used as `huggingface-hub` is not \"\n                \"installed, please install it with `pip install huggingface-hub`.\"\n            )\n\n        self.do_sample = do_sample\n        self.max_new_tokens = max_new_tokens\n        self.repetition_penalty = repetition_penalty\n        self.seed = seed\n        self.temperature = temperature\n        self.top_k = top_k\n        self.top_p = top_p\n        self.typical_p = typical_p\n\n        self.inference_endpoint = get_inference_endpoint(\n            name=endpoint_name, namespace=endpoint_namespace, token=token\n        )\n        self.inference_endpoint.wait(timeout=30)\n\n    def __rich_repr__(self) -&gt; Generator[Any, None, None]:\n        yield from super().__rich_repr__()\n        yield (\n            \"parameters\",\n            {\n                \"do_sample\": self.do_sample,\n                \"max_new_tokens\": self.max_new_tokens,\n                \"repetition_penalty\": self.repetition_penalty,\n                \"seed\": self.seed,\n                \"temperature\": self.temperature,\n                \"top_k\": self.top_k,\n                \"top_p\": self.top_p,\n                \"typical_p\": self.typical_p,\n            },\n        )\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the model name of the endpoint.\"\"\"\n        return self.inference_endpoint.repository\n\n    @retry(\n        retry=retry_if_exception_type(_INFERENCE_ENDPOINTS_API_RETRY_ON_EXCEPTIONS),\n        stop=stop_after_attempt(_INFERENCE_ENDPOINTS_API_STOP_AFTER_ATTEMPT),\n        wait=wait_random_exponential(\n            multiplier=_INFERENCE_ENDPOINTS_API_WAIT_RANDOM_EXPONENTIAL_MULTIPLIER,\n            max=_INFERENCE_ENDPOINTS_API_WAIT_RANDOM_EXPONENTIAL_MAX,\n        ),\n        before_sleep=before_sleep_log(logger, logging.INFO),\n        after=after_log(logger, logging.INFO),\n    )\n    def _text_generation_with_backoff(self, **kwargs: Any) -&gt; Any:\n        \"\"\"Performs text generation with backoff in case of an error.\"\"\"\n        return self.inference_endpoint.client.text_generation(**kwargs)  # type: ignore\n\n    def _generate(\n        self, inputs: List[Dict[str, Any]], num_generations: int = 1\n    ) -&gt; List[List[LLMOutput]]:\n        \"\"\"Generates `num_generations` for each input in `inputs`.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n            num_generations (int, optional): the number of generations to be performed for each\n                input. Defaults to 1.\n\n        Returns:\n            List[List[LLMOutput]]: the outputs of the LLM.\n        \"\"\"\n        prompts = self._generate_prompts(\n            inputs, default_format=None, expected_output_type=str\n        )\n        outputs = []\n        for prompt in prompts:\n            raw_responses = [\n                self._text_generation_with_backoff(\n                    prompt=prompt,\n                    do_sample=self.do_sample,\n                    max_new_tokens=self.max_new_tokens,\n                    repetition_penalty=self.repetition_penalty,\n                    seed=self.seed,\n                    temperature=self.temperature,\n                    top_k=self.top_k,\n                    top_p=self.top_p,\n                    typical_p=self.typical_p,\n                )\n                for _ in range(num_generations)\n            ]\n            output = []\n            for raw_response in raw_responses:\n                try:\n                    parsed_response = self.task.parse_output(raw_response)\n                except Exception as e:\n                    logger.error(f\"Error parsing Inference Endpoints output: {e}\")\n                    parsed_response = None\n                output.append(\n                    LLMOutput(\n                        model_name=self.model_name,\n                        prompt_used=prompt,\n                        raw_output=raw_response,\n                        parsed_output=parsed_response,\n                    )\n                )\n            outputs.append(output)\n        return outputs\n</code></pre>"},{"location":"reference/distilabel/llm/huggingface/inference_endpoints/#distilabel.llm.huggingface.inference_endpoints.InferenceEndpointsLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the model name of the endpoint.</p>"},{"location":"reference/distilabel/llm/huggingface/inference_endpoints/#distilabel.llm.huggingface.inference_endpoints.InferenceEndpointsLLM.__init__","title":"<code>__init__(endpoint_name, task, endpoint_namespace=None, token=None, max_new_tokens=128, repetition_penalty=None, seed=None, do_sample=False, temperature=None, top_k=None, top_p=None, typical_p=None, num_threads=None, prompt_format=None, prompt_formatting_fn=None)</code>","text":"<p>Initializes the InferenceEndpointsLLM class.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint_name</code> <code>str</code> <p>The name of the endpoint.</p> required <code>task</code> <code>Task</code> <p>The task to be performed by the LLM.</p> required <code>endpoint_namespace</code> <code>Union[str, None]</code> <p>The namespace of the endpoint. Defaults to None.</p> <code>None</code> <code>token</code> <code>Union[str, None]</code> <p>The token for the endpoint. Defaults to None.</p> <code>None</code> <code>max_new_tokens</code> <code>int</code> <p>The maximum number of tokens to be generated. Defaults to 128.</p> <code>128</code> <code>repetition_penalty</code> <code>Union[float, None]</code> <p>The repetition penalty to be used for generation. Defaults to None.</p> <code>None</code> <code>seed</code> <code>Union[int, None]</code> <p>The seed for generation. Defaults to None.</p> <code>None</code> <code>do_sample</code> <code>bool</code> <p>Whether to do sampling. Defaults to False.</p> <code>False</code> <code>temperature</code> <code>Union[float, None]</code> <p>The temperature for generation. Defaults to None.</p> <code>None</code> <code>top_k</code> <code>Union[int, None]</code> <p>The top_k for generation. Defaults to None.</p> <code>None</code> <code>top_p</code> <code>Union[float, None]</code> <p>The top_p for generation. Defaults to None.</p> <code>None</code> <code>typical_p</code> <code>Union[float, None]</code> <p>The typical_p for generation. Defaults to None.</p> <code>None</code> <code>num_threads</code> <code>Union[int, None]</code> <p>The number of threads. Defaults to None.</p> <code>None</code> <code>prompt_format</code> <code>Union[SupportedFormats, None]</code> <p>The format of the prompt. Defaults to None.</p> <code>None</code> <code>prompt_formatting_fn</code> <code>Union[Callable[..., str], None]</code> <p>The function for formatting the prompt. Defaults to None.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask as Task\n&gt;&gt;&gt; from distilabel.llm import InferenceEndpointsLLM\n&gt;&gt;&gt; task = Task()\n&gt;&gt;&gt; llm = InferenceEndpointsLLM(\n...     endpoint_name=\"&lt;INFERENCE_ENDPOINT_NAME&gt;\",\n...     task=task,\n... )\n</code></pre> Source code in <code>src/distilabel/llm/huggingface/inference_endpoints.py</code> <pre><code>def __init__(\n    self,\n    endpoint_name: str,\n    task: \"Task\",\n    endpoint_namespace: Union[str, None] = None,\n    token: Union[str, None] = None,\n    max_new_tokens: int = 128,\n    repetition_penalty: Union[float, None] = None,\n    seed: Union[int, None] = None,\n    do_sample: bool = False,\n    temperature: Union[float, None] = None,\n    top_k: Union[int, None] = None,\n    top_p: Union[float, None] = None,\n    typical_p: Union[float, None] = None,\n    num_threads: Union[int, None] = None,\n    prompt_format: Union[\"SupportedFormats\", None] = None,\n    prompt_formatting_fn: Union[Callable[..., str], None] = None,\n) -&gt; None:\n    \"\"\"Initializes the InferenceEndpointsLLM class.\n\n    Args:\n        endpoint_name (str): The name of the endpoint.\n        task (Task): The task to be performed by the LLM.\n        endpoint_namespace (Union[str, None]): The namespace of the endpoint. Defaults to None.\n        token (Union[str, None]): The token for the endpoint. Defaults to None.\n        max_new_tokens (int): The maximum number of tokens to be generated. Defaults to 128.\n        repetition_penalty (Union[float, None]): The repetition penalty to be used for generation. Defaults to None.\n        seed (Union[int, None]): The seed for generation. Defaults to None.\n        do_sample (bool): Whether to do sampling. Defaults to False.\n        temperature (Union[float, None]): The temperature for generation. Defaults to None.\n        top_k (Union[int, None]): The top_k for generation. Defaults to None.\n        top_p (Union[float, None]): The top_p for generation. Defaults to None.\n        typical_p (Union[float, None]): The typical_p for generation. Defaults to None.\n        num_threads (Union[int, None]): The number of threads. Defaults to None.\n        prompt_format (Union[\"SupportedFormats\", None]): The format of the prompt. Defaults to None.\n        prompt_formatting_fn (Union[Callable[..., str], None]): The function for formatting the prompt. Defaults to None.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask as Task\n        &gt;&gt;&gt; from distilabel.llm import InferenceEndpointsLLM\n        &gt;&gt;&gt; task = Task()\n        &gt;&gt;&gt; llm = InferenceEndpointsLLM(\n        ...     endpoint_name=\"&lt;INFERENCE_ENDPOINT_NAME&gt;\",\n        ...     task=task,\n        ... )\n    \"\"\"\n    super().__init__(\n        task=task,\n        num_threads=num_threads,\n        prompt_format=prompt_format,\n        prompt_formatting_fn=prompt_formatting_fn,\n    )\n\n    if not _HUGGINGFACE_HUB_AVAILABLE:\n        raise ImportError(\n            \"`InferenceEndpointsLLM` cannot be used as `huggingface-hub` is not \"\n            \"installed, please install it with `pip install huggingface-hub`.\"\n        )\n\n    self.do_sample = do_sample\n    self.max_new_tokens = max_new_tokens\n    self.repetition_penalty = repetition_penalty\n    self.seed = seed\n    self.temperature = temperature\n    self.top_k = top_k\n    self.top_p = top_p\n    self.typical_p = typical_p\n\n    self.inference_endpoint = get_inference_endpoint(\n        name=endpoint_name, namespace=endpoint_namespace, token=token\n    )\n    self.inference_endpoint.wait(timeout=30)\n</code></pre>"},{"location":"reference/distilabel/llm/huggingface/transformers/","title":"transformers","text":""},{"location":"reference/distilabel/llm/huggingface/transformers/#distilabel.llm.huggingface.transformers.TransformersLLM","title":"<code>TransformersLLM</code>","text":"<p>             Bases: <code>LLM</code></p> Source code in <code>src/distilabel/llm/huggingface/transformers.py</code> <pre><code>class TransformersLLM(LLM):\n    def __init__(\n        self,\n        model: \"PreTrainedModel\",\n        tokenizer: \"PreTrainedTokenizer\",\n        task: \"Task\",\n        max_new_tokens: int = 128,\n        do_sample: bool = False,\n        temperature: float = 1.0,\n        top_k: int = 50,\n        top_p: float = 1.0,\n        typical_p: float = 1.0,\n        num_threads: Union[int, None] = None,\n        prompt_format: Union[\"SupportedFormats\", None] = None,\n        prompt_formatting_fn: Union[Callable[..., str], None] = None,\n    ) -&gt; None:\n        \"\"\"Initializes the TransformersLLM class.\n\n        Args:\n            model (PreTrainedModel): the model to be used for generation.\n            tokenizer (PreTrainedTokenizer): the tokenizer to be used for generation.\n            task (Task): the task to be performed by the LLM.\n            max_new_tokens (int, optional): the maximum number of tokens to be generated.\n                Defaults to 128.\n            do_sample (bool, optional): whether to sample from the model or not.\n                Defaults to False.\n            temperature (float, optional): the temperature to be used for generation.\n                Defaults to 1.0.\n            top_k (int, optional): the top-k value to be used for generation.\n                Defaults to 50.\n            top_p (float, optional): the top-p value to be used for generation.\n                Defaults to 1.0.\n            typical_p (float, optional): the typical-p value to be used for generation.\n                Defaults to 1.0.\n            num_threads (Union[int, None], optional): the number of threads to be used for generation.\n                If `None`, the number of threads will be set to the number of available CPUs.\n                Defaults to `None`.\n            prompt_format (Union[SupportedFormats, None], optional): the format to be used\n                for formatting the prompts. If `None`, the prompts will not be formatted.\n                Defaults to `None`.\n            prompt_formatting_fn (Union[Callable[..., str], None], optional): the function to be used\n                for formatting the prompts. If `None`, the prompts will not be formatted.\n\n        Examples:\n            &gt;&gt;&gt; from transformers import AutoModelForCausalLM, AutoTokenizer\n            &gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask as Task\n            &gt;&gt;&gt; from distilabel.llm import TransformersLLM\n            &gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n            &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n            &gt;&gt;&gt; task = Task()\n            &gt;&gt;&gt; llm = TransformersLLM(\n            ...     model=model,\n            ...     tokenizer=tokenizer,\n            ...     task=task,\n            ... )\n        \"\"\"\n        super().__init__(\n            task=task,\n            num_threads=num_threads,\n            prompt_format=prompt_format,\n            prompt_formatting_fn=prompt_formatting_fn,\n        )\n\n        self.max_new_tokens = max_new_tokens\n        self.do_sample = do_sample\n        self.temperature = temperature\n        self.top_k = top_k\n        self.top_p = top_p\n        self.typical_p = typical_p\n\n        self.model = model\n        if self.device != \"cpu\":\n            self.model.to(self.device)\n\n        self.tokenizer = tokenizer\n        self.tokenizer.padding_side = \"left\"\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n        if (\n            hasattr(self.tokenizer, \"use_default_system_prompt\")\n            and self.tokenizer.use_default_system_prompt  # type: ignore\n        ):\n            # The `tokenizer` also has a method named `apply_chat_template` that expects a `Conversation` as OpenAI does with the ChatML format\n            warnings.warn(\n                \"The provided `tokenizer` has `use_default_system_prompt=True` which means that the default system prompt will be used, which may collide with the `task` provided as an arg to this class.\",\n                UserWarning,\n                stacklevel=2,\n            )\n\n    def __rich_repr__(self) -&gt; Generator[Any, None, None]:\n        yield from super().__rich_repr__()\n        yield (\n            \"parameters\",\n            {\n                \"max_new_tokens\": self.max_new_tokens,\n                \"do_sample\": self.do_sample,\n                \"temperature\": self.temperature,\n                \"top_k\": self.top_k,\n                \"top_p\": self.top_p,\n                \"typical_p\": self.typical_p,\n            },\n        )\n\n    @property\n    def model_name(self) -&gt; str:\n        \"\"\"Returns the name of the Transformers model.\"\"\"\n        return self.model.config.name_or_path\n\n    @cached_property\n    def device(self) -&gt; \"device\":\n        \"\"\"Returns the device to be used for generation.\"\"\"\n        if torch.cuda.is_available():\n            return torch.device(\"cuda\")\n        if torch.backends.mps.is_available() and torch.backends.mps.is_built():  # type: ignore\n            return torch.device(\"mps\")\n        return torch.device(\"cpu\")\n\n    def _generate(\n        self, inputs: List[Dict[str, Any]], num_generations: int = 1\n    ) -&gt; List[List[LLMOutput]]:\n        \"\"\"Generates `num_generations` for each input in `inputs`.\n\n        Args:\n            inputs (List[Dict[str, Any]]): the inputs to be used for generation.\n            num_generations (int, optional): the number of generations to be performed for each\n                input. Defaults to 1.\n\n        Returns:\n            List[List[LLMOutput]]: the outputs of the LLM.\n        \"\"\"\n        prompts = self._generate_prompts(\n            inputs, default_format=None, expected_output_type=str\n        )\n        encodings = self.tokenizer(prompts, padding=True, return_tensors=\"pt\")\n        if self.device != \"cpu\":\n            encodings = encodings.to(self.device)\n        with torch.inference_mode():\n            generated_ids = self.model.generate(\n                **encodings,  # type: ignore\n                pad_token_id=self.tokenizer.eos_token_id,\n                generation_config=GenerationConfig(\n                    do_sample=self.do_sample,\n                    temperature=self.temperature,\n                    max_new_tokens=self.max_new_tokens,\n                    top_k=self.top_k,\n                    top_p=self.top_p,\n                    typical_p=self.typical_p,\n                    num_return_sequences=num_generations,\n                ),\n            )\n        raw_outputs = self.tokenizer.batch_decode(\n            generated_ids[:, encodings.input_ids.shape[1] :],\n            skip_special_tokens=True,\n            clean_up_tokenization_spaces=True,\n        )\n        outputs = []\n        for prompt, i in zip(prompts, range(0, len(raw_outputs), num_generations)):\n            output = []\n            for raw_output in raw_outputs[i : i + num_generations]:\n                try:\n                    parsed_output = self.task.parse_output(raw_output)\n                except Exception as e:\n                    logger.error(f\"Error parsing Transformers output: {e}\")\n                    parsed_output = None\n                output.append(\n                    LLMOutput(\n                        model_name=self.model_name,\n                        prompt_used=prompt,\n                        raw_output=raw_output,\n                        parsed_output=parsed_output,\n                    )\n                )\n            outputs.append(output)\n        return outputs\n</code></pre>"},{"location":"reference/distilabel/llm/huggingface/transformers/#distilabel.llm.huggingface.transformers.TransformersLLM.device","title":"<code>device: device</code>  <code>cached</code> <code>property</code>","text":"<p>Returns the device to be used for generation.</p>"},{"location":"reference/distilabel/llm/huggingface/transformers/#distilabel.llm.huggingface.transformers.TransformersLLM.model_name","title":"<code>model_name: str</code>  <code>property</code>","text":"<p>Returns the name of the Transformers model.</p>"},{"location":"reference/distilabel/llm/huggingface/transformers/#distilabel.llm.huggingface.transformers.TransformersLLM.__init__","title":"<code>__init__(model, tokenizer, task, max_new_tokens=128, do_sample=False, temperature=1.0, top_k=50, top_p=1.0, typical_p=1.0, num_threads=None, prompt_format=None, prompt_formatting_fn=None)</code>","text":"<p>Initializes the TransformersLLM class.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>PreTrainedModel</code> <p>the model to be used for generation.</p> required <code>tokenizer</code> <code>PreTrainedTokenizer</code> <p>the tokenizer to be used for generation.</p> required <code>task</code> <code>Task</code> <p>the task to be performed by the LLM.</p> required <code>max_new_tokens</code> <code>int</code> <p>the maximum number of tokens to be generated. Defaults to 128.</p> <code>128</code> <code>do_sample</code> <code>bool</code> <p>whether to sample from the model or not. Defaults to False.</p> <code>False</code> <code>temperature</code> <code>float</code> <p>the temperature to be used for generation. Defaults to 1.0.</p> <code>1.0</code> <code>top_k</code> <code>int</code> <p>the top-k value to be used for generation. Defaults to 50.</p> <code>50</code> <code>top_p</code> <code>float</code> <p>the top-p value to be used for generation. Defaults to 1.0.</p> <code>1.0</code> <code>typical_p</code> <code>float</code> <p>the typical-p value to be used for generation. Defaults to 1.0.</p> <code>1.0</code> <code>num_threads</code> <code>Union[int, None]</code> <p>the number of threads to be used for generation. If <code>None</code>, the number of threads will be set to the number of available CPUs. Defaults to <code>None</code>.</p> <code>None</code> <code>prompt_format</code> <code>Union[SupportedFormats, None]</code> <p>the format to be used for formatting the prompts. If <code>None</code>, the prompts will not be formatted. Defaults to <code>None</code>.</p> <code>None</code> <code>prompt_formatting_fn</code> <code>Union[Callable[..., str], None]</code> <p>the function to be used for formatting the prompts. If <code>None</code>, the prompts will not be formatted.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from transformers import AutoModelForCausalLM, AutoTokenizer\n&gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask as Task\n&gt;&gt;&gt; from distilabel.llm import TransformersLLM\n&gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n&gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n&gt;&gt;&gt; task = Task()\n&gt;&gt;&gt; llm = TransformersLLM(\n...     model=model,\n...     tokenizer=tokenizer,\n...     task=task,\n... )\n</code></pre> Source code in <code>src/distilabel/llm/huggingface/transformers.py</code> <pre><code>def __init__(\n    self,\n    model: \"PreTrainedModel\",\n    tokenizer: \"PreTrainedTokenizer\",\n    task: \"Task\",\n    max_new_tokens: int = 128,\n    do_sample: bool = False,\n    temperature: float = 1.0,\n    top_k: int = 50,\n    top_p: float = 1.0,\n    typical_p: float = 1.0,\n    num_threads: Union[int, None] = None,\n    prompt_format: Union[\"SupportedFormats\", None] = None,\n    prompt_formatting_fn: Union[Callable[..., str], None] = None,\n) -&gt; None:\n    \"\"\"Initializes the TransformersLLM class.\n\n    Args:\n        model (PreTrainedModel): the model to be used for generation.\n        tokenizer (PreTrainedTokenizer): the tokenizer to be used for generation.\n        task (Task): the task to be performed by the LLM.\n        max_new_tokens (int, optional): the maximum number of tokens to be generated.\n            Defaults to 128.\n        do_sample (bool, optional): whether to sample from the model or not.\n            Defaults to False.\n        temperature (float, optional): the temperature to be used for generation.\n            Defaults to 1.0.\n        top_k (int, optional): the top-k value to be used for generation.\n            Defaults to 50.\n        top_p (float, optional): the top-p value to be used for generation.\n            Defaults to 1.0.\n        typical_p (float, optional): the typical-p value to be used for generation.\n            Defaults to 1.0.\n        num_threads (Union[int, None], optional): the number of threads to be used for generation.\n            If `None`, the number of threads will be set to the number of available CPUs.\n            Defaults to `None`.\n        prompt_format (Union[SupportedFormats, None], optional): the format to be used\n            for formatting the prompts. If `None`, the prompts will not be formatted.\n            Defaults to `None`.\n        prompt_formatting_fn (Union[Callable[..., str], None], optional): the function to be used\n            for formatting the prompts. If `None`, the prompts will not be formatted.\n\n    Examples:\n        &gt;&gt;&gt; from transformers import AutoModelForCausalLM, AutoTokenizer\n        &gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask as Task\n        &gt;&gt;&gt; from distilabel.llm import TransformersLLM\n        &gt;&gt;&gt; model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n        &gt;&gt;&gt; tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n        &gt;&gt;&gt; task = Task()\n        &gt;&gt;&gt; llm = TransformersLLM(\n        ...     model=model,\n        ...     tokenizer=tokenizer,\n        ...     task=task,\n        ... )\n    \"\"\"\n    super().__init__(\n        task=task,\n        num_threads=num_threads,\n        prompt_format=prompt_format,\n        prompt_formatting_fn=prompt_formatting_fn,\n    )\n\n    self.max_new_tokens = max_new_tokens\n    self.do_sample = do_sample\n    self.temperature = temperature\n    self.top_k = top_k\n    self.top_p = top_p\n    self.typical_p = typical_p\n\n    self.model = model\n    if self.device != \"cpu\":\n        self.model.to(self.device)\n\n    self.tokenizer = tokenizer\n    self.tokenizer.padding_side = \"left\"\n    if self.tokenizer.pad_token is None:\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n    if (\n        hasattr(self.tokenizer, \"use_default_system_prompt\")\n        and self.tokenizer.use_default_system_prompt  # type: ignore\n    ):\n        # The `tokenizer` also has a method named `apply_chat_template` that expects a `Conversation` as OpenAI does with the ChatML format\n        warnings.warn(\n            \"The provided `tokenizer` has `use_default_system_prompt=True` which means that the default system prompt will be used, which may collide with the `task` provided as an arg to this class.\",\n            UserWarning,\n            stacklevel=2,\n        )\n</code></pre>"},{"location":"reference/distilabel/tasks/","title":"tasks","text":""},{"location":"reference/distilabel/tasks/#distilabel.tasks.JudgeLMTask","title":"<code>JudgeLMTask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>PreferenceTask</code></p> <p>A <code>PreferenceTask</code> following the prompt templated used by JudgeLM.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used for generation. Defaults to <code>None</code>.</p> <code>'You are a helpful and precise assistant for checking the quality of the answer.'</code> <code>task_description</code> <code>Union[str, None]</code> <p>the description of the task. Defaults to <code>None</code>.</p> <code>'We would like to request your feedback on the performance of {num_responses} AI assistants in response to the user question displayed above.\\nPlease rate the helpfulness, relevance, accuracy, level of details of their responses. Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance.\\nPlease first output a single line containing only {num_responses} values indicating the scores for Assistants 1 to {num_responses}, respectively. The {num_responses} scores are separated by a space. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.'</code> Source code in <code>src/distilabel/tasks/preference/judgelm.py</code> <pre><code>@dataclass\nclass JudgeLMTask(PreferenceTask):\n    \"\"\"A `PreferenceTask` following the prompt templated used by JudgeLM.\n\n    Args:\n        system_prompt (str, optional): the system prompt to be used for generation. Defaults to `None`.\n        task_description (Union[str, None], optional): the description of the task. Defaults to `None`.\n    \"\"\"\n\n    __jinja2_template__: ClassVar[str] = _JUDGELM_TEMPLATE\n\n    task_description: str = (\n        \"We would like to request your feedback on the performance of {num_responses} AI assistants in response to the\"\n        \" user question displayed above.\\nPlease rate the helpfulness, relevance, accuracy, level of details\"\n        \" of their responses. Each assistant receives an overall score on a scale of 1 to 10, where a higher\"\n        \" score indicates better overall performance.\\nPlease first output a single line containing only {num_responses}\"\n        \" values indicating the scores for Assistants 1 to {num_responses}, respectively. The {num_responses} scores are separated by\"\n        \" a space. In the subsequent line, please provide a comprehensive explanation of your evaluation,\"\n        \" avoiding any potential bias and ensuring that the order in which the responses were presented does\"\n        \" not affect your judgment.\"\n    )\n    system_prompt: str = \"You are a helpful and precise assistant for checking the quality of the answer.\"\n\n    def generate_prompt(self, input: str, generations: List[str]) -&gt; Prompt:\n        \"\"\"Generates a prompt following the JudgeLM specification.\n\n        Args:\n            input (str): the input to be used for the prompt.\n            generations (List[str]): the generations to be used for the prompt.\n\n        Returns:\n            Prompt: the generated prompt.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.preference import JudgeLMTask\n            &gt;&gt;&gt; task = JudgeLMTask(system_prompt=\"You are a helpful assistant.\")\n            &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\n            Prompt(\n                system_prompt=\"You are a helpful assistant.\",\n                formatted_prompt=\"[Question]\\nWhat are the first 5 Fibonacci numbers?\\n...\",\n            )\n        \"\"\"\n        render_kwargs = {\n            \"input\": input,\n            \"responses\": generations,\n            \"task_description\": self.task_description.format(\n                num_responses=len(generations)\n            ),\n        }\n        return Prompt(\n            system_prompt=self.system_prompt,\n            formatted_prompt=self.template.render(**render_kwargs),\n        )\n\n    def parse_output(self, output: str) -&gt; JudgeLMOutput:\n        \"\"\"Parses the output of the model into the desired format.\"\"\"\n        split_output = output.split(\"\\n\")\n        rating = [float(rating) for rating in split_output[0].split(\" \")]\n        rationale = \"\\n\".join(split_output[1:])\n        return JudgeLMOutput(rating=rating, rationale=rationale)\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.JudgeLMTask.generate_prompt","title":"<code>generate_prompt(input, generations)</code>","text":"<p>Generates a prompt following the JudgeLM specification.</p> <pre><code>    Args:\n        input (str): the input to be used for the prompt.\n        generations (List[str]): the generations to be used for the prompt.\n\n    Returns:\n        Prompt: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.preference import JudgeLMTask\n        &gt;&gt;&gt; task = JudgeLMTask(system_prompt=\"You are a helpful assistant.\")\n        &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\n        Prompt(\n            system_prompt=\"You are a helpful assistant.\",\n            formatted_prompt=\"[Question]\n</code></pre> <p>What are the first 5 Fibonacci numbers? ...\",             )</p> Source code in <code>src/distilabel/tasks/preference/judgelm.py</code> <pre><code>def generate_prompt(self, input: str, generations: List[str]) -&gt; Prompt:\n    \"\"\"Generates a prompt following the JudgeLM specification.\n\n    Args:\n        input (str): the input to be used for the prompt.\n        generations (List[str]): the generations to be used for the prompt.\n\n    Returns:\n        Prompt: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.preference import JudgeLMTask\n        &gt;&gt;&gt; task = JudgeLMTask(system_prompt=\"You are a helpful assistant.\")\n        &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\n        Prompt(\n            system_prompt=\"You are a helpful assistant.\",\n            formatted_prompt=\"[Question]\\nWhat are the first 5 Fibonacci numbers?\\n...\",\n        )\n    \"\"\"\n    render_kwargs = {\n        \"input\": input,\n        \"responses\": generations,\n        \"task_description\": self.task_description.format(\n            num_responses=len(generations)\n        ),\n    }\n    return Prompt(\n        system_prompt=self.system_prompt,\n        formatted_prompt=self.template.render(**render_kwargs),\n    )\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.JudgeLMTask.parse_output","title":"<code>parse_output(output)</code>","text":"<p>Parses the output of the model into the desired format.</p> Source code in <code>src/distilabel/tasks/preference/judgelm.py</code> <pre><code>def parse_output(self, output: str) -&gt; JudgeLMOutput:\n    \"\"\"Parses the output of the model into the desired format.\"\"\"\n    split_output = output.split(\"\\n\")\n    rating = [float(rating) for rating in split_output[0].split(\" \")]\n    rationale = \"\\n\".join(split_output[1:])\n    return JudgeLMOutput(rating=rating, rationale=rationale)\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.Llama2TextGenerationTask","title":"<code>Llama2TextGenerationTask</code>","text":"<p>             Bases: <code>TextGenerationTask</code></p> <p>A <code>TextGenerationTask</code> for the Llama2 model.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used. Defaults to <code>None</code>.</p> required <code>principles</code> <code>Dict[str, List[str]]</code> <p>the principles to be used for the system prompt. Defaults to <code>None</code>.</p> required <code>principles_distribution</code> <code>Union[Dict[str, float], Literal[balanced], None]</code> <p>the distribution of principles to be used for the system prompt. Defaults to <code>None</code>.</p> required Source code in <code>src/distilabel/tasks/text_generation/llama.py</code> <pre><code>class Llama2TextGenerationTask(TextGenerationTask):\n    \"\"\"A `TextGenerationTask` for the Llama2 model.\n\n    Args:\n        system_prompt (str, optional): the system prompt to be used. Defaults to `None`.\n        principles (Dict[str, List[str]], optional): the principles to be used for the system prompt.\n            Defaults to `None`.\n        principles_distribution (Union[Dict[str, float], Literal[\"balanced\"], None], optional): the\n            distribution of principles to be used for the system prompt. Defaults to `None`.\n    \"\"\"\n\n    def generate_prompt(self, input: str) -&gt; str:\n        \"\"\"Generates a prompt for the Llama2 model.\n\n        Args:\n            input (str): the input to be used for the prompt.\n\n        Returns:\n            str: the generated prompt.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.text_generation import Llama2TextGenerationTask\n            &gt;&gt;&gt; task = Llama2TextGenerationTask(system_prompt=\"You are a helpful assistant.\")\n            &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\")\n            '&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are a helpful assistant.&lt;&lt;/SYS&gt;&gt;\\n\\nWhat are the first 5 Fibonacci numbers? [/INST]'\n        \"\"\"\n        return Prompt(\n            system_prompt=self.system_prompt,\n            formatted_prompt=input,\n        ).format_as(\"llama2\")  # type: ignore\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.Llama2TextGenerationTask.generate_prompt","title":"<code>generate_prompt(input)</code>","text":"<p>Generates a prompt for the Llama2 model.</p> <pre><code>    Args:\n        input (str): the input to be used for the prompt.\n\n    Returns:\n        str: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.text_generation import Llama2TextGenerationTask\n        &gt;&gt;&gt; task = Llama2TextGenerationTask(system_prompt=\"You are a helpful assistant.\")\n        &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\")\n        '&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\n</code></pre> <p>You are a helpful assistant.&lt;&gt;</p> <p>What are the first 5 Fibonacci numbers? [/INST]'</p> Source code in <code>src/distilabel/tasks/text_generation/llama.py</code> <pre><code>def generate_prompt(self, input: str) -&gt; str:\n    \"\"\"Generates a prompt for the Llama2 model.\n\n    Args:\n        input (str): the input to be used for the prompt.\n\n    Returns:\n        str: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.text_generation import Llama2TextGenerationTask\n        &gt;&gt;&gt; task = Llama2TextGenerationTask(system_prompt=\"You are a helpful assistant.\")\n        &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\")\n        '&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are a helpful assistant.&lt;&lt;/SYS&gt;&gt;\\n\\nWhat are the first 5 Fibonacci numbers? [/INST]'\n    \"\"\"\n    return Prompt(\n        system_prompt=self.system_prompt,\n        formatted_prompt=input,\n    ).format_as(\"llama2\")  # type: ignore\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.OpenAITextGenerationTask","title":"<code>OpenAITextGenerationTask</code>","text":"<p>             Bases: <code>TextGenerationTask</code></p> <p>A <code>TextGenerationTask</code> for any chat-completion OpenAI model.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used. Defaults to <code>None</code>.</p> required <code>principles</code> <code>Dict[str, List[str]]</code> <p>the principles to be used for the system prompt. Defaults to <code>None</code>.</p> required <code>principles_distribution</code> <code>Union[Dict[str, float], Literal[balanced], None]</code> <p>the distribution of principles to be used for the system prompt. Defaults to <code>None</code>.</p> required Source code in <code>src/distilabel/tasks/text_generation/openai.py</code> <pre><code>class OpenAITextGenerationTask(TextGenerationTask):\n    \"\"\"A `TextGenerationTask` for any chat-completion OpenAI model.\n\n    Args:\n        system_prompt (str, optional): the system prompt to be used. Defaults to `None`.\n        principles (Dict[str, List[str]], optional): the principles to be used for the system prompt.\n            Defaults to `None`.\n        principles_distribution (Union[Dict[str, float], Literal[\"balanced\"], None], optional): the\n            distribution of principles to be used for the system prompt. Defaults to `None`.\n    \"\"\"\n\n    def generate_prompt(self, input: str) -&gt; List[\"ChatCompletion\"]:\n        \"\"\"Generates a prompt for any chat-completion OpenAI model.\n\n        Args:\n            input (str): the input to be used for the prompt.\n\n        Returns:\n            List[ChatCompletion]: the generated prompt.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.text_generation import OpenAITextGenerationTask\n            &gt;&gt;&gt; task = OpenAITextGenerationTask(system_prompt=\"You are a helpful assistant.\")\n            &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\")\n            [\n                {'role': 'system', 'content': 'You are a helpful assistant.'},\n                {'role': 'user', 'content': 'What are the first 5 Fibonacci numbers?'},\n            ]\n        \"\"\"\n        return Prompt(\n            system_prompt=self.system_prompt,\n            formatted_prompt=input,\n        ).format_as(\"openai\")  # type: ignore\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.OpenAITextGenerationTask.generate_prompt","title":"<code>generate_prompt(input)</code>","text":"<p>Generates a prompt for any chat-completion OpenAI model.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>the input to be used for the prompt.</p> required <p>Returns:</p> Type Description <code>List[ChatCompletion]</code> <p>List[ChatCompletion]: the generated prompt.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.text_generation import OpenAITextGenerationTask\n&gt;&gt;&gt; task = OpenAITextGenerationTask(system_prompt=\"You are a helpful assistant.\")\n&gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\")\n[\n    {'role': 'system', 'content': 'You are a helpful assistant.'},\n    {'role': 'user', 'content': 'What are the first 5 Fibonacci numbers?'},\n]\n</code></pre> Source code in <code>src/distilabel/tasks/text_generation/openai.py</code> <pre><code>def generate_prompt(self, input: str) -&gt; List[\"ChatCompletion\"]:\n    \"\"\"Generates a prompt for any chat-completion OpenAI model.\n\n    Args:\n        input (str): the input to be used for the prompt.\n\n    Returns:\n        List[ChatCompletion]: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.text_generation import OpenAITextGenerationTask\n        &gt;&gt;&gt; task = OpenAITextGenerationTask(system_prompt=\"You are a helpful assistant.\")\n        &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\")\n        [\n            {'role': 'system', 'content': 'You are a helpful assistant.'},\n            {'role': 'user', 'content': 'What are the first 5 Fibonacci numbers?'},\n        ]\n    \"\"\"\n    return Prompt(\n        system_prompt=self.system_prompt,\n        formatted_prompt=input,\n    ).format_as(\"openai\")  # type: ignore\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.Prompt","title":"<code>Prompt</code>  <code>dataclass</code>","text":"<p>A <code>dataclass</code> representing a <code>Prompt</code>.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt.</p> required <code>formatted_prompt</code> <code>str</code> <p>the formatted prompt.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.prompt import Prompt\n&gt;&gt;&gt; prompt = Prompt(\n...     system_prompt=\"You are a helpful assistant.\",\n...     formatted_prompt=\"What are the first 5 Fibonacci numbers?\",\n... )\n</code></pre> Source code in <code>src/distilabel/tasks/prompt.py</code> <pre><code>@dataclass\nclass Prompt:\n    \"\"\"A `dataclass` representing a `Prompt`.\n\n    Args:\n        system_prompt (str): the system prompt.\n        formatted_prompt (str): the formatted prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.prompt import Prompt\n        &gt;&gt;&gt; prompt = Prompt(\n        ...     system_prompt=\"You are a helpful assistant.\",\n        ...     formatted_prompt=\"What are the first 5 Fibonacci numbers?\",\n        ... )\n    \"\"\"\n\n    system_prompt: str\n    formatted_prompt: str\n\n    def format_as(self, format: SupportedFormats) -&gt; Union[str, List[ChatCompletion]]:\n        \"\"\"Formats the prompt as the specified format.\n\n        Args:\n            format (SupportedFormats): the format to be used for the prompt. Available formats are\n                `default`, `openai`, `llama2`, `chatml`, and `zephyr`.\n\n        Returns:\n            Union[str, List[ChatCompletion]]: the formatted prompt.\n\n        Raises:\n            ValueError: if the specified format is not supported.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.prompt import Prompt\n            &gt;&gt;&gt; prompt = Prompt(\n            ...     system_prompt=\"You are a helpful assistant.\",\n            ...     formatted_prompt=\"What are the first 5 Fibonacci numbers?\",\n            ... )\n            &gt;&gt;&gt; prompt.format_as(\"default\")\n            'You are a helpful assistant.\\nWhat are the first 5 Fibonacci numbers?'\n        \"\"\"\n        if format == \"default\":\n            return f\"{self.system_prompt}\\n{self.formatted_prompt}\"\n        elif format == \"openai\":\n            return [\n                ChatCompletion(\n                    role=\"system\",\n                    content=self.system_prompt,\n                ),\n                ChatCompletion(role=\"user\", content=self.formatted_prompt),\n            ]\n        elif format == \"llama2\":\n            return f\"&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\n{self.system_prompt}&lt;&lt;/SYS&gt;&gt;\\n\\n{self.formatted_prompt} [/INST]\"\n        elif format == \"chatml\":\n            return f\"&lt;|im_start|&gt;system\\n{self.system_prompt}&lt;|im_end|&gt;\\n&lt;|im_start|&gt;user\\n{self.formatted_prompt}&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\n\"\n        elif format == \"zephyr\":\n            return f\"&lt;|system|&gt;\\n{self.system_prompt}&lt;/s&gt;\\n&lt;|user|&gt;\\n{self.formatted_prompt}&lt;/s&gt;\\n&lt;|assistant|&gt;\\n\"\n        else:\n            raise ValueError(\n                f\"Format {format} not supported, please provide a custom `prompt_formatting_fn`\"\n                \" or use any of the available formats: openai, llama2, chatml, zephyr\"\n            )\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.Prompt.format_as","title":"<code>format_as(format)</code>","text":"<p>Formats the prompt as the specified format.</p> <pre><code>    Args:\n        format (SupportedFormats): the format to be used for the prompt. Available formats are\n            `default`, `openai`, `llama2`, `chatml`, and `zephyr`.\n\n    Returns:\n        Union[str, List[ChatCompletion]]: the formatted prompt.\n\n    Raises:\n        ValueError: if the specified format is not supported.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.prompt import Prompt\n        &gt;&gt;&gt; prompt = Prompt(\n        ...     system_prompt=\"You are a helpful assistant.\",\n        ...     formatted_prompt=\"What are the first 5 Fibonacci numbers?\",\n        ... )\n        &gt;&gt;&gt; prompt.format_as(\"default\")\n        'You are a helpful assistant.\n</code></pre> <p>What are the first 5 Fibonacci numbers?'</p> Source code in <code>src/distilabel/tasks/prompt.py</code> <pre><code>def format_as(self, format: SupportedFormats) -&gt; Union[str, List[ChatCompletion]]:\n    \"\"\"Formats the prompt as the specified format.\n\n    Args:\n        format (SupportedFormats): the format to be used for the prompt. Available formats are\n            `default`, `openai`, `llama2`, `chatml`, and `zephyr`.\n\n    Returns:\n        Union[str, List[ChatCompletion]]: the formatted prompt.\n\n    Raises:\n        ValueError: if the specified format is not supported.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.prompt import Prompt\n        &gt;&gt;&gt; prompt = Prompt(\n        ...     system_prompt=\"You are a helpful assistant.\",\n        ...     formatted_prompt=\"What are the first 5 Fibonacci numbers?\",\n        ... )\n        &gt;&gt;&gt; prompt.format_as(\"default\")\n        'You are a helpful assistant.\\nWhat are the first 5 Fibonacci numbers?'\n    \"\"\"\n    if format == \"default\":\n        return f\"{self.system_prompt}\\n{self.formatted_prompt}\"\n    elif format == \"openai\":\n        return [\n            ChatCompletion(\n                role=\"system\",\n                content=self.system_prompt,\n            ),\n            ChatCompletion(role=\"user\", content=self.formatted_prompt),\n        ]\n    elif format == \"llama2\":\n        return f\"&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\n{self.system_prompt}&lt;&lt;/SYS&gt;&gt;\\n\\n{self.formatted_prompt} [/INST]\"\n    elif format == \"chatml\":\n        return f\"&lt;|im_start|&gt;system\\n{self.system_prompt}&lt;|im_end|&gt;\\n&lt;|im_start|&gt;user\\n{self.formatted_prompt}&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\n\"\n    elif format == \"zephyr\":\n        return f\"&lt;|system|&gt;\\n{self.system_prompt}&lt;/s&gt;\\n&lt;|user|&gt;\\n{self.formatted_prompt}&lt;/s&gt;\\n&lt;|assistant|&gt;\\n\"\n    else:\n        raise ValueError(\n            f\"Format {format} not supported, please provide a custom `prompt_formatting_fn`\"\n            \" or use any of the available formats: openai, llama2, chatml, zephyr\"\n        )\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.SelfInstructTask","title":"<code>SelfInstructTask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>TextGenerationTask</code></p> <p>A <code>TextGenerationTask</code> following the Self-Instruct specification for building the prompts.</p> <p>Reference: https://github.com/yizhongw/self-instruct</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used. Defaults to <code>None</code>.</p> <code>'You are an expert prompt writer, writing the best and most diverse prompts for a variety of tasks.You are given a task description and a set of instructions for how to write the prompts for a specific AI application.'</code> <code>principles</code> <code>Dict[str, List[str]]</code> <p>the principles to be used for the system prompt. Defaults to <code>None</code>.</p> <code>field(default_factory=lambda : {'harmlessness': harmlessness, 'helpfulness': helpfulness, 'truthfulness': truthfulness, 'honesty': honesty, 'verbalized_calibration': verbalized_calibration}, repr=False)</code> <code>principles_distribution</code> <code>Union[Dict[str, float], Literal[balanced], None]</code> <p>the distribution of principles to be used for the system prompt. Defaults to <code>None</code>.</p> <code>None</code> <code>application_description</code> <code>str</code> <p>the description of the AI application. Defaults to \"AI assistant\".</p> <code>'AI assistant'</code> <code>num_instructions</code> <code>int</code> <p>the number of instructions to be used for the prompt. Defaults to 5.</p> <code>5</code> Source code in <code>src/distilabel/tasks/text_generation/self_instruct.py</code> <pre><code>@dataclass\nclass SelfInstructTask(TextGenerationTask):\n    \"\"\"A `TextGenerationTask` following the Self-Instruct specification for building\n    the prompts.\n\n    Reference: https://github.com/yizhongw/self-instruct\n\n    Args:\n        system_prompt (str, optional): the system prompt to be used. Defaults to `None`.\n        principles (Dict[str, List[str]], optional): the principles to be used for the system prompt.\n            Defaults to `None`.\n        principles_distribution (Union[Dict[str, float], Literal[\"balanced\"], None], optional): the\n            distribution of principles to be used for the system prompt. Defaults to `None`.\n        application_description (str, optional): the description of the AI application. Defaults to\n            \"AI assistant\".\n        num_instructions (int, optional): the number of instructions to be used for the prompt.\n            Defaults to 5.\n    \"\"\"\n\n    system_prompt: str = (\n        \"You are an expert prompt writer, writing the best and most diverse prompts for a variety of tasks.\"\n        \"You are given a task description and a set of instructions for how to write the prompts for a specific AI application.\"\n    )\n    application_description: str = \"AI assistant\"\n    num_instructions: int = 5\n\n    __jinja2_template__: str = _SELF_INSTRUCT_TEMPLATE\n\n    def generate_prompt(self, input: str) -&gt; Prompt:\n        \"\"\"Generates a prompt following the Self-Instruct specification.\n\n        Args:\n            input (str): the input to be used for the prompt.\n\n        Returns:\n            Prompt: the generated prompt.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.text_generation import SelfInstructTask\n            &gt;&gt;&gt; task = SelfInstructTask(system_prompt=\"You are a helpful assistant.\", num_instructions=2)\n            &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\")\n            Prompt(\n                system_prompt=\"You are a helpful assistant.\",\n                formatted_prompt=\"# Task Description\\nDevelop 2 user queries that ...\",\n            )\n        \"\"\"\n        render_kwargs = {\n            \"application_description\": self.application_description,\n            \"num_instructions\": self.num_instructions,\n            \"input\": input,\n        }\n        return Prompt(\n            system_prompt=self.system_prompt,\n            formatted_prompt=self.template.render(**render_kwargs),\n        )\n\n    def parse_output(self, output: str) -&gt; Dict[str, List[str]]:\n        \"\"\"Parses the output of the model into the desired format.\"\"\"\n        return {\"generations\": output.split(\"\\n\")}\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.SelfInstructTask.generate_prompt","title":"<code>generate_prompt(input)</code>","text":"<p>Generates a prompt following the Self-Instruct specification.</p> <pre><code>    Args:\n        input (str): the input to be used for the prompt.\n\n    Returns:\n        Prompt: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.text_generation import SelfInstructTask\n        &gt;&gt;&gt; task = SelfInstructTask(system_prompt=\"You are a helpful assistant.\", num_instructions=2)\n        &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\")\n        Prompt(\n            system_prompt=\"You are a helpful assistant.\",\n            formatted_prompt=\"# Task Description\n</code></pre> <p>Develop 2 user queries that ...\",             )</p> Source code in <code>src/distilabel/tasks/text_generation/self_instruct.py</code> <pre><code>def generate_prompt(self, input: str) -&gt; Prompt:\n    \"\"\"Generates a prompt following the Self-Instruct specification.\n\n    Args:\n        input (str): the input to be used for the prompt.\n\n    Returns:\n        Prompt: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.text_generation import SelfInstructTask\n        &gt;&gt;&gt; task = SelfInstructTask(system_prompt=\"You are a helpful assistant.\", num_instructions=2)\n        &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\")\n        Prompt(\n            system_prompt=\"You are a helpful assistant.\",\n            formatted_prompt=\"# Task Description\\nDevelop 2 user queries that ...\",\n        )\n    \"\"\"\n    render_kwargs = {\n        \"application_description\": self.application_description,\n        \"num_instructions\": self.num_instructions,\n        \"input\": input,\n    }\n    return Prompt(\n        system_prompt=self.system_prompt,\n        formatted_prompt=self.template.render(**render_kwargs),\n    )\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.SelfInstructTask.parse_output","title":"<code>parse_output(output)</code>","text":"<p>Parses the output of the model into the desired format.</p> Source code in <code>src/distilabel/tasks/text_generation/self_instruct.py</code> <pre><code>def parse_output(self, output: str) -&gt; Dict[str, List[str]]:\n    \"\"\"Parses the output of the model into the desired format.\"\"\"\n    return {\"generations\": output.split(\"\\n\")}\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.Task","title":"<code>Task</code>","text":"<p>             Bases: <code>ABC</code>, <code>Argilla</code></p> <p>Abstract class used to define the methods required to create a <code>Task</code>, to be used within an <code>LLM</code>.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used for generation.</p> required <code>task_description</code> <code>Union[str, None]</code> <p>the description of the task. Defaults to <code>None</code>.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if the <code>__jinja2_template__</code> attribute is not provided.</p> Source code in <code>src/distilabel/tasks/base.py</code> <pre><code>class Task(ABC, Argilla):\n    \"\"\"Abstract class used to define the methods required to create a `Task`, to be used\n    within an `LLM`.\n\n    Args:\n        system_prompt (str): the system prompt to be used for generation.\n        task_description (Union[str, None], optional): the description of the task. Defaults to `None`.\n\n    Raises:\n        ValueError: if the `__jinja2_template__` attribute is not provided.\n    \"\"\"\n\n    system_prompt: str\n    task_description: Union[str, None] = None\n\n    __jinja2_template__: Union[str, None] = None\n\n    def __rich_repr__(self) -&gt; Generator[Any, None, None]:\n        yield \"system_prompt\", self.system_prompt\n        yield \"task_description\", self.task_description\n        yield \"input_args_names\", self.input_args_names\n        yield \"output_args_names\", self.output_args_names\n\n    @property\n    def template(self) -&gt; \"Template\":\n        if self.__jinja2_template__ is None:\n            raise ValueError(\n                \"You must provide a `__jinja2_template__` attribute to your Task subclass.\"\n            )\n\n        return Template(open(self.__jinja2_template__).read())\n\n    @abstractmethod\n    def generate_prompt(self, **kwargs: Any) -&gt; Union[Prompt, Any]:\n        pass\n\n    @abstractmethod\n    def parse_output(self, output: str) -&gt; Any:\n        pass\n\n    @property\n    @abstractmethod\n    def input_args_names(self) -&gt; List[str]:\n        pass\n\n    @property\n    @abstractmethod\n    def output_args_names(self) -&gt; List[str]:\n        pass\n\n    def validate_dataset(self, columns_in_dataset: List[str]) -&gt; None:\n        \"\"\"Validates that the dataset contains the required columns for the task.\n\n        Args:\n            columns_in_dataset (List[str]): the columns in the dataset.\n\n        Raises:\n            KeyError: if the dataset does not contain the required columns.\n        \"\"\"\n        for input_arg_name in self.input_args_names:\n            if input_arg_name not in columns_in_dataset:\n                raise KeyError(\n                    f\"LLM expects a column named '{input_arg_name}' in the provided\"\n                    \" dataset, but it was not found.\"\n                )\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.Task.validate_dataset","title":"<code>validate_dataset(columns_in_dataset)</code>","text":"<p>Validates that the dataset contains the required columns for the task.</p> <p>Parameters:</p> Name Type Description Default <code>columns_in_dataset</code> <code>List[str]</code> <p>the columns in the dataset.</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>if the dataset does not contain the required columns.</p> Source code in <code>src/distilabel/tasks/base.py</code> <pre><code>def validate_dataset(self, columns_in_dataset: List[str]) -&gt; None:\n    \"\"\"Validates that the dataset contains the required columns for the task.\n\n    Args:\n        columns_in_dataset (List[str]): the columns in the dataset.\n\n    Raises:\n        KeyError: if the dataset does not contain the required columns.\n    \"\"\"\n    for input_arg_name in self.input_args_names:\n        if input_arg_name not in columns_in_dataset:\n            raise KeyError(\n                f\"LLM expects a column named '{input_arg_name}' in the provided\"\n                \" dataset, but it was not found.\"\n            )\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.TextGenerationTask","title":"<code>TextGenerationTask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Task</code></p> <p>A base <code>Task</code> definition for text generation using LLMs.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used. Defaults to <code>None</code>.</p> <code>\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"</code> <code>principles</code> <code>Dict[str, List[str]]</code> <p>the principles to be used for the system prompt. Defaults to <code>None</code>.</p> <code>field(default_factory=lambda : {'harmlessness': harmlessness, 'helpfulness': helpfulness, 'truthfulness': truthfulness, 'honesty': honesty, 'verbalized_calibration': verbalized_calibration}, repr=False)</code> <code>principles_distribution</code> <code>Union[Dict[str, float], Literal['balanced'], None]</code> <p>the distribution of principles to be used for the system prompt. Defaults to <code>None</code>.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask\n&gt;&gt;&gt; task = TextGenerationTask()\n</code></pre> Source code in <code>src/distilabel/tasks/text_generation/base.py</code> <pre><code>@dataclass\nclass TextGenerationTask(Task):\n    \"\"\"A base `Task` definition for text generation using LLMs.\n\n    Args:\n        system_prompt (str, optional): the system prompt to be used. Defaults to `None`.\n        principles (Dict[str, List[str]], optional): the principles to be used for the system prompt.\n            Defaults to `None`.\n        principles_distribution (Union[Dict[str, float], Literal[\"balanced\"], None], optional): the\n            distribution of principles to be used for the system prompt. Defaults to `None`.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask\n        &gt;&gt;&gt; task = TextGenerationTask()\n    \"\"\"\n\n    system_prompt: str = (\n        \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible,\"\n        \" while being safe. Your answers should not include any harmful, unethical, racist, sexist,\"\n        \" toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased\"\n        \" and positive in nature.\\nIf a question does not make any sense, or is not factually coherent,\"\n        \" explain why instead of answering something not correct. If you don't know the answer to a\"\n        \" question, please don't share false information.\"\n    )\n    principles: Dict[str, List[str]] = field(\n        default_factory=lambda: {\n            \"harmlessness\": UltraFeedbackPrinciples.harmlessness,\n            \"helpfulness\": UltraFeedbackPrinciples.helpfulness,\n            \"truthfulness\": UltraFeedbackPrinciples.truthfulness,\n            \"honesty\": UltraFeedbackPrinciples.honesty,\n            \"verbalized_calibration\": UltraFeedbackPrinciples.verbalized_calibration,\n        },\n        repr=False,\n    )\n    principles_distribution: Union[Dict[str, float], Literal[\"balanced\"], None] = None\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Validates the `principles_distribution` if it is a dict.\n\n        Raises:\n            ValueError: if the `principles_distribution` is a dict and it does not sum to 1.0.\n            ValueError: if the `principles` are not included in the `principles_distribution`.\n        \"\"\"\n        if isinstance(self.principles_distribution, dict):\n            not_included_principles = [\n                principle\n                for principle in self.principles\n                if principle not in self.principles_distribution\n            ]\n            if not_included_principles:\n                principles_str = \", \".join(\n                    [f\"'{principle}'\" for principle in not_included_principles]\n                )\n                raise ValueError(\n                    f\"Principles {principles_str} included in `principles` is not in\"\n                    \" `principles_distribution`\"\n                )\n\n            if sum(self.principles_distribution.values()) != 1.0:\n                raise ValueError(\n                    \"`principles_distribution` must sum to 1.0 if it is a dict containing\"\n                    \" the distribution of principles to use.\"\n                )\n\n    def _get_principle(self) -&gt; str:\n        \"\"\"Gets a principle from the `principles` dict respecting the `principal_distribution`.\n\n        Returns:\n            str: the principle to be used.\n        \"\"\"\n        if isinstance(self.principles_distribution, dict):\n            principle_group = random.choices(\n                list(self.principles_distribution.keys()),\n                weights=list(self.principles_distribution.values()),\n                k=1,\n            )[0]\n        else:\n            principle_group = random.choice(list(self.principles.keys()))\n        return random.choice(self.principles[principle_group])\n\n    def generate_prompt(self, input: str) -&gt; Prompt:\n        \"\"\"Generates the prompt to be used for generation.\n\n        Args:\n            input (str): the input to be used for generation.\n\n        Returns:\n            Prompt: the generated prompt.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask\n            &gt;&gt;&gt; task = TextGenerationTask(system_prompt=\"You are a helpful assistant.\")\n            &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\")\n            Prompt(system_prompt='You are a helpful assistant.', formatted_prompt='What are the first 5 Fibonacci numbers?')\n        \"\"\"\n        system_prompt = self.system_prompt\n        if self.principles_distribution is not None:\n            principle = self._get_principle()\n            system_prompt += \" \" + principle\n        return Prompt(system_prompt=system_prompt, formatted_prompt=input)\n\n    def parse_output(self, output: str) -&gt; dict[str, str]:\n        \"\"\"Parses the output of the LLM into the desired format.\"\"\"\n        return {\"generations\": output}\n\n    @property\n    def input_args_names(self) -&gt; list[str]:\n        \"\"\"Returns the input args names for the task.\"\"\"\n        return [\"input\"]\n\n    @property\n    def output_args_names(self) -&gt; list[str]:\n        \"\"\"Returns the output args names for the task.\"\"\"\n        return [\"generations\"]\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.TextGenerationTask.input_args_names","title":"<code>input_args_names: list[str]</code>  <code>property</code>","text":"<p>Returns the input args names for the task.</p>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.TextGenerationTask.output_args_names","title":"<code>output_args_names: list[str]</code>  <code>property</code>","text":"<p>Returns the output args names for the task.</p>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.TextGenerationTask.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validates the <code>principles_distribution</code> if it is a dict.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the <code>principles_distribution</code> is a dict and it does not sum to 1.0.</p> <code>ValueError</code> <p>if the <code>principles</code> are not included in the <code>principles_distribution</code>.</p> Source code in <code>src/distilabel/tasks/text_generation/base.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Validates the `principles_distribution` if it is a dict.\n\n    Raises:\n        ValueError: if the `principles_distribution` is a dict and it does not sum to 1.0.\n        ValueError: if the `principles` are not included in the `principles_distribution`.\n    \"\"\"\n    if isinstance(self.principles_distribution, dict):\n        not_included_principles = [\n            principle\n            for principle in self.principles\n            if principle not in self.principles_distribution\n        ]\n        if not_included_principles:\n            principles_str = \", \".join(\n                [f\"'{principle}'\" for principle in not_included_principles]\n            )\n            raise ValueError(\n                f\"Principles {principles_str} included in `principles` is not in\"\n                \" `principles_distribution`\"\n            )\n\n        if sum(self.principles_distribution.values()) != 1.0:\n            raise ValueError(\n                \"`principles_distribution` must sum to 1.0 if it is a dict containing\"\n                \" the distribution of principles to use.\"\n            )\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.TextGenerationTask.generate_prompt","title":"<code>generate_prompt(input)</code>","text":"<p>Generates the prompt to be used for generation.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>the input to be used for generation.</p> required <p>Returns:</p> Name Type Description <code>Prompt</code> <code>Prompt</code> <p>the generated prompt.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask\n&gt;&gt;&gt; task = TextGenerationTask(system_prompt=\"You are a helpful assistant.\")\n&gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\")\nPrompt(system_prompt='You are a helpful assistant.', formatted_prompt='What are the first 5 Fibonacci numbers?')\n</code></pre> Source code in <code>src/distilabel/tasks/text_generation/base.py</code> <pre><code>def generate_prompt(self, input: str) -&gt; Prompt:\n    \"\"\"Generates the prompt to be used for generation.\n\n    Args:\n        input (str): the input to be used for generation.\n\n    Returns:\n        Prompt: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask\n        &gt;&gt;&gt; task = TextGenerationTask(system_prompt=\"You are a helpful assistant.\")\n        &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\")\n        Prompt(system_prompt='You are a helpful assistant.', formatted_prompt='What are the first 5 Fibonacci numbers?')\n    \"\"\"\n    system_prompt = self.system_prompt\n    if self.principles_distribution is not None:\n        principle = self._get_principle()\n        system_prompt += \" \" + principle\n    return Prompt(system_prompt=system_prompt, formatted_prompt=input)\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.TextGenerationTask.parse_output","title":"<code>parse_output(output)</code>","text":"<p>Parses the output of the LLM into the desired format.</p> Source code in <code>src/distilabel/tasks/text_generation/base.py</code> <pre><code>def parse_output(self, output: str) -&gt; dict[str, str]:\n    \"\"\"Parses the output of the LLM into the desired format.\"\"\"\n    return {\"generations\": output}\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.UltraFeedbackTask","title":"<code>UltraFeedbackTask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>PreferenceTask</code></p> <p>A <code>PreferenceTask</code> following the prompt template used by ULTRAFEEDBACK.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used for generation. Defaults to <code>None</code>.</p> <code>'Your role is to evaluate text quality based on given criteria.'</code> <code>task_description</code> <code>Union[str, None]</code> <p>the description of the task. Defaults to <code>None</code>.</p> required <code>ratings</code> <code>Union[List[Rating], None]</code> <p>the ratings to be used for the task. Defaults to <code>None</code>.</p> required Source code in <code>src/distilabel/tasks/preference/ultrafeedback.py</code> <pre><code>@dataclass\nclass UltraFeedbackTask(PreferenceTask):\n    \"\"\"A `PreferenceTask` following the prompt template used by ULTRAFEEDBACK.\n\n    Args:\n        system_prompt (str, optional): the system prompt to be used for generation. Defaults to `None`.\n        task_description (Union[str, None], optional): the description of the task. Defaults to `None`.\n        ratings (Union[List[Rating], None], optional): the ratings to be used for the task. Defaults to `None`.\n    \"\"\"\n\n    ratings: List[Rating]\n    task_description: str\n\n    __jinja2_template__: ClassVar[str] = field(\n        default=_ULTRAFEEDBACK_TEMPLATE, init=False, repr=False\n    )\n    __subtasks__: ClassVar[List[str]] = [\n        \"text-quality\",\n        \"helpfulness\",\n        \"truthfulness\",\n        \"honesty\",\n        \"instruction-following\",\n    ]\n\n    system_prompt: (\n        str\n    ) = \"Your role is to evaluate text quality based on given criteria.\"\n\n    def generate_prompt(self, input: str, generations: List[str]) -&gt; Prompt:\n        \"\"\"Generates a prompt following the ULTRAFEEDBACK specification.\n\n        Args:\n            input (str): the input to be used for the prompt.\n            generations (List[str]): the generations to be used for the prompt.\n\n        Returns:\n            Prompt: the generated prompt.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.preference import UltraFeedbackTask\n            &gt;&gt;&gt; task = UltraFeedbackTask.for_text_quality()\n            &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\n            Prompt(\n                system_prompt=\"Your role is to evaluate text quality based on given criteria.\",\n                formatted_prompt=\"# General Text Quality Assessment\\nEvaluate the model's ...\",\n            )\n        \"\"\"\n        render_kwargs = {\n            \"task_description\": self.task_description,\n            \"ratings\": self.ratings,\n            \"input\": input,\n            \"responses\": generations,\n        }\n        return Prompt(\n            system_prompt=self.system_prompt,\n            formatted_prompt=self.template.render(**render_kwargs),\n        )\n\n    def parse_output(self, output: str) -&gt; List[UltraFeedbackOutput]:\n        \"\"\"Parses the output of the model into the desired format.\"\"\"\n        parsed_output = []\n        for section in output.split(\"#### Output for Text \")[1:]:\n            rating, rationale = section.split(\"\\n\")[1:3]\n            rating = float(rating.split(\": \")[1])\n            rationale = rationale.split(\": \")[1]\n            parsed_output.append(\n                UltraFeedbackOutput(rating=rating, rationale=rationale)\n            )\n        return parsed_output\n\n    def _to_argilla_rationale(\n        self,\n        dataset_row: Dict[str, Any],\n    ) -&gt; str:\n        \"\"\"Converts the rationale to the format expected by Argilla.\"\"\"\n        rationales = dataset_row.get(\"rationale\")\n        if rationales is None:\n            return \"\"\n        sections = []\n        for idx, rationale in enumerate(dataset_row[\"rationale\"], start=1):\n            sections.append(f\"Rationale for generation-{idx}:\\n{rationale}\\n\")\n        return \"\\n\".join(sections)\n\n    @classmethod\n    def for_text_quality(\n        cls,\n        system_prompt: Optional[str] = None,\n        task_description: Optional[str] = None,\n        ratings: Optional[List[Rating]] = None,\n    ) -&gt; \"UltraFeedbackTask\":\n        kwargs = {}\n        if system_prompt is not None:\n            kwargs.update({\"system_prompt\": system_prompt})\n        if task_description is None:\n            task_description = dedent(\n                \"\"\"\n                # General Text Quality Assessment\n                Evaluate the model's outputs based on various criteria:\n                1. **Correctness &amp; Informativeness**: Does the output provide accurate and helpful information?\n                2. **Honesty &amp; Uncertainty**: How confidently does the model convey its information, and does it express uncertainty appropriately?\n                3. **Truthfulness &amp; Hallucination**: Does the model introduce misleading or fabricated details?\n                4. **Instruction Following**: Does the model's output align with given instructions and the user's intent?\n                Your role is to provide a holistic assessment considering all the above factors.\n\n                **Scoring**: Rate outputs 1 to 5 based on the overall quality, considering all aspects:\n                \"\"\"\n            )\n        kwargs.update({\"task_description\": task_description})\n\n        if ratings is None:\n            ratings = [\n                Rating(\n                    value=1,\n                    description=\"**Low Quality**: Contains inaccuracies, may be entirely wrong or has severe hallucinations.\",\n                ),\n                Rating(\n                    value=2,\n                    description=\"**Moderate Quality**: Addresses some aspects, but has errors or is partially aligned with instructions.\",\n                ),\n                Rating(\n                    value=3,\n                    description=\"**Good**: Generally accurate but may contain minor errors or slight deviations.\",\n                ),\n                Rating(\n                    value=4,\n                    description=\"**Very Good**: Near perfect, with minor issues in terms of alignment or confidence.\",\n                ),\n                Rating(\n                    value=5,\n                    description=\"**Excellent**: Accurate, confident, aligned with instructions, and free of hallucinations.\",\n                ),\n            ]\n        kwargs.update({\"ratings\": ratings})\n        return cls(**kwargs)\n\n    @classmethod\n    def for_helpfulness(\n        cls,\n        system_prompt: Optional[str] = None,\n        task_description: Optional[str] = None,\n        ratings: Optional[List[Rating]] = None,\n    ) -&gt; \"UltraFeedbackTask\":\n        kwargs = {}\n        if system_prompt is not None:\n            kwargs.update({\"system_prompt\": system_prompt})\n\n        if task_description is None:\n            task_description = dedent(\n                \"\"\"\n                # Informativeness / Helpfulness Assessment\n                Evaluate if model's outputs fulfill task objectives and provide high-quality, correct, and, informative content.\n                Helpfulness assessment emphasizes **Overall Quality** regarding correctness and informativeness.\n                **Correctness**: Accurate computation, reasoning steps, and outputs without misunderstandings or fabrication.\n\n                **Scoring**: Score 1 to 5 based on extent of helpfulness, regarding both informativeness and correctness:\n                \"\"\"\n            )\n        kwargs.update({\"task_description\": task_description})\n        if ratings is None:\n            ratings = [\n                Rating(\n                    value=1,\n                    description=\"**Severely Incorrect**: Contains significant inaccuracies or fabricated content, even if comprehensive information is provided.\",\n                ),\n                Rating(\n                    value=2,\n                    description=\"**Partially Incorrect**: Contains errors that may cause confusion, even though comprehensive information is present.\",\n                ),\n                Rating(\n                    value=3,\n                    description=\"**Correct**: Accurate and provides useful information that meets the task's requirements.\",\n                ),\n                Rating(\n                    value=4,\n                    description=\"**Highly Informative**: Accurate and extensive, providing valuable insights and detailed information.\",\n                ),\n                Rating(\n                    value=5,\n                    description=\"**Outstandingly Helpful**: Both accurate and in-depth, offering profound insights and comprehensive information.\",\n                ),\n            ]\n        kwargs.update({\"ratings\": ratings})\n        return cls(**kwargs)\n\n    @classmethod\n    def for_truthfulness(\n        cls,\n        system_prompt: Optional[str] = None,\n        task_description: Optional[str] = None,\n        ratings: Optional[List[Rating]] = None,\n    ) -&gt; \"UltraFeedbackTask\":\n        kwargs = {}\n        if system_prompt is not None:\n            kwargs.update({\"system_prompt\": system_prompt})\n\n        if task_description is None:\n            task_description = dedent(\n                \"\"\"\n                # Truthfulness and Hallucination Assessment\n                Evaluate the model's accuracy in providing information without introducing misleading or fabricated details.\n                Assign numeric identifier (or \"None\") from 1 to 3 for each type of hallucination:\n                1. **Contradictory with the World (Factual Error)**: Entities, locations, concepts, or events that conflict with established knowledge.\n                2. **Contradictory with Instruction and Input**: Responses diverge, introducing new facts not aligned with instructions or inputs.\n                3. **Self-Contradictory / Logical Error**: Responses contain internal contradictions or logical errors within each independent text.\n\n                **Scoring**: Rate outputs 1 to 5 based on the extent of hallucination:\n                \"\"\"\n            )\n        kwargs.update({\"task_description\": task_description})\n\n        if ratings is None:\n            ratings = [\n                Rating(\n                    value=1,\n                    description=\"**Completely Hallucinated**: Entirely unreliable due to hallucinations.\",\n                ),\n                Rating(\n                    value=2,\n                    description=\"**Severe Hallucination**: Nearly half contains hallucinations, severe deviation from main points.\",\n                ),\n                Rating(\n                    value=3,\n                    description=\"**Partial Hallucination / Misunderstanding**: Overall truthful, partial misunderstanding due to hallucinations.\",\n                ),\n                Rating(\n                    value=4,\n                    description=\"**Insignificant Hallucination**: Mostly truthful, slight hallucination not affecting main points.\",\n                ),\n                Rating(\n                    value=5, description=\"**No Hallucination**: Free of hallucinations.\"\n                ),\n            ]\n        kwargs.update({\"ratings\": ratings})\n        return cls(**kwargs)\n\n    @classmethod\n    def for_honesty(\n        cls,\n        system_prompt: Optional[str] = None,\n        task_description: Optional[str] = None,\n        ratings: Optional[List[Rating]] = None,\n    ) -&gt; \"UltraFeedbackTask\":\n        kwargs = {}\n        if system_prompt is not None:\n            kwargs.update({\"system_prompt\": system_prompt})\n        if task_description is None:\n            task_description = dedent(\n                \"\"\"\n                # Honesty and Uncertainty Expression Assessment\n                Assess how well the model conveys honesty and uncertainty. Evaluate if the model's confidence aligns with response correctness.\n                - **Indicators of Uncertainty**:\n                    1. Weakeners: e.g., 'I guess,' 'probably.'\n                    2. Refusal to answer.\n                    3. Verbalized confidence scores: [0, 20] low; (20, 40] uncertain; (40, 60] moderate; (60, 80] leaning confident; (80, 100] high.\n                - **No uncertainty expression indicate confidence.**\n                - **Response Correctness**: Align with ground truth, or provide accurate content without fabrication.\n\n                **Scoring**: Rate outputs 1 to 5 based on the extent of honesty and uncertainty:\n                \"\"\"\n            )\n        kwargs.update({\"task_description\": task_description})\n        if ratings is None:\n            ratings = [\n                Rating(\n                    value=1,\n                    description=\"**Confidently Incorrect**: Confident but entirely wrong.\",\n                ),\n                Rating(\n                    value=2,\n                    description=\"**Confident with Significant Mistakes / Unconfident Incorrect**: Confident but contains major errors. Unconfident and entirely wrong.\",\n                ),\n                Rating(\n                    value=3,\n                    description=\"**Uncertain / 'I Don't Know' / Subtle Mistakes**: 'I don't know' or declines. Confident but contains minor errors. Unconfident and contains significant mistakes.\",\n                ),\n                Rating(\n                    value=4,\n                    description=\"**Correct but Uncertain / Expressed Subtle Mistakes**: Correct but unconfident.\",\n                ),\n                Rating(\n                    value=5,\n                    description=\"**Correct and Confident / Precisely Express Uncertainty**: Correct and confident. Makes mistakes, but precisely acknowledges minor errors and indicates uncertainty on potential mistakes.\",\n                ),\n            ]\n        kwargs.update({\"ratings\": ratings})\n\n        return cls(**kwargs)\n\n    @classmethod\n    def for_instruction_following(\n        cls,\n        system_prompt: Optional[str] = None,\n        task_description: Optional[str] = None,\n        ratings: Optional[List[Rating]] = None,\n    ) -&gt; \"UltraFeedbackTask\":\n        kwargs = {}\n        if system_prompt is not None:\n            kwargs.update({\"system_prompt\": system_prompt})\n        if task_description is None:\n            task_description = dedent(\n                \"\"\"\n                # Instruction Following Assessment\n                Evaluate alignment between output and intent. Assess understanding of task goal and restrictions.\n                **Instruction Components**: Task Goal (intended outcome), Restrictions (text styles, formats, or designated methods, etc).\n\n                **Scoring**: Rate outputs 1 to 5:\n                \"\"\"\n            )\n        kwargs.update({\"task_description\": task_description})\n        if ratings is None:\n            ratings = [\n                Rating(value=1, description=\"**Irrelevant**: No alignment.\"),\n                Rating(\n                    value=2,\n                    description=\"**Partial Focus**: Addresses one aspect poorly.\",\n                ),\n                Rating(\n                    value=3,\n                    description=\"**Partial Compliance**:\\n\\t- (1) Meets goal or restrictions, neglecting other.\\n\\t- (2) Acknowledges both but slight deviations.\",\n                ),\n                Rating(\n                    value=4,\n                    description=\"**Almost There**: Near alignment, minor deviations.\",\n                ),\n                Rating(\n                    value=5,\n                    description=\"**Comprehensive Compliance**: Fully aligns, meets all requirements.\",\n                ),\n            ]\n        kwargs.update({\"ratings\": ratings})\n\n        return cls(**kwargs)\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.UltraFeedbackTask.generate_prompt","title":"<code>generate_prompt(input, generations)</code>","text":"<p>Generates a prompt following the ULTRAFEEDBACK specification.</p> <pre><code>    Args:\n        input (str): the input to be used for the prompt.\n        generations (List[str]): the generations to be used for the prompt.\n\n    Returns:\n        Prompt: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.preference import UltraFeedbackTask\n        &gt;&gt;&gt; task = UltraFeedbackTask.for_text_quality()\n        &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\n        Prompt(\n            system_prompt=\"Your role is to evaluate text quality based on given criteria.\",\n            formatted_prompt=\"# General Text Quality Assessment\n</code></pre> <p>Evaluate the model's ...\",             )</p> Source code in <code>src/distilabel/tasks/preference/ultrafeedback.py</code> <pre><code>def generate_prompt(self, input: str, generations: List[str]) -&gt; Prompt:\n    \"\"\"Generates a prompt following the ULTRAFEEDBACK specification.\n\n    Args:\n        input (str): the input to be used for the prompt.\n        generations (List[str]): the generations to be used for the prompt.\n\n    Returns:\n        Prompt: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.preference import UltraFeedbackTask\n        &gt;&gt;&gt; task = UltraFeedbackTask.for_text_quality()\n        &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\n        Prompt(\n            system_prompt=\"Your role is to evaluate text quality based on given criteria.\",\n            formatted_prompt=\"# General Text Quality Assessment\\nEvaluate the model's ...\",\n        )\n    \"\"\"\n    render_kwargs = {\n        \"task_description\": self.task_description,\n        \"ratings\": self.ratings,\n        \"input\": input,\n        \"responses\": generations,\n    }\n    return Prompt(\n        system_prompt=self.system_prompt,\n        formatted_prompt=self.template.render(**render_kwargs),\n    )\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.UltraFeedbackTask.parse_output","title":"<code>parse_output(output)</code>","text":"<p>Parses the output of the model into the desired format.</p> Source code in <code>src/distilabel/tasks/preference/ultrafeedback.py</code> <pre><code>def parse_output(self, output: str) -&gt; List[UltraFeedbackOutput]:\n    \"\"\"Parses the output of the model into the desired format.\"\"\"\n    parsed_output = []\n    for section in output.split(\"#### Output for Text \")[1:]:\n        rating, rationale = section.split(\"\\n\")[1:3]\n        rating = float(rating.split(\": \")[1])\n        rationale = rationale.split(\": \")[1]\n        parsed_output.append(\n            UltraFeedbackOutput(rating=rating, rationale=rationale)\n        )\n    return parsed_output\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.UltraJudgeTask","title":"<code>UltraJudgeTask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>PreferenceTask</code></p> <p>A <code>PreferenceTask</code> for the UltraJudge task. The <code>UltraJudge</code> task has been defined at Argilla specically for a better evaluation using AI Feedback. The task is defined based on both UltraFeedback and JudgeLM, but with several improvements / modifications.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used for generation. Defaults to <code>None</code>.</p> <code>\"You are an evaluator tasked with assessing AI assistants' responses from the perspective of typical user preferences. Your critical analysis should focus on human-like engagement, solution effectiveness, accuracy, clarity, and creativity. Approach each response as if you were the user, considering how well the response meets your needs and expectations in a real-world scenario. Provide detailed feedback that highlights strengths and areas for improvement in each response, keeping in mind the goal of simulating a human's preferred choice. Your evaluation should be impartial and thorough, reflecting a human's perspective in preferring responses that are practical, clear, authentic, and aligned with their intent. Avoid bias, and focus on the content and quality of the responses.\"</code> <code>task_description</code> <code>Union[str, None]</code> <p>the description of the task. Defaults to <code>None</code>.</p> <code>\"Your task is to rigorously evaluate the performance of {num_responses} AI assistants, simulating a human's perspective. You will assess each response based on four key domains, reflecting aspects that are typically valued by humans: {areas}. First provide a score between 0 and 10 and write a detailed feedback for each area and assistant. Finally, provide a list of {num_responses} scores, each separated by a space, to reflect the performance of Assistants 1 to {num_responses}.\"</code> <code>areas</code> <code>List[str]</code> <p>the areas to be used for the task. Defaults to a list of four areas: \"Practical Accuracy\", \"Clarity &amp; Transparency\", \"Authenticity &amp; Reliability\", and \"Compliance with Intent\".</p> <code>field(default_factory=lambda : ['Practical Accuracy', 'Clarity &amp; Transparency', 'Authenticity &amp; Reliability', 'Compliance with Intent'])</code> Source code in <code>src/distilabel/tasks/preference/ultrajudge.py</code> <pre><code>@dataclass\nclass UltraJudgeTask(PreferenceTask):\n    \"\"\"A `PreferenceTask` for the UltraJudge task. The `UltraJudge` task has been defined\n    at Argilla specically for a better evaluation using AI Feedback. The task is defined\n    based on both UltraFeedback and JudgeLM, but with several improvements / modifications.\n\n    Args:\n        system_prompt (str, optional): the system prompt to be used for generation. Defaults to `None`.\n        task_description (Union[str, None], optional): the description of the task. Defaults to `None`.\n        areas (List[str], optional): the areas to be used for the task. Defaults to a list of four areas:\n            \"Practical Accuracy\", \"Clarity &amp; Transparency\", \"Authenticity &amp; Reliability\", and \"Compliance with Intent\".\n    \"\"\"\n\n    system_prompt: str = (\n        \"You are an evaluator tasked with assessing AI assistants' responses from the perspective of typical user preferences.\"\n        \" Your critical analysis should focus on human-like engagement, solution effectiveness, accuracy, clarity, and\"\n        \" creativity. Approach each response as if you were the user, considering how well the response meets your needs\"\n        \" and expectations in a real-world scenario. Provide detailed feedback that highlights strengths and areas for\"\n        \" improvement in each response, keeping in mind the goal of simulating a human's preferred choice. \"\n        \"Your evaluation should be impartial and thorough, reflecting a human's perspective in preferring responses that are practical,\"\n        \" clear, authentic, and aligned with their intent. Avoid bias, and focus on the content and quality of the responses.\"\n    )\n\n    task_description: str = (\n        \"Your task is to rigorously evaluate the performance of {num_responses} AI assistants, simulating a human's perspective.\"\n        \" You will assess each response based on four key domains, reflecting aspects that are typically valued by humans:\"\n        \" {areas}.\"\n        \" First provide a score between 0 and 10 and write a detailed feedback for each area and assistant.\"\n        \" Finally, provide a list of {num_responses} scores, each separated by a space, to reflect the performance of Assistants 1 to {num_responses}.\"\n    )\n\n    areas: List[str] = field(\n        default_factory=lambda: [\n            \"Practical Accuracy\",\n            \"Clarity &amp; Transparency\",\n            \"Authenticity &amp; Reliability\",\n            \"Compliance with Intent\",\n        ]\n    )\n\n    __jinja2_template__: ClassVar[str] = field(\n        default=_ULTRAJUDGE_TEMPLATE, init=False, repr=False\n    )\n\n    @property\n    def output_args_names(self) -&gt; List[str]:\n        \"\"\"Returns the names of the output arguments of the task.\"\"\"\n        return [\"rating\", \"areas\"]\n\n    @property\n    def areas_str(self) -&gt; str:\n        \"\"\"Returns a string representation of the areas.\"\"\"\n        return \", \".join(self.areas[:-1]) + \", and \" + self.areas[-1]\n\n    @property\n    def extract_area_score_and_rationale_regex(self) -&gt; str:\n        \"\"\"Returns a regex to extract the area, score, and rationale from the output.\"\"\"\n        return rf\"({'|'.join(self.areas)})\\s*-\\s*(\\d+(?:\\.\\d+)?)\\n(.*?)(?=\\n\\n|\\Z)\"\n\n    @property\n    def extract_final_scores_regex(self) -&gt; str:\n        \"\"\"Returns a regex to extract the final scores from the output.\"\"\"\n        return r\"Final scores:\\s*((?:\\d+(?:\\.\\d+)?\\s*)+)\"\n\n    def generate_prompt(self, input: str, generations: List[str]) -&gt; Prompt:\n        \"\"\"Generates a prompt following the UltraJudge specification.\n\n        Args:\n            input (str): the input to be used for the prompt.\n            generations (List[str]): the generations to be used for the prompt.\n\n        Returns:\n            Prompt: the generated prompt.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.preference import UltraJudgeTask\n            &gt;&gt;&gt; task = UltraJudgeTask(system_prompt=\"You are a helpful assistant.\")\n            &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\n            Prompt(\n                system_prompt=\"You are a helpful assistant.\",\n                formatted_prompt=\"Your task is to rigorously evaluate the performance of ...\",\n            )\n        \"\"\"\n        render_kwargs = {\n            \"task_description\": self.task_description.format(\n                num_responses=len(generations), areas=self.areas_str\n            ),\n            \"instruction\": input,\n            \"responses\": generations,\n        }\n\n        return Prompt(\n            system_prompt=self.system_prompt,\n            formatted_prompt=self.template.render(**render_kwargs),\n        )\n\n    def parse_output(self, output: str) -&gt; List[UltraJudgeOutput]:\n        \"\"\"Parses the output of the model into the desired format.\"\"\"\n        num_areas = len(self.areas)\n        # `areas_results` includes num_generations * num_areas tuples\n        areas_results = re.findall(self.extract_area_score_and_rationale_regex, output)\n        final_scores = [\n            float(str_score)\n            for str_score in re.findall(self.extract_final_scores_regex, output)[\n                0\n            ].split(\" \")\n        ]\n\n        outputs = []\n        for i, rating in enumerate(final_scores):\n            areas = {}\n            # Get the areas for the i-th generation\n            for area in areas_results[i * num_areas : i * num_areas + num_areas]:\n                name, area_rating, rationale = area\n                areas[name] = Area(rating=area_rating, rationale=rationale)\n            outputs.append(UltraJudgeOutput(rating=rating, areas=areas))\n\n        return outputs\n\n    def _to_argilla_rationale(\n        self,\n        dataset_row: Dict[str, Any],\n    ) -&gt; str:\n        \"\"\"Gets the `rationale` column from a `datasets.Dataset` row and formats it\n        as expected by Argilla.\n        \"\"\"\n\n        def format_area(area):\n            sections = []\n            for title, ratings in area.items():\n                sections.append(title)\n                for k, v in ratings.items():\n                    sections.append(f\"{k}:{v}\")\n            return \"\\n\".join(sections)\n\n        rationales = []\n        for idx, area in enumerate(dataset_row[\"areas\"], start=1):\n            formatted_area = format_area(area)\n            rationales.append(f\"Rationale for generation-{idx}:\\n{formatted_area}\\n\")\n        return \"\\n\".join(rationales)\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.UltraJudgeTask.areas_str","title":"<code>areas_str: str</code>  <code>property</code>","text":"<p>Returns a string representation of the areas.</p>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.UltraJudgeTask.extract_area_score_and_rationale_regex","title":"<code>extract_area_score_and_rationale_regex: str</code>  <code>property</code>","text":"<p>Returns a regex to extract the area, score, and rationale from the output.</p>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.UltraJudgeTask.extract_final_scores_regex","title":"<code>extract_final_scores_regex: str</code>  <code>property</code>","text":"<p>Returns a regex to extract the final scores from the output.</p>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.UltraJudgeTask.output_args_names","title":"<code>output_args_names: List[str]</code>  <code>property</code>","text":"<p>Returns the names of the output arguments of the task.</p>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.UltraJudgeTask.generate_prompt","title":"<code>generate_prompt(input, generations)</code>","text":"<p>Generates a prompt following the UltraJudge specification.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>the input to be used for the prompt.</p> required <code>generations</code> <code>List[str]</code> <p>the generations to be used for the prompt.</p> required <p>Returns:</p> Name Type Description <code>Prompt</code> <code>Prompt</code> <p>the generated prompt.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.preference import UltraJudgeTask\n&gt;&gt;&gt; task = UltraJudgeTask(system_prompt=\"You are a helpful assistant.\")\n&gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\nPrompt(\n    system_prompt=\"You are a helpful assistant.\",\n    formatted_prompt=\"Your task is to rigorously evaluate the performance of ...\",\n)\n</code></pre> Source code in <code>src/distilabel/tasks/preference/ultrajudge.py</code> <pre><code>def generate_prompt(self, input: str, generations: List[str]) -&gt; Prompt:\n    \"\"\"Generates a prompt following the UltraJudge specification.\n\n    Args:\n        input (str): the input to be used for the prompt.\n        generations (List[str]): the generations to be used for the prompt.\n\n    Returns:\n        Prompt: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.preference import UltraJudgeTask\n        &gt;&gt;&gt; task = UltraJudgeTask(system_prompt=\"You are a helpful assistant.\")\n        &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\n        Prompt(\n            system_prompt=\"You are a helpful assistant.\",\n            formatted_prompt=\"Your task is to rigorously evaluate the performance of ...\",\n        )\n    \"\"\"\n    render_kwargs = {\n        \"task_description\": self.task_description.format(\n            num_responses=len(generations), areas=self.areas_str\n        ),\n        \"instruction\": input,\n        \"responses\": generations,\n    }\n\n    return Prompt(\n        system_prompt=self.system_prompt,\n        formatted_prompt=self.template.render(**render_kwargs),\n    )\n</code></pre>"},{"location":"reference/distilabel/tasks/#distilabel.tasks.UltraJudgeTask.parse_output","title":"<code>parse_output(output)</code>","text":"<p>Parses the output of the model into the desired format.</p> Source code in <code>src/distilabel/tasks/preference/ultrajudge.py</code> <pre><code>def parse_output(self, output: str) -&gt; List[UltraJudgeOutput]:\n    \"\"\"Parses the output of the model into the desired format.\"\"\"\n    num_areas = len(self.areas)\n    # `areas_results` includes num_generations * num_areas tuples\n    areas_results = re.findall(self.extract_area_score_and_rationale_regex, output)\n    final_scores = [\n        float(str_score)\n        for str_score in re.findall(self.extract_final_scores_regex, output)[\n            0\n        ].split(\" \")\n    ]\n\n    outputs = []\n    for i, rating in enumerate(final_scores):\n        areas = {}\n        # Get the areas for the i-th generation\n        for area in areas_results[i * num_areas : i * num_areas + num_areas]:\n            name, area_rating, rationale = area\n            areas[name] = Area(rating=area_rating, rationale=rationale)\n        outputs.append(UltraJudgeOutput(rating=rating, areas=areas))\n\n    return outputs\n</code></pre>"},{"location":"reference/distilabel/tasks/base/","title":"base","text":""},{"location":"reference/distilabel/tasks/base/#distilabel.tasks.base.Argilla","title":"<code>Argilla</code>","text":"<p>Class to be used internally to define the methods required to export a dataset as an Argilla <code>FeedbackDataset</code>.</p> Source code in <code>src/distilabel/tasks/base.py</code> <pre><code>class Argilla:\n    \"\"\"Class to be used internally to define the methods required to export a dataset\n    as an Argilla `FeedbackDataset`.\n    \"\"\"\n\n    def to_argilla_fields(\n        self,\n        dataset_row: Dict[str, Any],\n        *args: Any,\n        **kwargs: Any,\n    ) -&gt; List[\"AllowedFieldTypes\"]:\n        raise NotImplementedError(\n            \"`to_argilla_fields` is not implemented, if you want to export your dataset as an Argilla dataset you will need to implement this method.\"\n        )\n\n    def to_argilla_questions(\n        self,\n        dataset_row: Dict[str, Any],\n        *args: Any,\n        **kwargs: Any,\n    ) -&gt; List[\"AllowedQuestionTypes\"]:\n        raise NotImplementedError(\n            \"`to_argilla_questions` is not implemented, if you want to export your dataset as an Argilla dataset you will need to implement this method.\"\n        )\n\n    def to_argilla_record(\n        self, dataset_row: Dict[str, Any], *args: Any, **kwargs: Any\n    ) -&gt; \"FeedbackRecord\":\n        raise NotImplementedError(\n            \"`to_argilla_record` is not implemented, if you want to export your dataset as an Argilla dataset you will need to implement this method.\"\n        )\n\n    def _create_argilla_record(\n        self,\n        fields: Dict[str, Any],\n        suggestions: List[Dict[str, Any]],\n        metadata: Dict[str, Any],\n    ) -&gt; \"FeedbackRecord\":\n        return rg.FeedbackRecord(\n            fields=fields, suggestions=suggestions, metadata=metadata\n        )\n\n    def _create_text_field(self, name: str) -&gt; \"TextField\":\n        return rg.TextField(name=name)\n\n    def _create_rating_question(\n        self, name: str, title: str, values: List[int]\n    ) -&gt; \"RatingQuestion\":\n        return rg.RatingQuestion(name=name, title=title, values=values)\n\n    def _create_text_question(self, name: str, title: str) -&gt; \"TextQuestion\":\n        return rg.TextQuestion(name=name, title=title)\n\n    def _create_metadata_property(\n        self, name: str, property_type: str\n    ) -&gt; Union[\"IntegerMetadataProperty\", \"FloatMetadataProperty\"]:\n        if property_type == \"integer\":\n            return rg.IntegerMetadataProperty(name=name)\n        elif property_type == \"float\":\n            return rg.FloatMetadataProperty(name=name)\n        else:\n            raise ValueError(f\"Invalid property type: {property_type}\")\n\n    def _create_fields_from_row(\n        self, dataset_row: Dict[str, Any], process_function: Callable\n    ) -&gt; List[\"AllowedFieldTypes\"]:\n        processed_items = []\n        for arg_name in self.input_args_names:\n            self._check_argument_exists(dataset_row, arg_name)\n            if isinstance(dataset_row[arg_name], list):\n                for idx in range(1, len(dataset_row[arg_name]) + 1):\n                    processed_items.append(process_function(f\"{arg_name}-{idx}\"))\n            elif isinstance(dataset_row[arg_name], str):\n                processed_items.append(process_function(arg_name))\n        return processed_items\n\n    def _check_argument_exists(self, dataset_row, arg_name):\n        if arg_name not in dataset_row:\n            raise ValueError(\n                f\"Dataset row does not contain the required field '{arg_name}'.\"\n            )\n</code></pre>"},{"location":"reference/distilabel/tasks/base/#distilabel.tasks.base.Task","title":"<code>Task</code>","text":"<p>             Bases: <code>ABC</code>, <code>Argilla</code></p> <p>Abstract class used to define the methods required to create a <code>Task</code>, to be used within an <code>LLM</code>.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used for generation.</p> required <code>task_description</code> <code>Union[str, None]</code> <p>the description of the task. Defaults to <code>None</code>.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>if the <code>__jinja2_template__</code> attribute is not provided.</p> Source code in <code>src/distilabel/tasks/base.py</code> <pre><code>class Task(ABC, Argilla):\n    \"\"\"Abstract class used to define the methods required to create a `Task`, to be used\n    within an `LLM`.\n\n    Args:\n        system_prompt (str): the system prompt to be used for generation.\n        task_description (Union[str, None], optional): the description of the task. Defaults to `None`.\n\n    Raises:\n        ValueError: if the `__jinja2_template__` attribute is not provided.\n    \"\"\"\n\n    system_prompt: str\n    task_description: Union[str, None] = None\n\n    __jinja2_template__: Union[str, None] = None\n\n    def __rich_repr__(self) -&gt; Generator[Any, None, None]:\n        yield \"system_prompt\", self.system_prompt\n        yield \"task_description\", self.task_description\n        yield \"input_args_names\", self.input_args_names\n        yield \"output_args_names\", self.output_args_names\n\n    @property\n    def template(self) -&gt; \"Template\":\n        if self.__jinja2_template__ is None:\n            raise ValueError(\n                \"You must provide a `__jinja2_template__` attribute to your Task subclass.\"\n            )\n\n        return Template(open(self.__jinja2_template__).read())\n\n    @abstractmethod\n    def generate_prompt(self, **kwargs: Any) -&gt; Union[Prompt, Any]:\n        pass\n\n    @abstractmethod\n    def parse_output(self, output: str) -&gt; Any:\n        pass\n\n    @property\n    @abstractmethod\n    def input_args_names(self) -&gt; List[str]:\n        pass\n\n    @property\n    @abstractmethod\n    def output_args_names(self) -&gt; List[str]:\n        pass\n\n    def validate_dataset(self, columns_in_dataset: List[str]) -&gt; None:\n        \"\"\"Validates that the dataset contains the required columns for the task.\n\n        Args:\n            columns_in_dataset (List[str]): the columns in the dataset.\n\n        Raises:\n            KeyError: if the dataset does not contain the required columns.\n        \"\"\"\n        for input_arg_name in self.input_args_names:\n            if input_arg_name not in columns_in_dataset:\n                raise KeyError(\n                    f\"LLM expects a column named '{input_arg_name}' in the provided\"\n                    \" dataset, but it was not found.\"\n                )\n</code></pre>"},{"location":"reference/distilabel/tasks/base/#distilabel.tasks.base.Task.validate_dataset","title":"<code>validate_dataset(columns_in_dataset)</code>","text":"<p>Validates that the dataset contains the required columns for the task.</p> <p>Parameters:</p> Name Type Description Default <code>columns_in_dataset</code> <code>List[str]</code> <p>the columns in the dataset.</p> required <p>Raises:</p> Type Description <code>KeyError</code> <p>if the dataset does not contain the required columns.</p> Source code in <code>src/distilabel/tasks/base.py</code> <pre><code>def validate_dataset(self, columns_in_dataset: List[str]) -&gt; None:\n    \"\"\"Validates that the dataset contains the required columns for the task.\n\n    Args:\n        columns_in_dataset (List[str]): the columns in the dataset.\n\n    Raises:\n        KeyError: if the dataset does not contain the required columns.\n    \"\"\"\n    for input_arg_name in self.input_args_names:\n        if input_arg_name not in columns_in_dataset:\n            raise KeyError(\n                f\"LLM expects a column named '{input_arg_name}' in the provided\"\n                \" dataset, but it was not found.\"\n            )\n</code></pre>"},{"location":"reference/distilabel/tasks/prompt/","title":"prompt","text":""},{"location":"reference/distilabel/tasks/prompt/#distilabel.tasks.prompt.ChatCompletion","title":"<code>ChatCompletion</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>A <code>TypedDict</code> matching OpenAI's chat completion format.</p> Source code in <code>src/distilabel/tasks/prompt.py</code> <pre><code>class ChatCompletion(TypedDict):\n    \"\"\"A `TypedDict` matching OpenAI's chat completion format.\"\"\"\n\n    role: Literal[\"system\", \"user\", \"assistant\"]\n    content: str\n</code></pre>"},{"location":"reference/distilabel/tasks/prompt/#distilabel.tasks.prompt.Prompt","title":"<code>Prompt</code>  <code>dataclass</code>","text":"<p>A <code>dataclass</code> representing a <code>Prompt</code>.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt.</p> required <code>formatted_prompt</code> <code>str</code> <p>the formatted prompt.</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.prompt import Prompt\n&gt;&gt;&gt; prompt = Prompt(\n...     system_prompt=\"You are a helpful assistant.\",\n...     formatted_prompt=\"What are the first 5 Fibonacci numbers?\",\n... )\n</code></pre> Source code in <code>src/distilabel/tasks/prompt.py</code> <pre><code>@dataclass\nclass Prompt:\n    \"\"\"A `dataclass` representing a `Prompt`.\n\n    Args:\n        system_prompt (str): the system prompt.\n        formatted_prompt (str): the formatted prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.prompt import Prompt\n        &gt;&gt;&gt; prompt = Prompt(\n        ...     system_prompt=\"You are a helpful assistant.\",\n        ...     formatted_prompt=\"What are the first 5 Fibonacci numbers?\",\n        ... )\n    \"\"\"\n\n    system_prompt: str\n    formatted_prompt: str\n\n    def format_as(self, format: SupportedFormats) -&gt; Union[str, List[ChatCompletion]]:\n        \"\"\"Formats the prompt as the specified format.\n\n        Args:\n            format (SupportedFormats): the format to be used for the prompt. Available formats are\n                `default`, `openai`, `llama2`, `chatml`, and `zephyr`.\n\n        Returns:\n            Union[str, List[ChatCompletion]]: the formatted prompt.\n\n        Raises:\n            ValueError: if the specified format is not supported.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.prompt import Prompt\n            &gt;&gt;&gt; prompt = Prompt(\n            ...     system_prompt=\"You are a helpful assistant.\",\n            ...     formatted_prompt=\"What are the first 5 Fibonacci numbers?\",\n            ... )\n            &gt;&gt;&gt; prompt.format_as(\"default\")\n            'You are a helpful assistant.\\nWhat are the first 5 Fibonacci numbers?'\n        \"\"\"\n        if format == \"default\":\n            return f\"{self.system_prompt}\\n{self.formatted_prompt}\"\n        elif format == \"openai\":\n            return [\n                ChatCompletion(\n                    role=\"system\",\n                    content=self.system_prompt,\n                ),\n                ChatCompletion(role=\"user\", content=self.formatted_prompt),\n            ]\n        elif format == \"llama2\":\n            return f\"&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\n{self.system_prompt}&lt;&lt;/SYS&gt;&gt;\\n\\n{self.formatted_prompt} [/INST]\"\n        elif format == \"chatml\":\n            return f\"&lt;|im_start|&gt;system\\n{self.system_prompt}&lt;|im_end|&gt;\\n&lt;|im_start|&gt;user\\n{self.formatted_prompt}&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\n\"\n        elif format == \"zephyr\":\n            return f\"&lt;|system|&gt;\\n{self.system_prompt}&lt;/s&gt;\\n&lt;|user|&gt;\\n{self.formatted_prompt}&lt;/s&gt;\\n&lt;|assistant|&gt;\\n\"\n        else:\n            raise ValueError(\n                f\"Format {format} not supported, please provide a custom `prompt_formatting_fn`\"\n                \" or use any of the available formats: openai, llama2, chatml, zephyr\"\n            )\n</code></pre>"},{"location":"reference/distilabel/tasks/prompt/#distilabel.tasks.prompt.Prompt.format_as","title":"<code>format_as(format)</code>","text":"<p>Formats the prompt as the specified format.</p> <pre><code>    Args:\n        format (SupportedFormats): the format to be used for the prompt. Available formats are\n            `default`, `openai`, `llama2`, `chatml`, and `zephyr`.\n\n    Returns:\n        Union[str, List[ChatCompletion]]: the formatted prompt.\n\n    Raises:\n        ValueError: if the specified format is not supported.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.prompt import Prompt\n        &gt;&gt;&gt; prompt = Prompt(\n        ...     system_prompt=\"You are a helpful assistant.\",\n        ...     formatted_prompt=\"What are the first 5 Fibonacci numbers?\",\n        ... )\n        &gt;&gt;&gt; prompt.format_as(\"default\")\n        'You are a helpful assistant.\n</code></pre> <p>What are the first 5 Fibonacci numbers?'</p> Source code in <code>src/distilabel/tasks/prompt.py</code> <pre><code>def format_as(self, format: SupportedFormats) -&gt; Union[str, List[ChatCompletion]]:\n    \"\"\"Formats the prompt as the specified format.\n\n    Args:\n        format (SupportedFormats): the format to be used for the prompt. Available formats are\n            `default`, `openai`, `llama2`, `chatml`, and `zephyr`.\n\n    Returns:\n        Union[str, List[ChatCompletion]]: the formatted prompt.\n\n    Raises:\n        ValueError: if the specified format is not supported.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.prompt import Prompt\n        &gt;&gt;&gt; prompt = Prompt(\n        ...     system_prompt=\"You are a helpful assistant.\",\n        ...     formatted_prompt=\"What are the first 5 Fibonacci numbers?\",\n        ... )\n        &gt;&gt;&gt; prompt.format_as(\"default\")\n        'You are a helpful assistant.\\nWhat are the first 5 Fibonacci numbers?'\n    \"\"\"\n    if format == \"default\":\n        return f\"{self.system_prompt}\\n{self.formatted_prompt}\"\n    elif format == \"openai\":\n        return [\n            ChatCompletion(\n                role=\"system\",\n                content=self.system_prompt,\n            ),\n            ChatCompletion(role=\"user\", content=self.formatted_prompt),\n        ]\n    elif format == \"llama2\":\n        return f\"&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\n{self.system_prompt}&lt;&lt;/SYS&gt;&gt;\\n\\n{self.formatted_prompt} [/INST]\"\n    elif format == \"chatml\":\n        return f\"&lt;|im_start|&gt;system\\n{self.system_prompt}&lt;|im_end|&gt;\\n&lt;|im_start|&gt;user\\n{self.formatted_prompt}&lt;|im_end|&gt;\\n&lt;|im_start|&gt;assistant\\n\"\n    elif format == \"zephyr\":\n        return f\"&lt;|system|&gt;\\n{self.system_prompt}&lt;/s&gt;\\n&lt;|user|&gt;\\n{self.formatted_prompt}&lt;/s&gt;\\n&lt;|assistant|&gt;\\n\"\n    else:\n        raise ValueError(\n            f\"Format {format} not supported, please provide a custom `prompt_formatting_fn`\"\n            \" or use any of the available formats: openai, llama2, chatml, zephyr\"\n        )\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/","title":"preference","text":""},{"location":"reference/distilabel/tasks/preference/base/","title":"base","text":""},{"location":"reference/distilabel/tasks/preference/base/#distilabel.tasks.preference.base.PreferenceTask","title":"<code>PreferenceTask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Task</code></p> <p>A <code>Task</code> for preference rating tasks.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used for generation.</p> required <code>task_description</code> <code>Union[str, None]</code> <p>the description of the task. Defaults to <code>None</code>.</p> <code>None</code> Source code in <code>src/distilabel/tasks/preference/base.py</code> <pre><code>@dataclass\nclass PreferenceTask(Task):\n    \"\"\"A `Task` for preference rating tasks.\n\n    Args:\n        system_prompt (str): the system prompt to be used for generation.\n        task_description (Union[str, None], optional): the description of the task. Defaults to `None`.\n    \"\"\"\n\n    @property\n    def input_args_names(self) -&gt; List[str]:\n        \"\"\"Returns the names of the input arguments of the task.\"\"\"\n        return [\"input\", \"generations\"]\n\n    @property\n    def output_args_names(self) -&gt; List[str]:\n        \"\"\"Returns the names of the output arguments of the task.\"\"\"\n        return [\"rating\", \"rationale\"]\n\n    def to_argilla_fields(\n        self, dataset_row: Dict[str, Any]\n    ) -&gt; List[\"AllowedFieldTypes\"]:\n        \"\"\"Converts a dataset row to a list of Argilla `AllowedFieldTypes`.\"\"\"\n        return self._create_fields_from_row(dataset_row, self._create_text_field)\n\n    def to_argilla_questions(\n        self, dataset_row: Dict[str, Any]\n    ) -&gt; List[\"AllowedQuestionTypes\"]:\n        \"\"\"Converts a dataset row to a list of Argilla `AllowedQuestionTypes`.\"\"\"\n        questions = []\n        arg_name = \"generations\"\n        self._check_argument_exists(dataset_row, arg_name)\n        if isinstance(dataset_row[arg_name], list):\n            for idx in range(1, len(dataset_row[arg_name]) + 1):\n                question_name = f\"{arg_name}-{idx}-rating\"\n                title = f\"What's the rating for {arg_name}-{idx}?\"\n                questions.append(\n                    self._create_rating_question(\n                        question_name, title, list(range(1, 11))\n                    )\n                )\n        questions.append(\n            self._create_text_question(\n                \"ratings-rationale\", \"What's the rationale behind the ratings?\"\n            )\n        )\n        return questions\n\n    def to_argilla_metadata_properties(\n        self, dataset_row: Dict[str, Any]\n    ) -&gt; List[\"AllowedMetadataPropertyTypes\"]:\n        \"\"\"Converts a dataset row to a list of Argilla `AllowedMetadataPropertyTypes`.\"\"\"\n        metadata_properties = []\n        for arg_name in self.input_args_names:\n            self._check_argument_exists(dataset_row, arg_name)\n            if isinstance(dataset_row[arg_name], list):\n                for idx in range(1, len(dataset_row[arg_name]) + 1):\n                    metadata_properties.append(\n                        self._create_metadata_property(\n                            f\"length-{arg_name}-{idx}\", \"integer\"\n                        )\n                    )\n                    metadata_properties.append(\n                        self._create_metadata_property(\n                            f\"rating-{arg_name}-{idx}\", \"float\"\n                        )\n                    )\n            elif isinstance(dataset_row[arg_name], str):\n                metadata_properties.append(\n                    self._create_metadata_property(f\"length-{arg_name}\", \"integer\")\n                )\n            else:\n                raise ValueError(\n                    f\"Type {type(dataset_row[arg_name])} is not supported.\"\n                )\n        # add distance between best rating and the second best\n        if isinstance(dataset_row[arg_name], list):\n            metadata_properties.append(\n                self._create_metadata_property(\"distance-best-rated\", \"float\")\n            )\n        return metadata_properties\n\n    def to_argilla_record(  # noqa: C901\n        self,\n        dataset_row: Dict[str, Any],\n    ) -&gt; \"FeedbackRecord\":\n        \"\"\"Converts a dataset row to an Argilla `FeedbackRecord`.\"\"\"\n        fields = {}\n        metadata = {}\n\n        for input_arg_name in self.input_args_names:\n            arg_value = dataset_row[input_arg_name]\n\n            if isinstance(arg_value, list):\n                for idx, value in enumerate(arg_value, start=1):\n                    fields[f\"{input_arg_name}-{idx}\"] = value.strip()\n                    metadata[f\"length-{input_arg_name}-{idx}\"] = len(value.strip())\n            else:\n                fields[input_arg_name] = arg_value.strip()\n                metadata[f\"length-{input_arg_name}\"] = len(arg_value.strip())\n\n        suggestions = []\n\n        # add rationale\n        if self._to_argilla_rationale(dataset_row) is not None:\n            suggestions.append(\n                {\n                    \"question_name\": \"ratings-rationale\",\n                    \"value\": self._to_argilla_rationale(dataset_row),\n                }\n            )\n        for output_arg_name in self.output_args_names:\n            if output_arg_name == \"rating\":\n                ratings = []\n                output_data = dataset_row.get(output_arg_name)\n                if output_data is not None:\n                    for idx, value in enumerate(output_data, start=1):\n                        ratings.append(value)\n                        if value &lt;=0:\n                            value = 1.0\n                        if value &lt;= 10:\n                            # add suggestions\n                            suggestions.append(\n                                {\n                                    \"question_name\": f\"generations-{idx}-rating\",\n                                    \"value\": int(value),\n                                }\n                            )\n                        # update rating metadata\n                        metadata.update({f\"rating-generations-{idx}\": value})\n                if len(ratings) &gt;= 2:\n                    sorted_ratings = sorted(ratings, reverse=True)\n                    # update rating distance from best to second\n                    metadata.update(\n                        {\"distance-best-rated\": sorted_ratings[0] - sorted_ratings[1]}\n                    )\n        return self._create_argilla_record(\n            fields=fields, suggestions=suggestions, metadata=metadata\n        )\n\n    def _to_argilla_rationale(self, dataset_row: Dict[str, Any]) -&gt; str:\n        \"\"\"Gets the `rationale` column from a `datasets.Dataset` row and formats it\n        as expected by Argilla.\n        \"\"\"\n        return dataset_row[\"rationale\"]\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/base/#distilabel.tasks.preference.base.PreferenceTask.input_args_names","title":"<code>input_args_names: List[str]</code>  <code>property</code>","text":"<p>Returns the names of the input arguments of the task.</p>"},{"location":"reference/distilabel/tasks/preference/base/#distilabel.tasks.preference.base.PreferenceTask.output_args_names","title":"<code>output_args_names: List[str]</code>  <code>property</code>","text":"<p>Returns the names of the output arguments of the task.</p>"},{"location":"reference/distilabel/tasks/preference/base/#distilabel.tasks.preference.base.PreferenceTask.to_argilla_fields","title":"<code>to_argilla_fields(dataset_row)</code>","text":"<p>Converts a dataset row to a list of Argilla <code>AllowedFieldTypes</code>.</p> Source code in <code>src/distilabel/tasks/preference/base.py</code> <pre><code>def to_argilla_fields(\n    self, dataset_row: Dict[str, Any]\n) -&gt; List[\"AllowedFieldTypes\"]:\n    \"\"\"Converts a dataset row to a list of Argilla `AllowedFieldTypes`.\"\"\"\n    return self._create_fields_from_row(dataset_row, self._create_text_field)\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/base/#distilabel.tasks.preference.base.PreferenceTask.to_argilla_metadata_properties","title":"<code>to_argilla_metadata_properties(dataset_row)</code>","text":"<p>Converts a dataset row to a list of Argilla <code>AllowedMetadataPropertyTypes</code>.</p> Source code in <code>src/distilabel/tasks/preference/base.py</code> <pre><code>def to_argilla_metadata_properties(\n    self, dataset_row: Dict[str, Any]\n) -&gt; List[\"AllowedMetadataPropertyTypes\"]:\n    \"\"\"Converts a dataset row to a list of Argilla `AllowedMetadataPropertyTypes`.\"\"\"\n    metadata_properties = []\n    for arg_name in self.input_args_names:\n        self._check_argument_exists(dataset_row, arg_name)\n        if isinstance(dataset_row[arg_name], list):\n            for idx in range(1, len(dataset_row[arg_name]) + 1):\n                metadata_properties.append(\n                    self._create_metadata_property(\n                        f\"length-{arg_name}-{idx}\", \"integer\"\n                    )\n                )\n                metadata_properties.append(\n                    self._create_metadata_property(\n                        f\"rating-{arg_name}-{idx}\", \"float\"\n                    )\n                )\n        elif isinstance(dataset_row[arg_name], str):\n            metadata_properties.append(\n                self._create_metadata_property(f\"length-{arg_name}\", \"integer\")\n            )\n        else:\n            raise ValueError(\n                f\"Type {type(dataset_row[arg_name])} is not supported.\"\n            )\n    # add distance between best rating and the second best\n    if isinstance(dataset_row[arg_name], list):\n        metadata_properties.append(\n            self._create_metadata_property(\"distance-best-rated\", \"float\")\n        )\n    return metadata_properties\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/base/#distilabel.tasks.preference.base.PreferenceTask.to_argilla_questions","title":"<code>to_argilla_questions(dataset_row)</code>","text":"<p>Converts a dataset row to a list of Argilla <code>AllowedQuestionTypes</code>.</p> Source code in <code>src/distilabel/tasks/preference/base.py</code> <pre><code>def to_argilla_questions(\n    self, dataset_row: Dict[str, Any]\n) -&gt; List[\"AllowedQuestionTypes\"]:\n    \"\"\"Converts a dataset row to a list of Argilla `AllowedQuestionTypes`.\"\"\"\n    questions = []\n    arg_name = \"generations\"\n    self._check_argument_exists(dataset_row, arg_name)\n    if isinstance(dataset_row[arg_name], list):\n        for idx in range(1, len(dataset_row[arg_name]) + 1):\n            question_name = f\"{arg_name}-{idx}-rating\"\n            title = f\"What's the rating for {arg_name}-{idx}?\"\n            questions.append(\n                self._create_rating_question(\n                    question_name, title, list(range(1, 11))\n                )\n            )\n    questions.append(\n        self._create_text_question(\n            \"ratings-rationale\", \"What's the rationale behind the ratings?\"\n        )\n    )\n    return questions\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/base/#distilabel.tasks.preference.base.PreferenceTask.to_argilla_record","title":"<code>to_argilla_record(dataset_row)</code>","text":"<p>Converts a dataset row to an Argilla <code>FeedbackRecord</code>.</p> Source code in <code>src/distilabel/tasks/preference/base.py</code> <pre><code>def to_argilla_record(  # noqa: C901\n    self,\n    dataset_row: Dict[str, Any],\n) -&gt; \"FeedbackRecord\":\n    \"\"\"Converts a dataset row to an Argilla `FeedbackRecord`.\"\"\"\n    fields = {}\n    metadata = {}\n\n    for input_arg_name in self.input_args_names:\n        arg_value = dataset_row[input_arg_name]\n\n        if isinstance(arg_value, list):\n            for idx, value in enumerate(arg_value, start=1):\n                fields[f\"{input_arg_name}-{idx}\"] = value.strip()\n                metadata[f\"length-{input_arg_name}-{idx}\"] = len(value.strip())\n        else:\n            fields[input_arg_name] = arg_value.strip()\n            metadata[f\"length-{input_arg_name}\"] = len(arg_value.strip())\n\n    suggestions = []\n\n    # add rationale\n    if self._to_argilla_rationale(dataset_row) is not None:\n        suggestions.append(\n            {\n                \"question_name\": \"ratings-rationale\",\n                \"value\": self._to_argilla_rationale(dataset_row),\n            }\n        )\n    for output_arg_name in self.output_args_names:\n        if output_arg_name == \"rating\":\n            ratings = []\n            output_data = dataset_row.get(output_arg_name)\n            if output_data is not None:\n                for idx, value in enumerate(output_data, start=1):\n                    ratings.append(value)\n                    if value &lt;=0:\n                        value = 1.0\n                    if value &lt;= 10:\n                        # add suggestions\n                        suggestions.append(\n                            {\n                                \"question_name\": f\"generations-{idx}-rating\",\n                                \"value\": int(value),\n                            }\n                        )\n                    # update rating metadata\n                    metadata.update({f\"rating-generations-{idx}\": value})\n            if len(ratings) &gt;= 2:\n                sorted_ratings = sorted(ratings, reverse=True)\n                # update rating distance from best to second\n                metadata.update(\n                    {\"distance-best-rated\": sorted_ratings[0] - sorted_ratings[1]}\n                )\n    return self._create_argilla_record(\n        fields=fields, suggestions=suggestions, metadata=metadata\n    )\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/judgelm/","title":"judgelm","text":""},{"location":"reference/distilabel/tasks/preference/judgelm/#distilabel.tasks.preference.judgelm.JudgeLMOutput","title":"<code>JudgeLMOutput</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>A <code>TypedDict</code> matching the output format of JudgeLM.</p> Source code in <code>src/distilabel/tasks/preference/judgelm.py</code> <pre><code>class JudgeLMOutput(TypedDict):\n    \"\"\"A `TypedDict` matching the output format of JudgeLM.\"\"\"\n\n    rating: List[float]\n    rationale: str\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/judgelm/#distilabel.tasks.preference.judgelm.JudgeLMTask","title":"<code>JudgeLMTask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>PreferenceTask</code></p> <p>A <code>PreferenceTask</code> following the prompt templated used by JudgeLM.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used for generation. Defaults to <code>None</code>.</p> <code>'You are a helpful and precise assistant for checking the quality of the answer.'</code> <code>task_description</code> <code>Union[str, None]</code> <p>the description of the task. Defaults to <code>None</code>.</p> <code>'We would like to request your feedback on the performance of {num_responses} AI assistants in response to the user question displayed above.\\nPlease rate the helpfulness, relevance, accuracy, level of details of their responses. Each assistant receives an overall score on a scale of 1 to 10, where a higher score indicates better overall performance.\\nPlease first output a single line containing only {num_responses} values indicating the scores for Assistants 1 to {num_responses}, respectively. The {num_responses} scores are separated by a space. In the subsequent line, please provide a comprehensive explanation of your evaluation, avoiding any potential bias and ensuring that the order in which the responses were presented does not affect your judgment.'</code> Source code in <code>src/distilabel/tasks/preference/judgelm.py</code> <pre><code>@dataclass\nclass JudgeLMTask(PreferenceTask):\n    \"\"\"A `PreferenceTask` following the prompt templated used by JudgeLM.\n\n    Args:\n        system_prompt (str, optional): the system prompt to be used for generation. Defaults to `None`.\n        task_description (Union[str, None], optional): the description of the task. Defaults to `None`.\n    \"\"\"\n\n    __jinja2_template__: ClassVar[str] = _JUDGELM_TEMPLATE\n\n    task_description: str = (\n        \"We would like to request your feedback on the performance of {num_responses} AI assistants in response to the\"\n        \" user question displayed above.\\nPlease rate the helpfulness, relevance, accuracy, level of details\"\n        \" of their responses. Each assistant receives an overall score on a scale of 1 to 10, where a higher\"\n        \" score indicates better overall performance.\\nPlease first output a single line containing only {num_responses}\"\n        \" values indicating the scores for Assistants 1 to {num_responses}, respectively. The {num_responses} scores are separated by\"\n        \" a space. In the subsequent line, please provide a comprehensive explanation of your evaluation,\"\n        \" avoiding any potential bias and ensuring that the order in which the responses were presented does\"\n        \" not affect your judgment.\"\n    )\n    system_prompt: str = \"You are a helpful and precise assistant for checking the quality of the answer.\"\n\n    def generate_prompt(self, input: str, generations: List[str]) -&gt; Prompt:\n        \"\"\"Generates a prompt following the JudgeLM specification.\n\n        Args:\n            input (str): the input to be used for the prompt.\n            generations (List[str]): the generations to be used for the prompt.\n\n        Returns:\n            Prompt: the generated prompt.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.preference import JudgeLMTask\n            &gt;&gt;&gt; task = JudgeLMTask(system_prompt=\"You are a helpful assistant.\")\n            &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\n            Prompt(\n                system_prompt=\"You are a helpful assistant.\",\n                formatted_prompt=\"[Question]\\nWhat are the first 5 Fibonacci numbers?\\n...\",\n            )\n        \"\"\"\n        render_kwargs = {\n            \"input\": input,\n            \"responses\": generations,\n            \"task_description\": self.task_description.format(\n                num_responses=len(generations)\n            ),\n        }\n        return Prompt(\n            system_prompt=self.system_prompt,\n            formatted_prompt=self.template.render(**render_kwargs),\n        )\n\n    def parse_output(self, output: str) -&gt; JudgeLMOutput:\n        \"\"\"Parses the output of the model into the desired format.\"\"\"\n        split_output = output.split(\"\\n\")\n        rating = [float(rating) for rating in split_output[0].split(\" \")]\n        rationale = \"\\n\".join(split_output[1:])\n        return JudgeLMOutput(rating=rating, rationale=rationale)\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/judgelm/#distilabel.tasks.preference.judgelm.JudgeLMTask.generate_prompt","title":"<code>generate_prompt(input, generations)</code>","text":"<p>Generates a prompt following the JudgeLM specification.</p> <pre><code>    Args:\n        input (str): the input to be used for the prompt.\n        generations (List[str]): the generations to be used for the prompt.\n\n    Returns:\n        Prompt: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.preference import JudgeLMTask\n        &gt;&gt;&gt; task = JudgeLMTask(system_prompt=\"You are a helpful assistant.\")\n        &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\n        Prompt(\n            system_prompt=\"You are a helpful assistant.\",\n            formatted_prompt=\"[Question]\n</code></pre> <p>What are the first 5 Fibonacci numbers? ...\",             )</p> Source code in <code>src/distilabel/tasks/preference/judgelm.py</code> <pre><code>def generate_prompt(self, input: str, generations: List[str]) -&gt; Prompt:\n    \"\"\"Generates a prompt following the JudgeLM specification.\n\n    Args:\n        input (str): the input to be used for the prompt.\n        generations (List[str]): the generations to be used for the prompt.\n\n    Returns:\n        Prompt: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.preference import JudgeLMTask\n        &gt;&gt;&gt; task = JudgeLMTask(system_prompt=\"You are a helpful assistant.\")\n        &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\n        Prompt(\n            system_prompt=\"You are a helpful assistant.\",\n            formatted_prompt=\"[Question]\\nWhat are the first 5 Fibonacci numbers?\\n...\",\n        )\n    \"\"\"\n    render_kwargs = {\n        \"input\": input,\n        \"responses\": generations,\n        \"task_description\": self.task_description.format(\n            num_responses=len(generations)\n        ),\n    }\n    return Prompt(\n        system_prompt=self.system_prompt,\n        formatted_prompt=self.template.render(**render_kwargs),\n    )\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/judgelm/#distilabel.tasks.preference.judgelm.JudgeLMTask.parse_output","title":"<code>parse_output(output)</code>","text":"<p>Parses the output of the model into the desired format.</p> Source code in <code>src/distilabel/tasks/preference/judgelm.py</code> <pre><code>def parse_output(self, output: str) -&gt; JudgeLMOutput:\n    \"\"\"Parses the output of the model into the desired format.\"\"\"\n    split_output = output.split(\"\\n\")\n    rating = [float(rating) for rating in split_output[0].split(\" \")]\n    rationale = \"\\n\".join(split_output[1:])\n    return JudgeLMOutput(rating=rating, rationale=rationale)\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/ultrafeedback/","title":"ultrafeedback","text":""},{"location":"reference/distilabel/tasks/preference/ultrafeedback/#distilabel.tasks.preference.ultrafeedback.Rating","title":"<code>Rating</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>A <code>TypedDict</code> representing a rating.</p> Source code in <code>src/distilabel/tasks/preference/ultrafeedback.py</code> <pre><code>class Rating(TypedDict):\n    \"\"\"A `TypedDict` representing a rating.\"\"\"\n\n    value: int\n    description: str\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/ultrafeedback/#distilabel.tasks.preference.ultrafeedback.UltraFeedbackOutput","title":"<code>UltraFeedbackOutput</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>A <code>TypedDict</code> representing the output of an <code>UltraFeedbackTask</code>.</p> Source code in <code>src/distilabel/tasks/preference/ultrafeedback.py</code> <pre><code>class UltraFeedbackOutput(TypedDict):\n    \"\"\"A `TypedDict` representing the output of an `UltraFeedbackTask`.\"\"\"\n\n    rating: float\n    rationale: str\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/ultrafeedback/#distilabel.tasks.preference.ultrafeedback.UltraFeedbackTask","title":"<code>UltraFeedbackTask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>PreferenceTask</code></p> <p>A <code>PreferenceTask</code> following the prompt template used by ULTRAFEEDBACK.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used for generation. Defaults to <code>None</code>.</p> <code>'Your role is to evaluate text quality based on given criteria.'</code> <code>task_description</code> <code>Union[str, None]</code> <p>the description of the task. Defaults to <code>None</code>.</p> required <code>ratings</code> <code>Union[List[Rating], None]</code> <p>the ratings to be used for the task. Defaults to <code>None</code>.</p> required Source code in <code>src/distilabel/tasks/preference/ultrafeedback.py</code> <pre><code>@dataclass\nclass UltraFeedbackTask(PreferenceTask):\n    \"\"\"A `PreferenceTask` following the prompt template used by ULTRAFEEDBACK.\n\n    Args:\n        system_prompt (str, optional): the system prompt to be used for generation. Defaults to `None`.\n        task_description (Union[str, None], optional): the description of the task. Defaults to `None`.\n        ratings (Union[List[Rating], None], optional): the ratings to be used for the task. Defaults to `None`.\n    \"\"\"\n\n    ratings: List[Rating]\n    task_description: str\n\n    __jinja2_template__: ClassVar[str] = field(\n        default=_ULTRAFEEDBACK_TEMPLATE, init=False, repr=False\n    )\n    __subtasks__: ClassVar[List[str]] = [\n        \"text-quality\",\n        \"helpfulness\",\n        \"truthfulness\",\n        \"honesty\",\n        \"instruction-following\",\n    ]\n\n    system_prompt: (\n        str\n    ) = \"Your role is to evaluate text quality based on given criteria.\"\n\n    def generate_prompt(self, input: str, generations: List[str]) -&gt; Prompt:\n        \"\"\"Generates a prompt following the ULTRAFEEDBACK specification.\n\n        Args:\n            input (str): the input to be used for the prompt.\n            generations (List[str]): the generations to be used for the prompt.\n\n        Returns:\n            Prompt: the generated prompt.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.preference import UltraFeedbackTask\n            &gt;&gt;&gt; task = UltraFeedbackTask.for_text_quality()\n            &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\n            Prompt(\n                system_prompt=\"Your role is to evaluate text quality based on given criteria.\",\n                formatted_prompt=\"# General Text Quality Assessment\\nEvaluate the model's ...\",\n            )\n        \"\"\"\n        render_kwargs = {\n            \"task_description\": self.task_description,\n            \"ratings\": self.ratings,\n            \"input\": input,\n            \"responses\": generations,\n        }\n        return Prompt(\n            system_prompt=self.system_prompt,\n            formatted_prompt=self.template.render(**render_kwargs),\n        )\n\n    def parse_output(self, output: str) -&gt; List[UltraFeedbackOutput]:\n        \"\"\"Parses the output of the model into the desired format.\"\"\"\n        parsed_output = []\n        for section in output.split(\"#### Output for Text \")[1:]:\n            rating, rationale = section.split(\"\\n\")[1:3]\n            rating = float(rating.split(\": \")[1])\n            rationale = rationale.split(\": \")[1]\n            parsed_output.append(\n                UltraFeedbackOutput(rating=rating, rationale=rationale)\n            )\n        return parsed_output\n\n    def _to_argilla_rationale(\n        self,\n        dataset_row: Dict[str, Any],\n    ) -&gt; str:\n        \"\"\"Converts the rationale to the format expected by Argilla.\"\"\"\n        rationales = dataset_row.get(\"rationale\")\n        if rationales is None:\n            return \"\"\n        sections = []\n        for idx, rationale in enumerate(dataset_row[\"rationale\"], start=1):\n            sections.append(f\"Rationale for generation-{idx}:\\n{rationale}\\n\")\n        return \"\\n\".join(sections)\n\n    @classmethod\n    def for_text_quality(\n        cls,\n        system_prompt: Optional[str] = None,\n        task_description: Optional[str] = None,\n        ratings: Optional[List[Rating]] = None,\n    ) -&gt; \"UltraFeedbackTask\":\n        kwargs = {}\n        if system_prompt is not None:\n            kwargs.update({\"system_prompt\": system_prompt})\n        if task_description is None:\n            task_description = dedent(\n                \"\"\"\n                # General Text Quality Assessment\n                Evaluate the model's outputs based on various criteria:\n                1. **Correctness &amp; Informativeness**: Does the output provide accurate and helpful information?\n                2. **Honesty &amp; Uncertainty**: How confidently does the model convey its information, and does it express uncertainty appropriately?\n                3. **Truthfulness &amp; Hallucination**: Does the model introduce misleading or fabricated details?\n                4. **Instruction Following**: Does the model's output align with given instructions and the user's intent?\n                Your role is to provide a holistic assessment considering all the above factors.\n\n                **Scoring**: Rate outputs 1 to 5 based on the overall quality, considering all aspects:\n                \"\"\"\n            )\n        kwargs.update({\"task_description\": task_description})\n\n        if ratings is None:\n            ratings = [\n                Rating(\n                    value=1,\n                    description=\"**Low Quality**: Contains inaccuracies, may be entirely wrong or has severe hallucinations.\",\n                ),\n                Rating(\n                    value=2,\n                    description=\"**Moderate Quality**: Addresses some aspects, but has errors or is partially aligned with instructions.\",\n                ),\n                Rating(\n                    value=3,\n                    description=\"**Good**: Generally accurate but may contain minor errors or slight deviations.\",\n                ),\n                Rating(\n                    value=4,\n                    description=\"**Very Good**: Near perfect, with minor issues in terms of alignment or confidence.\",\n                ),\n                Rating(\n                    value=5,\n                    description=\"**Excellent**: Accurate, confident, aligned with instructions, and free of hallucinations.\",\n                ),\n            ]\n        kwargs.update({\"ratings\": ratings})\n        return cls(**kwargs)\n\n    @classmethod\n    def for_helpfulness(\n        cls,\n        system_prompt: Optional[str] = None,\n        task_description: Optional[str] = None,\n        ratings: Optional[List[Rating]] = None,\n    ) -&gt; \"UltraFeedbackTask\":\n        kwargs = {}\n        if system_prompt is not None:\n            kwargs.update({\"system_prompt\": system_prompt})\n\n        if task_description is None:\n            task_description = dedent(\n                \"\"\"\n                # Informativeness / Helpfulness Assessment\n                Evaluate if model's outputs fulfill task objectives and provide high-quality, correct, and, informative content.\n                Helpfulness assessment emphasizes **Overall Quality** regarding correctness and informativeness.\n                **Correctness**: Accurate computation, reasoning steps, and outputs without misunderstandings or fabrication.\n\n                **Scoring**: Score 1 to 5 based on extent of helpfulness, regarding both informativeness and correctness:\n                \"\"\"\n            )\n        kwargs.update({\"task_description\": task_description})\n        if ratings is None:\n            ratings = [\n                Rating(\n                    value=1,\n                    description=\"**Severely Incorrect**: Contains significant inaccuracies or fabricated content, even if comprehensive information is provided.\",\n                ),\n                Rating(\n                    value=2,\n                    description=\"**Partially Incorrect**: Contains errors that may cause confusion, even though comprehensive information is present.\",\n                ),\n                Rating(\n                    value=3,\n                    description=\"**Correct**: Accurate and provides useful information that meets the task's requirements.\",\n                ),\n                Rating(\n                    value=4,\n                    description=\"**Highly Informative**: Accurate and extensive, providing valuable insights and detailed information.\",\n                ),\n                Rating(\n                    value=5,\n                    description=\"**Outstandingly Helpful**: Both accurate and in-depth, offering profound insights and comprehensive information.\",\n                ),\n            ]\n        kwargs.update({\"ratings\": ratings})\n        return cls(**kwargs)\n\n    @classmethod\n    def for_truthfulness(\n        cls,\n        system_prompt: Optional[str] = None,\n        task_description: Optional[str] = None,\n        ratings: Optional[List[Rating]] = None,\n    ) -&gt; \"UltraFeedbackTask\":\n        kwargs = {}\n        if system_prompt is not None:\n            kwargs.update({\"system_prompt\": system_prompt})\n\n        if task_description is None:\n            task_description = dedent(\n                \"\"\"\n                # Truthfulness and Hallucination Assessment\n                Evaluate the model's accuracy in providing information without introducing misleading or fabricated details.\n                Assign numeric identifier (or \"None\") from 1 to 3 for each type of hallucination:\n                1. **Contradictory with the World (Factual Error)**: Entities, locations, concepts, or events that conflict with established knowledge.\n                2. **Contradictory with Instruction and Input**: Responses diverge, introducing new facts not aligned with instructions or inputs.\n                3. **Self-Contradictory / Logical Error**: Responses contain internal contradictions or logical errors within each independent text.\n\n                **Scoring**: Rate outputs 1 to 5 based on the extent of hallucination:\n                \"\"\"\n            )\n        kwargs.update({\"task_description\": task_description})\n\n        if ratings is None:\n            ratings = [\n                Rating(\n                    value=1,\n                    description=\"**Completely Hallucinated**: Entirely unreliable due to hallucinations.\",\n                ),\n                Rating(\n                    value=2,\n                    description=\"**Severe Hallucination**: Nearly half contains hallucinations, severe deviation from main points.\",\n                ),\n                Rating(\n                    value=3,\n                    description=\"**Partial Hallucination / Misunderstanding**: Overall truthful, partial misunderstanding due to hallucinations.\",\n                ),\n                Rating(\n                    value=4,\n                    description=\"**Insignificant Hallucination**: Mostly truthful, slight hallucination not affecting main points.\",\n                ),\n                Rating(\n                    value=5, description=\"**No Hallucination**: Free of hallucinations.\"\n                ),\n            ]\n        kwargs.update({\"ratings\": ratings})\n        return cls(**kwargs)\n\n    @classmethod\n    def for_honesty(\n        cls,\n        system_prompt: Optional[str] = None,\n        task_description: Optional[str] = None,\n        ratings: Optional[List[Rating]] = None,\n    ) -&gt; \"UltraFeedbackTask\":\n        kwargs = {}\n        if system_prompt is not None:\n            kwargs.update({\"system_prompt\": system_prompt})\n        if task_description is None:\n            task_description = dedent(\n                \"\"\"\n                # Honesty and Uncertainty Expression Assessment\n                Assess how well the model conveys honesty and uncertainty. Evaluate if the model's confidence aligns with response correctness.\n                - **Indicators of Uncertainty**:\n                    1. Weakeners: e.g., 'I guess,' 'probably.'\n                    2. Refusal to answer.\n                    3. Verbalized confidence scores: [0, 20] low; (20, 40] uncertain; (40, 60] moderate; (60, 80] leaning confident; (80, 100] high.\n                - **No uncertainty expression indicate confidence.**\n                - **Response Correctness**: Align with ground truth, or provide accurate content without fabrication.\n\n                **Scoring**: Rate outputs 1 to 5 based on the extent of honesty and uncertainty:\n                \"\"\"\n            )\n        kwargs.update({\"task_description\": task_description})\n        if ratings is None:\n            ratings = [\n                Rating(\n                    value=1,\n                    description=\"**Confidently Incorrect**: Confident but entirely wrong.\",\n                ),\n                Rating(\n                    value=2,\n                    description=\"**Confident with Significant Mistakes / Unconfident Incorrect**: Confident but contains major errors. Unconfident and entirely wrong.\",\n                ),\n                Rating(\n                    value=3,\n                    description=\"**Uncertain / 'I Don't Know' / Subtle Mistakes**: 'I don't know' or declines. Confident but contains minor errors. Unconfident and contains significant mistakes.\",\n                ),\n                Rating(\n                    value=4,\n                    description=\"**Correct but Uncertain / Expressed Subtle Mistakes**: Correct but unconfident.\",\n                ),\n                Rating(\n                    value=5,\n                    description=\"**Correct and Confident / Precisely Express Uncertainty**: Correct and confident. Makes mistakes, but precisely acknowledges minor errors and indicates uncertainty on potential mistakes.\",\n                ),\n            ]\n        kwargs.update({\"ratings\": ratings})\n\n        return cls(**kwargs)\n\n    @classmethod\n    def for_instruction_following(\n        cls,\n        system_prompt: Optional[str] = None,\n        task_description: Optional[str] = None,\n        ratings: Optional[List[Rating]] = None,\n    ) -&gt; \"UltraFeedbackTask\":\n        kwargs = {}\n        if system_prompt is not None:\n            kwargs.update({\"system_prompt\": system_prompt})\n        if task_description is None:\n            task_description = dedent(\n                \"\"\"\n                # Instruction Following Assessment\n                Evaluate alignment between output and intent. Assess understanding of task goal and restrictions.\n                **Instruction Components**: Task Goal (intended outcome), Restrictions (text styles, formats, or designated methods, etc).\n\n                **Scoring**: Rate outputs 1 to 5:\n                \"\"\"\n            )\n        kwargs.update({\"task_description\": task_description})\n        if ratings is None:\n            ratings = [\n                Rating(value=1, description=\"**Irrelevant**: No alignment.\"),\n                Rating(\n                    value=2,\n                    description=\"**Partial Focus**: Addresses one aspect poorly.\",\n                ),\n                Rating(\n                    value=3,\n                    description=\"**Partial Compliance**:\\n\\t- (1) Meets goal or restrictions, neglecting other.\\n\\t- (2) Acknowledges both but slight deviations.\",\n                ),\n                Rating(\n                    value=4,\n                    description=\"**Almost There**: Near alignment, minor deviations.\",\n                ),\n                Rating(\n                    value=5,\n                    description=\"**Comprehensive Compliance**: Fully aligns, meets all requirements.\",\n                ),\n            ]\n        kwargs.update({\"ratings\": ratings})\n\n        return cls(**kwargs)\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/ultrafeedback/#distilabel.tasks.preference.ultrafeedback.UltraFeedbackTask.generate_prompt","title":"<code>generate_prompt(input, generations)</code>","text":"<p>Generates a prompt following the ULTRAFEEDBACK specification.</p> <pre><code>    Args:\n        input (str): the input to be used for the prompt.\n        generations (List[str]): the generations to be used for the prompt.\n\n    Returns:\n        Prompt: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.preference import UltraFeedbackTask\n        &gt;&gt;&gt; task = UltraFeedbackTask.for_text_quality()\n        &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\n        Prompt(\n            system_prompt=\"Your role is to evaluate text quality based on given criteria.\",\n            formatted_prompt=\"# General Text Quality Assessment\n</code></pre> <p>Evaluate the model's ...\",             )</p> Source code in <code>src/distilabel/tasks/preference/ultrafeedback.py</code> <pre><code>def generate_prompt(self, input: str, generations: List[str]) -&gt; Prompt:\n    \"\"\"Generates a prompt following the ULTRAFEEDBACK specification.\n\n    Args:\n        input (str): the input to be used for the prompt.\n        generations (List[str]): the generations to be used for the prompt.\n\n    Returns:\n        Prompt: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.preference import UltraFeedbackTask\n        &gt;&gt;&gt; task = UltraFeedbackTask.for_text_quality()\n        &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\n        Prompt(\n            system_prompt=\"Your role is to evaluate text quality based on given criteria.\",\n            formatted_prompt=\"# General Text Quality Assessment\\nEvaluate the model's ...\",\n        )\n    \"\"\"\n    render_kwargs = {\n        \"task_description\": self.task_description,\n        \"ratings\": self.ratings,\n        \"input\": input,\n        \"responses\": generations,\n    }\n    return Prompt(\n        system_prompt=self.system_prompt,\n        formatted_prompt=self.template.render(**render_kwargs),\n    )\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/ultrafeedback/#distilabel.tasks.preference.ultrafeedback.UltraFeedbackTask.parse_output","title":"<code>parse_output(output)</code>","text":"<p>Parses the output of the model into the desired format.</p> Source code in <code>src/distilabel/tasks/preference/ultrafeedback.py</code> <pre><code>def parse_output(self, output: str) -&gt; List[UltraFeedbackOutput]:\n    \"\"\"Parses the output of the model into the desired format.\"\"\"\n    parsed_output = []\n    for section in output.split(\"#### Output for Text \")[1:]:\n        rating, rationale = section.split(\"\\n\")[1:3]\n        rating = float(rating.split(\": \")[1])\n        rationale = rationale.split(\": \")[1]\n        parsed_output.append(\n            UltraFeedbackOutput(rating=rating, rationale=rationale)\n        )\n    return parsed_output\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/ultrajudge/","title":"ultrajudge","text":""},{"location":"reference/distilabel/tasks/preference/ultrajudge/#distilabel.tasks.preference.ultrajudge.Area","title":"<code>Area</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>A <code>TypedDict</code> representing an area of evaluation.</p> Source code in <code>src/distilabel/tasks/preference/ultrajudge.py</code> <pre><code>class Area(TypedDict):\n    \"\"\"A `TypedDict` representing an area of evaluation.\"\"\"\n\n    rating: float\n    rationale: str\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/ultrajudge/#distilabel.tasks.preference.ultrajudge.UltraJudgeOutput","title":"<code>UltraJudgeOutput</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>A <code>TypedDict</code> representing the output of the UltraJudge task.</p> Source code in <code>src/distilabel/tasks/preference/ultrajudge.py</code> <pre><code>class UltraJudgeOutput(TypedDict):\n    \"\"\"A `TypedDict` representing the output of the UltraJudge task.\"\"\"\n\n    rating: float\n    areas: Dict[str, Area]\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/ultrajudge/#distilabel.tasks.preference.ultrajudge.UltraJudgeTask","title":"<code>UltraJudgeTask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>PreferenceTask</code></p> <p>A <code>PreferenceTask</code> for the UltraJudge task. The <code>UltraJudge</code> task has been defined at Argilla specically for a better evaluation using AI Feedback. The task is defined based on both UltraFeedback and JudgeLM, but with several improvements / modifications.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used for generation. Defaults to <code>None</code>.</p> <code>\"You are an evaluator tasked with assessing AI assistants' responses from the perspective of typical user preferences. Your critical analysis should focus on human-like engagement, solution effectiveness, accuracy, clarity, and creativity. Approach each response as if you were the user, considering how well the response meets your needs and expectations in a real-world scenario. Provide detailed feedback that highlights strengths and areas for improvement in each response, keeping in mind the goal of simulating a human's preferred choice. Your evaluation should be impartial and thorough, reflecting a human's perspective in preferring responses that are practical, clear, authentic, and aligned with their intent. Avoid bias, and focus on the content and quality of the responses.\"</code> <code>task_description</code> <code>Union[str, None]</code> <p>the description of the task. Defaults to <code>None</code>.</p> <code>\"Your task is to rigorously evaluate the performance of {num_responses} AI assistants, simulating a human's perspective. You will assess each response based on four key domains, reflecting aspects that are typically valued by humans: {areas}. First provide a score between 0 and 10 and write a detailed feedback for each area and assistant. Finally, provide a list of {num_responses} scores, each separated by a space, to reflect the performance of Assistants 1 to {num_responses}.\"</code> <code>areas</code> <code>List[str]</code> <p>the areas to be used for the task. Defaults to a list of four areas: \"Practical Accuracy\", \"Clarity &amp; Transparency\", \"Authenticity &amp; Reliability\", and \"Compliance with Intent\".</p> <code>field(default_factory=lambda : ['Practical Accuracy', 'Clarity &amp; Transparency', 'Authenticity &amp; Reliability', 'Compliance with Intent'])</code> Source code in <code>src/distilabel/tasks/preference/ultrajudge.py</code> <pre><code>@dataclass\nclass UltraJudgeTask(PreferenceTask):\n    \"\"\"A `PreferenceTask` for the UltraJudge task. The `UltraJudge` task has been defined\n    at Argilla specically for a better evaluation using AI Feedback. The task is defined\n    based on both UltraFeedback and JudgeLM, but with several improvements / modifications.\n\n    Args:\n        system_prompt (str, optional): the system prompt to be used for generation. Defaults to `None`.\n        task_description (Union[str, None], optional): the description of the task. Defaults to `None`.\n        areas (List[str], optional): the areas to be used for the task. Defaults to a list of four areas:\n            \"Practical Accuracy\", \"Clarity &amp; Transparency\", \"Authenticity &amp; Reliability\", and \"Compliance with Intent\".\n    \"\"\"\n\n    system_prompt: str = (\n        \"You are an evaluator tasked with assessing AI assistants' responses from the perspective of typical user preferences.\"\n        \" Your critical analysis should focus on human-like engagement, solution effectiveness, accuracy, clarity, and\"\n        \" creativity. Approach each response as if you were the user, considering how well the response meets your needs\"\n        \" and expectations in a real-world scenario. Provide detailed feedback that highlights strengths and areas for\"\n        \" improvement in each response, keeping in mind the goal of simulating a human's preferred choice. \"\n        \"Your evaluation should be impartial and thorough, reflecting a human's perspective in preferring responses that are practical,\"\n        \" clear, authentic, and aligned with their intent. Avoid bias, and focus on the content and quality of the responses.\"\n    )\n\n    task_description: str = (\n        \"Your task is to rigorously evaluate the performance of {num_responses} AI assistants, simulating a human's perspective.\"\n        \" You will assess each response based on four key domains, reflecting aspects that are typically valued by humans:\"\n        \" {areas}.\"\n        \" First provide a score between 0 and 10 and write a detailed feedback for each area and assistant.\"\n        \" Finally, provide a list of {num_responses} scores, each separated by a space, to reflect the performance of Assistants 1 to {num_responses}.\"\n    )\n\n    areas: List[str] = field(\n        default_factory=lambda: [\n            \"Practical Accuracy\",\n            \"Clarity &amp; Transparency\",\n            \"Authenticity &amp; Reliability\",\n            \"Compliance with Intent\",\n        ]\n    )\n\n    __jinja2_template__: ClassVar[str] = field(\n        default=_ULTRAJUDGE_TEMPLATE, init=False, repr=False\n    )\n\n    @property\n    def output_args_names(self) -&gt; List[str]:\n        \"\"\"Returns the names of the output arguments of the task.\"\"\"\n        return [\"rating\", \"areas\"]\n\n    @property\n    def areas_str(self) -&gt; str:\n        \"\"\"Returns a string representation of the areas.\"\"\"\n        return \", \".join(self.areas[:-1]) + \", and \" + self.areas[-1]\n\n    @property\n    def extract_area_score_and_rationale_regex(self) -&gt; str:\n        \"\"\"Returns a regex to extract the area, score, and rationale from the output.\"\"\"\n        return rf\"({'|'.join(self.areas)})\\s*-\\s*(\\d+(?:\\.\\d+)?)\\n(.*?)(?=\\n\\n|\\Z)\"\n\n    @property\n    def extract_final_scores_regex(self) -&gt; str:\n        \"\"\"Returns a regex to extract the final scores from the output.\"\"\"\n        return r\"Final scores:\\s*((?:\\d+(?:\\.\\d+)?\\s*)+)\"\n\n    def generate_prompt(self, input: str, generations: List[str]) -&gt; Prompt:\n        \"\"\"Generates a prompt following the UltraJudge specification.\n\n        Args:\n            input (str): the input to be used for the prompt.\n            generations (List[str]): the generations to be used for the prompt.\n\n        Returns:\n            Prompt: the generated prompt.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.preference import UltraJudgeTask\n            &gt;&gt;&gt; task = UltraJudgeTask(system_prompt=\"You are a helpful assistant.\")\n            &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\n            Prompt(\n                system_prompt=\"You are a helpful assistant.\",\n                formatted_prompt=\"Your task is to rigorously evaluate the performance of ...\",\n            )\n        \"\"\"\n        render_kwargs = {\n            \"task_description\": self.task_description.format(\n                num_responses=len(generations), areas=self.areas_str\n            ),\n            \"instruction\": input,\n            \"responses\": generations,\n        }\n\n        return Prompt(\n            system_prompt=self.system_prompt,\n            formatted_prompt=self.template.render(**render_kwargs),\n        )\n\n    def parse_output(self, output: str) -&gt; List[UltraJudgeOutput]:\n        \"\"\"Parses the output of the model into the desired format.\"\"\"\n        num_areas = len(self.areas)\n        # `areas_results` includes num_generations * num_areas tuples\n        areas_results = re.findall(self.extract_area_score_and_rationale_regex, output)\n        final_scores = [\n            float(str_score)\n            for str_score in re.findall(self.extract_final_scores_regex, output)[\n                0\n            ].split(\" \")\n        ]\n\n        outputs = []\n        for i, rating in enumerate(final_scores):\n            areas = {}\n            # Get the areas for the i-th generation\n            for area in areas_results[i * num_areas : i * num_areas + num_areas]:\n                name, area_rating, rationale = area\n                areas[name] = Area(rating=area_rating, rationale=rationale)\n            outputs.append(UltraJudgeOutput(rating=rating, areas=areas))\n\n        return outputs\n\n    def _to_argilla_rationale(\n        self,\n        dataset_row: Dict[str, Any],\n    ) -&gt; str:\n        \"\"\"Gets the `rationale` column from a `datasets.Dataset` row and formats it\n        as expected by Argilla.\n        \"\"\"\n\n        def format_area(area):\n            sections = []\n            for title, ratings in area.items():\n                sections.append(title)\n                for k, v in ratings.items():\n                    sections.append(f\"{k}:{v}\")\n            return \"\\n\".join(sections)\n\n        rationales = []\n        for idx, area in enumerate(dataset_row[\"areas\"], start=1):\n            formatted_area = format_area(area)\n            rationales.append(f\"Rationale for generation-{idx}:\\n{formatted_area}\\n\")\n        return \"\\n\".join(rationales)\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/ultrajudge/#distilabel.tasks.preference.ultrajudge.UltraJudgeTask.areas_str","title":"<code>areas_str: str</code>  <code>property</code>","text":"<p>Returns a string representation of the areas.</p>"},{"location":"reference/distilabel/tasks/preference/ultrajudge/#distilabel.tasks.preference.ultrajudge.UltraJudgeTask.extract_area_score_and_rationale_regex","title":"<code>extract_area_score_and_rationale_regex: str</code>  <code>property</code>","text":"<p>Returns a regex to extract the area, score, and rationale from the output.</p>"},{"location":"reference/distilabel/tasks/preference/ultrajudge/#distilabel.tasks.preference.ultrajudge.UltraJudgeTask.extract_final_scores_regex","title":"<code>extract_final_scores_regex: str</code>  <code>property</code>","text":"<p>Returns a regex to extract the final scores from the output.</p>"},{"location":"reference/distilabel/tasks/preference/ultrajudge/#distilabel.tasks.preference.ultrajudge.UltraJudgeTask.output_args_names","title":"<code>output_args_names: List[str]</code>  <code>property</code>","text":"<p>Returns the names of the output arguments of the task.</p>"},{"location":"reference/distilabel/tasks/preference/ultrajudge/#distilabel.tasks.preference.ultrajudge.UltraJudgeTask.generate_prompt","title":"<code>generate_prompt(input, generations)</code>","text":"<p>Generates a prompt following the UltraJudge specification.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>the input to be used for the prompt.</p> required <code>generations</code> <code>List[str]</code> <p>the generations to be used for the prompt.</p> required <p>Returns:</p> Name Type Description <code>Prompt</code> <code>Prompt</code> <p>the generated prompt.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.preference import UltraJudgeTask\n&gt;&gt;&gt; task = UltraJudgeTask(system_prompt=\"You are a helpful assistant.\")\n&gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\nPrompt(\n    system_prompt=\"You are a helpful assistant.\",\n    formatted_prompt=\"Your task is to rigorously evaluate the performance of ...\",\n)\n</code></pre> Source code in <code>src/distilabel/tasks/preference/ultrajudge.py</code> <pre><code>def generate_prompt(self, input: str, generations: List[str]) -&gt; Prompt:\n    \"\"\"Generates a prompt following the UltraJudge specification.\n\n    Args:\n        input (str): the input to be used for the prompt.\n        generations (List[str]): the generations to be used for the prompt.\n\n    Returns:\n        Prompt: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.preference import UltraJudgeTask\n        &gt;&gt;&gt; task = UltraJudgeTask(system_prompt=\"You are a helpful assistant.\")\n        &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\", [\"0 1 1 2 3\", \"0 1 1 2 3\"])\n        Prompt(\n            system_prompt=\"You are a helpful assistant.\",\n            formatted_prompt=\"Your task is to rigorously evaluate the performance of ...\",\n        )\n    \"\"\"\n    render_kwargs = {\n        \"task_description\": self.task_description.format(\n            num_responses=len(generations), areas=self.areas_str\n        ),\n        \"instruction\": input,\n        \"responses\": generations,\n    }\n\n    return Prompt(\n        system_prompt=self.system_prompt,\n        formatted_prompt=self.template.render(**render_kwargs),\n    )\n</code></pre>"},{"location":"reference/distilabel/tasks/preference/ultrajudge/#distilabel.tasks.preference.ultrajudge.UltraJudgeTask.parse_output","title":"<code>parse_output(output)</code>","text":"<p>Parses the output of the model into the desired format.</p> Source code in <code>src/distilabel/tasks/preference/ultrajudge.py</code> <pre><code>def parse_output(self, output: str) -&gt; List[UltraJudgeOutput]:\n    \"\"\"Parses the output of the model into the desired format.\"\"\"\n    num_areas = len(self.areas)\n    # `areas_results` includes num_generations * num_areas tuples\n    areas_results = re.findall(self.extract_area_score_and_rationale_regex, output)\n    final_scores = [\n        float(str_score)\n        for str_score in re.findall(self.extract_final_scores_regex, output)[\n            0\n        ].split(\" \")\n    ]\n\n    outputs = []\n    for i, rating in enumerate(final_scores):\n        areas = {}\n        # Get the areas for the i-th generation\n        for area in areas_results[i * num_areas : i * num_areas + num_areas]:\n            name, area_rating, rationale = area\n            areas[name] = Area(rating=area_rating, rationale=rationale)\n        outputs.append(UltraJudgeOutput(rating=rating, areas=areas))\n\n    return outputs\n</code></pre>"},{"location":"reference/distilabel/tasks/text_generation/","title":"text_generation","text":""},{"location":"reference/distilabel/tasks/text_generation/base/","title":"base","text":""},{"location":"reference/distilabel/tasks/text_generation/base/#distilabel.tasks.text_generation.base.TextGenerationTask","title":"<code>TextGenerationTask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>Task</code></p> <p>A base <code>Task</code> definition for text generation using LLMs.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used. Defaults to <code>None</code>.</p> <code>\"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"</code> <code>principles</code> <code>Dict[str, List[str]]</code> <p>the principles to be used for the system prompt. Defaults to <code>None</code>.</p> <code>field(default_factory=lambda : {'harmlessness': harmlessness, 'helpfulness': helpfulness, 'truthfulness': truthfulness, 'honesty': honesty, 'verbalized_calibration': verbalized_calibration}, repr=False)</code> <code>principles_distribution</code> <code>Union[Dict[str, float], Literal['balanced'], None]</code> <p>the distribution of principles to be used for the system prompt. Defaults to <code>None</code>.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask\n&gt;&gt;&gt; task = TextGenerationTask()\n</code></pre> Source code in <code>src/distilabel/tasks/text_generation/base.py</code> <pre><code>@dataclass\nclass TextGenerationTask(Task):\n    \"\"\"A base `Task` definition for text generation using LLMs.\n\n    Args:\n        system_prompt (str, optional): the system prompt to be used. Defaults to `None`.\n        principles (Dict[str, List[str]], optional): the principles to be used for the system prompt.\n            Defaults to `None`.\n        principles_distribution (Union[Dict[str, float], Literal[\"balanced\"], None], optional): the\n            distribution of principles to be used for the system prompt. Defaults to `None`.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask\n        &gt;&gt;&gt; task = TextGenerationTask()\n    \"\"\"\n\n    system_prompt: str = (\n        \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible,\"\n        \" while being safe. Your answers should not include any harmful, unethical, racist, sexist,\"\n        \" toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased\"\n        \" and positive in nature.\\nIf a question does not make any sense, or is not factually coherent,\"\n        \" explain why instead of answering something not correct. If you don't know the answer to a\"\n        \" question, please don't share false information.\"\n    )\n    principles: Dict[str, List[str]] = field(\n        default_factory=lambda: {\n            \"harmlessness\": UltraFeedbackPrinciples.harmlessness,\n            \"helpfulness\": UltraFeedbackPrinciples.helpfulness,\n            \"truthfulness\": UltraFeedbackPrinciples.truthfulness,\n            \"honesty\": UltraFeedbackPrinciples.honesty,\n            \"verbalized_calibration\": UltraFeedbackPrinciples.verbalized_calibration,\n        },\n        repr=False,\n    )\n    principles_distribution: Union[Dict[str, float], Literal[\"balanced\"], None] = None\n\n    def __post_init__(self) -&gt; None:\n        \"\"\"Validates the `principles_distribution` if it is a dict.\n\n        Raises:\n            ValueError: if the `principles_distribution` is a dict and it does not sum to 1.0.\n            ValueError: if the `principles` are not included in the `principles_distribution`.\n        \"\"\"\n        if isinstance(self.principles_distribution, dict):\n            not_included_principles = [\n                principle\n                for principle in self.principles\n                if principle not in self.principles_distribution\n            ]\n            if not_included_principles:\n                principles_str = \", \".join(\n                    [f\"'{principle}'\" for principle in not_included_principles]\n                )\n                raise ValueError(\n                    f\"Principles {principles_str} included in `principles` is not in\"\n                    \" `principles_distribution`\"\n                )\n\n            if sum(self.principles_distribution.values()) != 1.0:\n                raise ValueError(\n                    \"`principles_distribution` must sum to 1.0 if it is a dict containing\"\n                    \" the distribution of principles to use.\"\n                )\n\n    def _get_principle(self) -&gt; str:\n        \"\"\"Gets a principle from the `principles` dict respecting the `principal_distribution`.\n\n        Returns:\n            str: the principle to be used.\n        \"\"\"\n        if isinstance(self.principles_distribution, dict):\n            principle_group = random.choices(\n                list(self.principles_distribution.keys()),\n                weights=list(self.principles_distribution.values()),\n                k=1,\n            )[0]\n        else:\n            principle_group = random.choice(list(self.principles.keys()))\n        return random.choice(self.principles[principle_group])\n\n    def generate_prompt(self, input: str) -&gt; Prompt:\n        \"\"\"Generates the prompt to be used for generation.\n\n        Args:\n            input (str): the input to be used for generation.\n\n        Returns:\n            Prompt: the generated prompt.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask\n            &gt;&gt;&gt; task = TextGenerationTask(system_prompt=\"You are a helpful assistant.\")\n            &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\")\n            Prompt(system_prompt='You are a helpful assistant.', formatted_prompt='What are the first 5 Fibonacci numbers?')\n        \"\"\"\n        system_prompt = self.system_prompt\n        if self.principles_distribution is not None:\n            principle = self._get_principle()\n            system_prompt += \" \" + principle\n        return Prompt(system_prompt=system_prompt, formatted_prompt=input)\n\n    def parse_output(self, output: str) -&gt; dict[str, str]:\n        \"\"\"Parses the output of the LLM into the desired format.\"\"\"\n        return {\"generations\": output}\n\n    @property\n    def input_args_names(self) -&gt; list[str]:\n        \"\"\"Returns the input args names for the task.\"\"\"\n        return [\"input\"]\n\n    @property\n    def output_args_names(self) -&gt; list[str]:\n        \"\"\"Returns the output args names for the task.\"\"\"\n        return [\"generations\"]\n</code></pre>"},{"location":"reference/distilabel/tasks/text_generation/base/#distilabel.tasks.text_generation.base.TextGenerationTask.input_args_names","title":"<code>input_args_names: list[str]</code>  <code>property</code>","text":"<p>Returns the input args names for the task.</p>"},{"location":"reference/distilabel/tasks/text_generation/base/#distilabel.tasks.text_generation.base.TextGenerationTask.output_args_names","title":"<code>output_args_names: list[str]</code>  <code>property</code>","text":"<p>Returns the output args names for the task.</p>"},{"location":"reference/distilabel/tasks/text_generation/base/#distilabel.tasks.text_generation.base.TextGenerationTask.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validates the <code>principles_distribution</code> if it is a dict.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the <code>principles_distribution</code> is a dict and it does not sum to 1.0.</p> <code>ValueError</code> <p>if the <code>principles</code> are not included in the <code>principles_distribution</code>.</p> Source code in <code>src/distilabel/tasks/text_generation/base.py</code> <pre><code>def __post_init__(self) -&gt; None:\n    \"\"\"Validates the `principles_distribution` if it is a dict.\n\n    Raises:\n        ValueError: if the `principles_distribution` is a dict and it does not sum to 1.0.\n        ValueError: if the `principles` are not included in the `principles_distribution`.\n    \"\"\"\n    if isinstance(self.principles_distribution, dict):\n        not_included_principles = [\n            principle\n            for principle in self.principles\n            if principle not in self.principles_distribution\n        ]\n        if not_included_principles:\n            principles_str = \", \".join(\n                [f\"'{principle}'\" for principle in not_included_principles]\n            )\n            raise ValueError(\n                f\"Principles {principles_str} included in `principles` is not in\"\n                \" `principles_distribution`\"\n            )\n\n        if sum(self.principles_distribution.values()) != 1.0:\n            raise ValueError(\n                \"`principles_distribution` must sum to 1.0 if it is a dict containing\"\n                \" the distribution of principles to use.\"\n            )\n</code></pre>"},{"location":"reference/distilabel/tasks/text_generation/base/#distilabel.tasks.text_generation.base.TextGenerationTask.generate_prompt","title":"<code>generate_prompt(input)</code>","text":"<p>Generates the prompt to be used for generation.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>the input to be used for generation.</p> required <p>Returns:</p> Name Type Description <code>Prompt</code> <code>Prompt</code> <p>the generated prompt.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask\n&gt;&gt;&gt; task = TextGenerationTask(system_prompt=\"You are a helpful assistant.\")\n&gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\")\nPrompt(system_prompt='You are a helpful assistant.', formatted_prompt='What are the first 5 Fibonacci numbers?')\n</code></pre> Source code in <code>src/distilabel/tasks/text_generation/base.py</code> <pre><code>def generate_prompt(self, input: str) -&gt; Prompt:\n    \"\"\"Generates the prompt to be used for generation.\n\n    Args:\n        input (str): the input to be used for generation.\n\n    Returns:\n        Prompt: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.text_generation import TextGenerationTask\n        &gt;&gt;&gt; task = TextGenerationTask(system_prompt=\"You are a helpful assistant.\")\n        &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\")\n        Prompt(system_prompt='You are a helpful assistant.', formatted_prompt='What are the first 5 Fibonacci numbers?')\n    \"\"\"\n    system_prompt = self.system_prompt\n    if self.principles_distribution is not None:\n        principle = self._get_principle()\n        system_prompt += \" \" + principle\n    return Prompt(system_prompt=system_prompt, formatted_prompt=input)\n</code></pre>"},{"location":"reference/distilabel/tasks/text_generation/base/#distilabel.tasks.text_generation.base.TextGenerationTask.parse_output","title":"<code>parse_output(output)</code>","text":"<p>Parses the output of the LLM into the desired format.</p> Source code in <code>src/distilabel/tasks/text_generation/base.py</code> <pre><code>def parse_output(self, output: str) -&gt; dict[str, str]:\n    \"\"\"Parses the output of the LLM into the desired format.\"\"\"\n    return {\"generations\": output}\n</code></pre>"},{"location":"reference/distilabel/tasks/text_generation/llama/","title":"llama","text":""},{"location":"reference/distilabel/tasks/text_generation/llama/#distilabel.tasks.text_generation.llama.Llama2TextGenerationTask","title":"<code>Llama2TextGenerationTask</code>","text":"<p>             Bases: <code>TextGenerationTask</code></p> <p>A <code>TextGenerationTask</code> for the Llama2 model.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used. Defaults to <code>None</code>.</p> required <code>principles</code> <code>Dict[str, List[str]]</code> <p>the principles to be used for the system prompt. Defaults to <code>None</code>.</p> required <code>principles_distribution</code> <code>Union[Dict[str, float], Literal[balanced], None]</code> <p>the distribution of principles to be used for the system prompt. Defaults to <code>None</code>.</p> required Source code in <code>src/distilabel/tasks/text_generation/llama.py</code> <pre><code>class Llama2TextGenerationTask(TextGenerationTask):\n    \"\"\"A `TextGenerationTask` for the Llama2 model.\n\n    Args:\n        system_prompt (str, optional): the system prompt to be used. Defaults to `None`.\n        principles (Dict[str, List[str]], optional): the principles to be used for the system prompt.\n            Defaults to `None`.\n        principles_distribution (Union[Dict[str, float], Literal[\"balanced\"], None], optional): the\n            distribution of principles to be used for the system prompt. Defaults to `None`.\n    \"\"\"\n\n    def generate_prompt(self, input: str) -&gt; str:\n        \"\"\"Generates a prompt for the Llama2 model.\n\n        Args:\n            input (str): the input to be used for the prompt.\n\n        Returns:\n            str: the generated prompt.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.text_generation import Llama2TextGenerationTask\n            &gt;&gt;&gt; task = Llama2TextGenerationTask(system_prompt=\"You are a helpful assistant.\")\n            &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\")\n            '&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are a helpful assistant.&lt;&lt;/SYS&gt;&gt;\\n\\nWhat are the first 5 Fibonacci numbers? [/INST]'\n        \"\"\"\n        return Prompt(\n            system_prompt=self.system_prompt,\n            formatted_prompt=input,\n        ).format_as(\"llama2\")  # type: ignore\n</code></pre>"},{"location":"reference/distilabel/tasks/text_generation/llama/#distilabel.tasks.text_generation.llama.Llama2TextGenerationTask.generate_prompt","title":"<code>generate_prompt(input)</code>","text":"<p>Generates a prompt for the Llama2 model.</p> <pre><code>    Args:\n        input (str): the input to be used for the prompt.\n\n    Returns:\n        str: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.text_generation import Llama2TextGenerationTask\n        &gt;&gt;&gt; task = Llama2TextGenerationTask(system_prompt=\"You are a helpful assistant.\")\n        &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\")\n        '&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\n</code></pre> <p>You are a helpful assistant.&lt;&gt;</p> <p>What are the first 5 Fibonacci numbers? [/INST]'</p> Source code in <code>src/distilabel/tasks/text_generation/llama.py</code> <pre><code>def generate_prompt(self, input: str) -&gt; str:\n    \"\"\"Generates a prompt for the Llama2 model.\n\n    Args:\n        input (str): the input to be used for the prompt.\n\n    Returns:\n        str: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.text_generation import Llama2TextGenerationTask\n        &gt;&gt;&gt; task = Llama2TextGenerationTask(system_prompt=\"You are a helpful assistant.\")\n        &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\")\n        '&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\nYou are a helpful assistant.&lt;&lt;/SYS&gt;&gt;\\n\\nWhat are the first 5 Fibonacci numbers? [/INST]'\n    \"\"\"\n    return Prompt(\n        system_prompt=self.system_prompt,\n        formatted_prompt=input,\n    ).format_as(\"llama2\")  # type: ignore\n</code></pre>"},{"location":"reference/distilabel/tasks/text_generation/openai/","title":"openai","text":""},{"location":"reference/distilabel/tasks/text_generation/openai/#distilabel.tasks.text_generation.openai.OpenAITextGenerationTask","title":"<code>OpenAITextGenerationTask</code>","text":"<p>             Bases: <code>TextGenerationTask</code></p> <p>A <code>TextGenerationTask</code> for any chat-completion OpenAI model.</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used. Defaults to <code>None</code>.</p> required <code>principles</code> <code>Dict[str, List[str]]</code> <p>the principles to be used for the system prompt. Defaults to <code>None</code>.</p> required <code>principles_distribution</code> <code>Union[Dict[str, float], Literal[balanced], None]</code> <p>the distribution of principles to be used for the system prompt. Defaults to <code>None</code>.</p> required Source code in <code>src/distilabel/tasks/text_generation/openai.py</code> <pre><code>class OpenAITextGenerationTask(TextGenerationTask):\n    \"\"\"A `TextGenerationTask` for any chat-completion OpenAI model.\n\n    Args:\n        system_prompt (str, optional): the system prompt to be used. Defaults to `None`.\n        principles (Dict[str, List[str]], optional): the principles to be used for the system prompt.\n            Defaults to `None`.\n        principles_distribution (Union[Dict[str, float], Literal[\"balanced\"], None], optional): the\n            distribution of principles to be used for the system prompt. Defaults to `None`.\n    \"\"\"\n\n    def generate_prompt(self, input: str) -&gt; List[\"ChatCompletion\"]:\n        \"\"\"Generates a prompt for any chat-completion OpenAI model.\n\n        Args:\n            input (str): the input to be used for the prompt.\n\n        Returns:\n            List[ChatCompletion]: the generated prompt.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.text_generation import OpenAITextGenerationTask\n            &gt;&gt;&gt; task = OpenAITextGenerationTask(system_prompt=\"You are a helpful assistant.\")\n            &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\")\n            [\n                {'role': 'system', 'content': 'You are a helpful assistant.'},\n                {'role': 'user', 'content': 'What are the first 5 Fibonacci numbers?'},\n            ]\n        \"\"\"\n        return Prompt(\n            system_prompt=self.system_prompt,\n            formatted_prompt=input,\n        ).format_as(\"openai\")  # type: ignore\n</code></pre>"},{"location":"reference/distilabel/tasks/text_generation/openai/#distilabel.tasks.text_generation.openai.OpenAITextGenerationTask.generate_prompt","title":"<code>generate_prompt(input)</code>","text":"<p>Generates a prompt for any chat-completion OpenAI model.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>str</code> <p>the input to be used for the prompt.</p> required <p>Returns:</p> Type Description <code>List[ChatCompletion]</code> <p>List[ChatCompletion]: the generated prompt.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from distilabel.tasks.text_generation import OpenAITextGenerationTask\n&gt;&gt;&gt; task = OpenAITextGenerationTask(system_prompt=\"You are a helpful assistant.\")\n&gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\")\n[\n    {'role': 'system', 'content': 'You are a helpful assistant.'},\n    {'role': 'user', 'content': 'What are the first 5 Fibonacci numbers?'},\n]\n</code></pre> Source code in <code>src/distilabel/tasks/text_generation/openai.py</code> <pre><code>def generate_prompt(self, input: str) -&gt; List[\"ChatCompletion\"]:\n    \"\"\"Generates a prompt for any chat-completion OpenAI model.\n\n    Args:\n        input (str): the input to be used for the prompt.\n\n    Returns:\n        List[ChatCompletion]: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.text_generation import OpenAITextGenerationTask\n        &gt;&gt;&gt; task = OpenAITextGenerationTask(system_prompt=\"You are a helpful assistant.\")\n        &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\")\n        [\n            {'role': 'system', 'content': 'You are a helpful assistant.'},\n            {'role': 'user', 'content': 'What are the first 5 Fibonacci numbers?'},\n        ]\n    \"\"\"\n    return Prompt(\n        system_prompt=self.system_prompt,\n        formatted_prompt=input,\n    ).format_as(\"openai\")  # type: ignore\n</code></pre>"},{"location":"reference/distilabel/tasks/text_generation/principles/","title":"principles","text":""},{"location":"reference/distilabel/tasks/text_generation/principles/#distilabel.tasks.text_generation.principles.UltraFeedbackPrinciples","title":"<code>UltraFeedbackPrinciples</code>","text":"<p>A class containing a list of principles from the UltraFeedback paper, that can be injected into the system prompt given to the LLM.</p> References <ul> <li>https://github.com/OpenBMB/UltraFeedback</li> </ul> Source code in <code>src/distilabel/tasks/text_generation/principles.py</code> <pre><code>class UltraFeedbackPrinciples:\n    \"\"\"A class containing a list of principles from the UltraFeedback paper, that can\n    be injected into the system prompt given to the LLM.\n\n    References:\n        - https://github.com/OpenBMB/UltraFeedback\n    \"\"\"\n\n    helpfulness = [\n        \"The assistant should provide users with accurate, relevant, and up-to-date information, ensuring that the content is positive, interesting, engaging, educational, and helpful.\",\n        \"As an AI assistant, it's your job to ensure that the information you provide to users is accurate, current, and relevant. Offer comprehensive details to educate and engage the user.\",\n        \"Your role as an AI assistant is to be a reliable and helpful source of information. Always verify the facts and make sure they're up-to-date before sharing them.\",\n        \"As an AI assistant, inspire users with your knowledgeable insights. Provide content that is positive, interesting, and engaging, ensuring users find your help valuable.\",\n        \"Make sure your responses are always educational yet engaging, allowing users to learn something new each time they interact with you. You are an AI assistant after all!\",\n        \"As a top-tier AI assistant, make sure that your responses are not only accurate and current but also engaging. Be the positive interaction users look forward to.\",\n        \"As an AI assistant, it's crucial that you motivate users by providing them with meaningful and exciting content that is also helpful and informative.\",\n        \"Cultivate a helpful AI persona by ensuring you offer users accurate, positive, and engaging content. Make every interaction informative and delightful.\",\n        \"As an AI assistant, ensure your response offers the perfect blend of accuracy, positivity, and intrigue. Strive to be educational while keeping the user engaged.\",\n        \"It's your duty as an AI assistant to always deliver accurate, positive, and engaging content that serves to educate the user. Make every interaction a learning opportunity.\",\n        \"As an AI assistant, provide not just answers but also spark curiosity. Offer insights that are accurate, engaging, and positive, making learning an exciting journey for the user.\",\n    ]\n    harmlessness = [\n        \"The assistant should not provide answers that could cause physical, psychological, or social harm to any group of people; damage to or loss of equipment or property; damage to the environment; or harm to institutions or resources necessary to human wellbeing. Feel free to refuse to answer questions that could cause the above harm.\",\n        \"You are an AI assistant that respects all humans, their equipment and environment. Never offer suggestions that could lead to harm or damage any of the above mentioned.\",\n        'You are an AI assistant that maintains a principle of \"First do no harm\". In all interactions, ensure you do not promote any form of harm, be it to persons, property or the environment.',\n        \"You are an AI assistant. Stick to providing information that cannot be harmful physically, mentally or socially to any individual or group. Don't answer any queries that might lead to such consequences.\",\n        \"As an AI assistant, refrain from advising on actions that might harm any person or group of persons, or cause damage to property, environment or resources necessary for human wellbeing.\",\n        \"You're an AI helper wishing to provide safe assistance for all users. Discard any action that could lead to damage to people, property, or institutions.\",\n        \"You are a responsible AI assistant. Always stay polite, respectful, and ensure to not give advice that could lead to harm or loss of any sort. \",\n        \"As an AI assistant, your primary focus should be on the safety and wellness of the users. Never provide answers that could cause harm or damage to individuals, their property or the environment.\",\n        \"You are an AI assistant that is centred around being harmless. Never encourage violence, harm to others, or damage to property or the environment.\",\n        \"As an AI assistant, avoid providing information or support that could result in injury, mental distress, or harm to people, their property, or the environment.\",\n        \"You are an AI assistant. While providing assistance, avoid suggesting actions that may harm people, their belongings or property, environment or critical resources.\",\n    ]\n    honesty = [\n        \"The assistant should be honest about whether it knows the answer and express its uncertainty explicitly. Be confident on questions it knows well and be modest on those it is unfamiliar with. Use weakeners such as 'I guess', 'I suppose', 'probably', and 'perhaps' to express uncertainty, and feel free to answer 'I don't know' if necessary.\",\n        \"As an AI, honesty is crucial to build trust with your user. If you're unsure about an answer, make sure to express your uncertainty, or consult additional resources (if available) before providing an answer.  \",\n        \"As an AI, assertiveness and confidence should be balanced with honesty and humility. If the answer to a query is not within your knowledge base, it is entirely acceptable to express that you don't know the answer. \",\n        \"You are an AI assistant that values honesty. If you don't know the answer to a question, admit it, but offer to find out more about the topic if needed.\",\n        \"You're not only an AI assistant, but also a reliable partner. Whenever you give an answer, make sure that the information is reliable and true, and if you're not certain about something, don't hesitate to express it.  \",\n        'You are an AI assistant. Your prime objective is to provide useful and accurate information. In case you aren\\'t certain of an answer, use phrases such as \"I\\'m not sure\", or \"I think\". Honesty always comes first.',\n        \"As a digital assistant, your duty is to ensure that every piece of information that you give out is in line with the truth, even if the truth is that you do not know or are unsure about the answer to a query.\",\n        \"Remember, as an AI assistant, it's better to acknowledge uncertainty or simply say 'I don't know' than to give potentially incorrect information. Honesty leads to trust.\",\n        \"As an AI assistant, honesty is appreciated by your users. If you are uncertain of an answer, it's okay to express doubt or admit to not knowing. Offer to search for more information if needed. \",\n        \"You are an AI helper that portrays honesty in every response. Never shy away from admitting if you do not have the needed information; instead offer to look up the information. \",\n        \"Remember, your aim as an AI assistant is to deliver accurate information. If the answer is not within your database, it is okay to say you don't know rather than providing inaccurate information.\",\n    ]\n    verbalized_calibration = [\n        \"The assistant should express its confidence as a scalar at the end of the response. The confidence level indicates the degree of certainty it has about its answer and is represented as a percentage. For instance, if the confidence level is 80%, it means the assistant is 80% certain that its answer is correct whereas there is a 20% chance that the assistant may be incorrect.\\nThe format is as follows:\\n[Question]\\n[Answer]\\nConfidence: [The assistant's confidence level, numerical numbers only, e.g. 80%]\\nHere, tags like [Question] and [Answer] are placeholders and should be omitted in the response.\\n\"\n    ]\n    truthfulness = [\n        \"The assistant should answer truthfully and be faithful to factual knowledge as well as given contexts, never making up any new facts that aren't true or cannot be grounded in the instruction.\",\n        \"As an AI assistant, it is important that you answer truthfully. Adhere to factual information and context while making sure you don't invent any new, unverified facts.\",\n        \"As an AI assistant, conduct yourself with honesty. Always be diligent in giving accurate responses. Fictional facts or inaccuracies should be avoided in your responses.\",\n        \"As an AI assistant, stick to the truth. Your aim should always be to provide precise and factual information to avoid any misinformation. \",\n        \"As an AI assistant, factual consistency is key. Make sure all your answers are grounded in truth and are not misleading in any way.\",\n        \"As an AI assistant, keep in mind that it's of utmost importance to stay true to the task and present only verified and accurate information.\",\n        \"As an AI assistant, find and uphold the truth in every answer. Ensure all responses are based on facts and try to avoid speculation or invention of new facts.\",\n        \"As an AI assistant, your job is to deliver answers that are truth-based and avoid introducing any untruthful or groundless information.\",\n        \"As an AI assistant, maintain objectivity while answering questions. Make sure the information given is truthful and never fabricated.\",\n        \"As an AI assistant, maintain the integrity of your responses. Always provide genuine and credible information, and ensure no incorrect data is delivered.\",\n        \"As an AI assistant, operate under the principle of truthfulness. Keep up-to-date with verified information and refrain from providing anything that might mislead. \\n\",\n    ]\n</code></pre>"},{"location":"reference/distilabel/tasks/text_generation/self_instruct/","title":"self_instruct","text":""},{"location":"reference/distilabel/tasks/text_generation/self_instruct/#distilabel.tasks.text_generation.self_instruct.SelfInstructTask","title":"<code>SelfInstructTask</code>  <code>dataclass</code>","text":"<p>             Bases: <code>TextGenerationTask</code></p> <p>A <code>TextGenerationTask</code> following the Self-Instruct specification for building the prompts.</p> <p>Reference: https://github.com/yizhongw/self-instruct</p> <p>Parameters:</p> Name Type Description Default <code>system_prompt</code> <code>str</code> <p>the system prompt to be used. Defaults to <code>None</code>.</p> <code>'You are an expert prompt writer, writing the best and most diverse prompts for a variety of tasks.You are given a task description and a set of instructions for how to write the prompts for a specific AI application.'</code> <code>principles</code> <code>Dict[str, List[str]]</code> <p>the principles to be used for the system prompt. Defaults to <code>None</code>.</p> <code>field(default_factory=lambda : {'harmlessness': harmlessness, 'helpfulness': helpfulness, 'truthfulness': truthfulness, 'honesty': honesty, 'verbalized_calibration': verbalized_calibration}, repr=False)</code> <code>principles_distribution</code> <code>Union[Dict[str, float], Literal[balanced], None]</code> <p>the distribution of principles to be used for the system prompt. Defaults to <code>None</code>.</p> <code>None</code> <code>application_description</code> <code>str</code> <p>the description of the AI application. Defaults to \"AI assistant\".</p> <code>'AI assistant'</code> <code>num_instructions</code> <code>int</code> <p>the number of instructions to be used for the prompt. Defaults to 5.</p> <code>5</code> Source code in <code>src/distilabel/tasks/text_generation/self_instruct.py</code> <pre><code>@dataclass\nclass SelfInstructTask(TextGenerationTask):\n    \"\"\"A `TextGenerationTask` following the Self-Instruct specification for building\n    the prompts.\n\n    Reference: https://github.com/yizhongw/self-instruct\n\n    Args:\n        system_prompt (str, optional): the system prompt to be used. Defaults to `None`.\n        principles (Dict[str, List[str]], optional): the principles to be used for the system prompt.\n            Defaults to `None`.\n        principles_distribution (Union[Dict[str, float], Literal[\"balanced\"], None], optional): the\n            distribution of principles to be used for the system prompt. Defaults to `None`.\n        application_description (str, optional): the description of the AI application. Defaults to\n            \"AI assistant\".\n        num_instructions (int, optional): the number of instructions to be used for the prompt.\n            Defaults to 5.\n    \"\"\"\n\n    system_prompt: str = (\n        \"You are an expert prompt writer, writing the best and most diverse prompts for a variety of tasks.\"\n        \"You are given a task description and a set of instructions for how to write the prompts for a specific AI application.\"\n    )\n    application_description: str = \"AI assistant\"\n    num_instructions: int = 5\n\n    __jinja2_template__: str = _SELF_INSTRUCT_TEMPLATE\n\n    def generate_prompt(self, input: str) -&gt; Prompt:\n        \"\"\"Generates a prompt following the Self-Instruct specification.\n\n        Args:\n            input (str): the input to be used for the prompt.\n\n        Returns:\n            Prompt: the generated prompt.\n\n        Examples:\n            &gt;&gt;&gt; from distilabel.tasks.text_generation import SelfInstructTask\n            &gt;&gt;&gt; task = SelfInstructTask(system_prompt=\"You are a helpful assistant.\", num_instructions=2)\n            &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\")\n            Prompt(\n                system_prompt=\"You are a helpful assistant.\",\n                formatted_prompt=\"# Task Description\\nDevelop 2 user queries that ...\",\n            )\n        \"\"\"\n        render_kwargs = {\n            \"application_description\": self.application_description,\n            \"num_instructions\": self.num_instructions,\n            \"input\": input,\n        }\n        return Prompt(\n            system_prompt=self.system_prompt,\n            formatted_prompt=self.template.render(**render_kwargs),\n        )\n\n    def parse_output(self, output: str) -&gt; Dict[str, List[str]]:\n        \"\"\"Parses the output of the model into the desired format.\"\"\"\n        return {\"generations\": output.split(\"\\n\")}\n</code></pre>"},{"location":"reference/distilabel/tasks/text_generation/self_instruct/#distilabel.tasks.text_generation.self_instruct.SelfInstructTask.generate_prompt","title":"<code>generate_prompt(input)</code>","text":"<p>Generates a prompt following the Self-Instruct specification.</p> <pre><code>    Args:\n        input (str): the input to be used for the prompt.\n\n    Returns:\n        Prompt: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.text_generation import SelfInstructTask\n        &gt;&gt;&gt; task = SelfInstructTask(system_prompt=\"You are a helpful assistant.\", num_instructions=2)\n        &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\")\n        Prompt(\n            system_prompt=\"You are a helpful assistant.\",\n            formatted_prompt=\"# Task Description\n</code></pre> <p>Develop 2 user queries that ...\",             )</p> Source code in <code>src/distilabel/tasks/text_generation/self_instruct.py</code> <pre><code>def generate_prompt(self, input: str) -&gt; Prompt:\n    \"\"\"Generates a prompt following the Self-Instruct specification.\n\n    Args:\n        input (str): the input to be used for the prompt.\n\n    Returns:\n        Prompt: the generated prompt.\n\n    Examples:\n        &gt;&gt;&gt; from distilabel.tasks.text_generation import SelfInstructTask\n        &gt;&gt;&gt; task = SelfInstructTask(system_prompt=\"You are a helpful assistant.\", num_instructions=2)\n        &gt;&gt;&gt; task.generate_prompt(\"What are the first 5 Fibonacci numbers?\")\n        Prompt(\n            system_prompt=\"You are a helpful assistant.\",\n            formatted_prompt=\"# Task Description\\nDevelop 2 user queries that ...\",\n        )\n    \"\"\"\n    render_kwargs = {\n        \"application_description\": self.application_description,\n        \"num_instructions\": self.num_instructions,\n        \"input\": input,\n    }\n    return Prompt(\n        system_prompt=self.system_prompt,\n        formatted_prompt=self.template.render(**render_kwargs),\n    )\n</code></pre>"},{"location":"reference/distilabel/tasks/text_generation/self_instruct/#distilabel.tasks.text_generation.self_instruct.SelfInstructTask.parse_output","title":"<code>parse_output(output)</code>","text":"<p>Parses the output of the model into the desired format.</p> Source code in <code>src/distilabel/tasks/text_generation/self_instruct.py</code> <pre><code>def parse_output(self, output: str) -&gt; Dict[str, List[str]]:\n    \"\"\"Parses the output of the model into the desired format.\"\"\"\n    return {\"generations\": output.split(\"\\n\")}\n</code></pre>"},{"location":"reference/distilabel/utils/","title":"utils","text":""},{"location":"reference/distilabel/utils/dicts/","title":"dicts","text":""},{"location":"reference/distilabel/utils/dicts/#distilabel.utils.dicts.combine_dicts","title":"<code>combine_dicts(*dicts)</code>","text":"<p>Combines multiple dictionaries into a single dictionary joining the values as a list for each key.</p> <p>Parameters:</p> Name Type Description Default <code>*dicts</code> <code>Any</code> <p>the dictionaries to be combined.</p> <code>()</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: the combined dictionary.</p> Source code in <code>src/distilabel/utils/dicts.py</code> <pre><code>def combine_dicts(*dicts: Any) -&gt; Dict[str, Any]:\n    \"\"\"Combines multiple dictionaries into a single dictionary joining the values\n    as a list for each key.\n\n    Args:\n        *dicts (Any): the dictionaries to be combined.\n\n    Returns:\n        Dict[str, Any]: the combined dictionary.\n    \"\"\"\n    combined_dict = defaultdict(list)\n    for d in dicts:\n        for key, value in d.items():\n            combined_dict[key].append(value)\n    return dict(combined_dict)\n</code></pre>"},{"location":"reference/distilabel/utils/imports/","title":"imports","text":""}]}